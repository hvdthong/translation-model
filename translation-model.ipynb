{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Neural Translation Model in PyTorch</h1>\n",
    "by Mac Brennan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align: center !important;'>\n",
    " <img src='https://github.com/macbrennan90/macbrennan90.github.io/blob/master/images/encoder-decoder.png?raw=true'\n",
    "      alt='Translation Model Summary'>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project will be broken up into several parts as follows:\n",
    "\n",
    "__Part 1:__ Preparing the words\n",
    "\n",
    "+ Inspecting the Dataset\n",
    "+ Using Word Embeddings\n",
    "+ Organizing the Data\n",
    "\n",
    "__Part 2:__ Building the Model\n",
    "\n",
    "+ Bi-Directional LSTM Encoder\n",
    "+ Building Attention\n",
    "+ Decoder with Attention\n",
    "\n",
    "__Part 3:__ Training the Model\n",
    "\n",
    "+ Training Function\n",
    "+ Training Loop\n",
    "\n",
    "__Part 4:__ Using the Model for Evaluation\n",
    "\n",
    "__Part 5:__ Vizualize the Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we get started we will load all the packages we will need\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import os.path\n",
    "\n",
    "# Use gpu if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Preparing the Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset that will be used is a text file of english sentences and the corresponding french sentences.\n",
    "\n",
    "Each sentence is on a new line. The sentences will be split into a list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data\n",
    "The data will be stored in two lists where each item is a sentence. The lists are:\n",
    "+ english_sentences\n",
    "+ french_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/small_vocab_en', \"r\") as f:\n",
    "    data1 = f.read()\n",
    "with open('data/small_vocab_fr', \"r\") as f:\n",
    "    data2 = f.read()\n",
    "    \n",
    "# The data is just in a text file with each sentence on its own line\n",
    "english_sentences = data1.split('\\n')\n",
    "french_sentences = data2.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of English sentences: 137861 \n",
      "Number of French sentences: 137861 \n",
      "\n",
      "Example/Target pair:\n",
      "\n",
      "  california is usually quiet during march , and it is usually hot in june .\n",
      "  california est généralement calme en mars , et il est généralement chaud en juin .\n"
     ]
    }
   ],
   "source": [
    "print('Number of English sentences:', len(english_sentences), \n",
    "      '\\nNumber of French sentences:', len(french_sentences),'\\n')\n",
    "print('Example/Target pair:\\n')\n",
    "print('  '+english_sentences[2])\n",
    "print('  '+french_sentences[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vocabulary\n",
    "Let's take a closer look at the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['california',\n",
       " 'is',\n",
       " 'usually',\n",
       " 'quiet',\n",
       " 'during',\n",
       " 'march',\n",
       " ',',\n",
       " 'and',\n",
       " 'it',\n",
       " 'is',\n",
       " 'usually',\n",
       " 'hot',\n",
       " 'in',\n",
       " 'june',\n",
       " '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_sentences[2].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest english sentence in our dataset is: 17\n"
     ]
    }
   ],
   "source": [
    "max_en_length = 0\n",
    "for sentence in english_sentences:\n",
    "    length = len(sentence.split())\n",
    "    max_en_length = max(max_en_length, length)\n",
    "print(\"The longest english sentence in our dataset is:\", max_en_length)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest french sentence in our dataset is: 23\n"
     ]
    }
   ],
   "source": [
    "max_fr_length = 0\n",
    "for sentence in french_sentences:\n",
    "    length = len(sentence.split())\n",
    "    max_fr_length = max(max_fr_length, length)\n",
    "print(\"The longest french sentence in our dataset is:\", max_fr_length)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_word_count = {}\n",
    "fr_word_count = {}\n",
    "\n",
    "for sentence in english_sentences:\n",
    "    for word in sentence.split():\n",
    "        if word in en_word_count:\n",
    "            en_word_count[word] +=1\n",
    "        else:\n",
    "            en_word_count[word] = 1\n",
    "            \n",
    "for sentence in french_sentences:\n",
    "    for word in sentence.split():\n",
    "        if word in fr_word_count:\n",
    "            fr_word_count[word] +=1\n",
    "        else:\n",
    "            fr_word_count[word] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique English words: 227\n",
      "Number of unique French words: 355\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique English words:', len(en_word_count))\n",
    "print('Number of unique French words:', len(fr_word_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value(items_tuple):\n",
    "    return items_tuple[1]\n",
    "\n",
    "sorted_en_words= sorted(en_word_count.items(), key=get_value, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('is', 205858),\n",
       " (',', 140897),\n",
       " ('.', 129039),\n",
       " ('in', 75525),\n",
       " ('it', 75137),\n",
       " ('during', 74933),\n",
       " ('the', 67628),\n",
       " ('but', 63987),\n",
       " ('and', 59850),\n",
       " ('sometimes', 37746),\n",
       " ('usually', 37507),\n",
       " ('never', 37500),\n",
       " ('least', 27564),\n",
       " ('favorite', 27371),\n",
       " ('fruit', 27105),\n",
       " ('most', 14934),\n",
       " ('loved', 13666),\n",
       " ('liked', 13546),\n",
       " ('new', 12197),\n",
       " ('paris', 11334),\n",
       " ('india', 11277),\n",
       " ('united', 11270),\n",
       " ('states', 11270),\n",
       " ('california', 11250),\n",
       " ('jersey', 11225),\n",
       " ('france', 11170),\n",
       " ('china', 10953),\n",
       " ('he', 10786),\n",
       " ('she', 10786),\n",
       " ('grapefruit', 10118),\n",
       " ('your', 9734),\n",
       " ('my', 9700),\n",
       " ('his', 9700),\n",
       " ('her', 9700),\n",
       " ('fall', 9134),\n",
       " ('june', 9133),\n",
       " ('spring', 9102),\n",
       " ('january', 9090),\n",
       " ('winter', 9038),\n",
       " ('march', 9023),\n",
       " ('autumn', 9004),\n",
       " ('may', 8995),\n",
       " ('nice', 8984),\n",
       " ('september', 8958),\n",
       " ('july', 8956),\n",
       " ('april', 8954),\n",
       " ('november', 8951),\n",
       " ('summer', 8948),\n",
       " ('december', 8945),\n",
       " ('february', 8942),\n",
       " ('our', 8932),\n",
       " ('their', 8932),\n",
       " ('freezing', 8928),\n",
       " ('pleasant', 8916),\n",
       " ('beautiful', 8915),\n",
       " ('october', 8910),\n",
       " ('snowy', 8898),\n",
       " ('warm', 8890),\n",
       " ('cold', 8878),\n",
       " ('wonderful', 8808),\n",
       " ('dry', 8794),\n",
       " ('busy', 8791),\n",
       " ('august', 8789),\n",
       " ('chilly', 8770),\n",
       " ('rainy', 8761),\n",
       " ('mild', 8743),\n",
       " ('wet', 8726),\n",
       " ('relaxing', 8696),\n",
       " ('quiet', 8693),\n",
       " ('hot', 8639),\n",
       " ('dislikes', 7314),\n",
       " ('likes', 7314),\n",
       " ('limes', 5554),\n",
       " ('mangoes', 5549),\n",
       " ('lemons', 5533),\n",
       " ('grapes', 5525),\n",
       " ('apples', 5452),\n",
       " ('oranges', 5452),\n",
       " ('strawberries', 5452),\n",
       " ('bananas', 5452),\n",
       " ('peaches', 5451),\n",
       " ('pears', 5451),\n",
       " ('to', 5166),\n",
       " ('strawberry', 4715),\n",
       " ('grape', 4703),\n",
       " ('lime', 4680),\n",
       " ('apple', 4652),\n",
       " ('lemon', 4652),\n",
       " ('banana', 4652),\n",
       " ('mango', 4652),\n",
       " ('pear', 4652),\n",
       " ('peach', 4652),\n",
       " ('orange', 4651),\n",
       " ('like', 4588),\n",
       " ('dislike', 4444),\n",
       " ('they', 3222),\n",
       " ('that', 2712),\n",
       " ('i', 2664),\n",
       " ('we', 2532),\n",
       " ('you', 2414),\n",
       " ('animal', 2304),\n",
       " ('a', 1944),\n",
       " ('truck', 1944),\n",
       " ('car', 1944),\n",
       " ('automobile', 1944),\n",
       " ('was', 1867),\n",
       " ('next', 1666),\n",
       " ('go', 1386),\n",
       " ('driving', 1296),\n",
       " ('visit', 1224),\n",
       " ('little', 1016),\n",
       " ('big', 1016),\n",
       " ('old', 972),\n",
       " ('yellow', 972),\n",
       " ('red', 972),\n",
       " ('rusty', 972),\n",
       " ('blue', 972),\n",
       " ('white', 972),\n",
       " ('black', 972),\n",
       " ('green', 972),\n",
       " ('shiny', 972),\n",
       " ('favorite.', 961),\n",
       " ('are', 870),\n",
       " ('?', 811),\n",
       " ('last', 781),\n",
       " ('feared', 768),\n",
       " ('animals', 768),\n",
       " ('this', 768),\n",
       " ('plan', 714),\n",
       " ('going', 666),\n",
       " ('saw', 648),\n",
       " ('disliked', 648),\n",
       " ('drives', 648),\n",
       " ('drove', 648),\n",
       " ('grapefruit.', 574),\n",
       " ('between', 540),\n",
       " ('liked.', 500),\n",
       " ('loved.', 500),\n",
       " ('translate', 480),\n",
       " ('plans', 476),\n",
       " ('peaches.', 393),\n",
       " ('pears.', 393),\n",
       " ('bananas.', 392),\n",
       " ('oranges.', 392),\n",
       " ('apples.', 392),\n",
       " ('strawberries.', 392),\n",
       " ('were', 384),\n",
       " ('went', 378),\n",
       " ('might', 378),\n",
       " ('wanted', 378),\n",
       " ('thinks', 360),\n",
       " ('grapes.', 319),\n",
       " ('spanish', 312),\n",
       " ('portuguese', 312),\n",
       " ('chinese', 312),\n",
       " ('english', 312),\n",
       " ('french', 312),\n",
       " ('lemons.', 311),\n",
       " ('translating', 300),\n",
       " ('mangoes.', 295),\n",
       " ('limes.', 290),\n",
       " ('difficult', 260),\n",
       " ('fun', 260),\n",
       " ('easy', 260),\n",
       " ('wants', 252),\n",
       " ('think', 240),\n",
       " ('why', 240),\n",
       " (\"it's\", 240),\n",
       " ('did', 204),\n",
       " ('orange.', 197),\n",
       " ('mango.', 196),\n",
       " ('banana.', 196),\n",
       " ('peach.', 196),\n",
       " ('lemon.', 196),\n",
       " ('pear.', 196),\n",
       " ('apple.', 196),\n",
       " ('cat', 192),\n",
       " ('shark', 192),\n",
       " ('bird', 192),\n",
       " ('mouse', 192),\n",
       " ('horse', 192),\n",
       " ('elephant', 192),\n",
       " ('dog', 192),\n",
       " ('monkey', 192),\n",
       " ('lion', 192),\n",
       " ('bear', 192),\n",
       " ('rabbit', 192),\n",
       " ('snake', 192),\n",
       " ('lime.', 168),\n",
       " ('grape.', 145),\n",
       " ('when', 144),\n",
       " ('strawberry.', 133),\n",
       " ('want', 126),\n",
       " ('fruit.', 87),\n",
       " ('do', 84),\n",
       " ('how', 67),\n",
       " ('elephants', 64),\n",
       " ('horses', 64),\n",
       " ('dogs', 64),\n",
       " ('sharks', 64),\n",
       " ('snakes', 64),\n",
       " ('cats', 64),\n",
       " ('rabbits', 64),\n",
       " ('monkeys', 64),\n",
       " ('bears', 64),\n",
       " ('birds', 64),\n",
       " ('lions', 64),\n",
       " ('mice', 64),\n",
       " (\"didn't\", 60),\n",
       " ('eiffel', 57),\n",
       " ('tower', 57),\n",
       " ('grocery', 57),\n",
       " ('store', 57),\n",
       " ('football', 57),\n",
       " ('field', 57),\n",
       " ('lake', 57),\n",
       " ('school', 57),\n",
       " ('would', 48),\n",
       " (\"aren't\", 36),\n",
       " ('been', 36),\n",
       " ('weather', 33),\n",
       " ('does', 24),\n",
       " ('has', 24),\n",
       " (\"isn't\", 24),\n",
       " ('am', 24),\n",
       " ('where', 12),\n",
       " ('have', 12)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_en_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_fr_words = sorted(fr_word_count.items(), key=get_value, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('est', 196809),\n",
       " ('.', 135619),\n",
       " (',', 123135),\n",
       " ('en', 105768),\n",
       " ('il', 84079),\n",
       " ('les', 65255),\n",
       " ('mais', 63987),\n",
       " ('et', 59851),\n",
       " ('la', 49861),\n",
       " ('parfois', 37746),\n",
       " ('jamais', 37215),\n",
       " ('le', 35306),\n",
       " (\"l'\", 32917),\n",
       " ('généralement', 31292),\n",
       " ('moins', 27557),\n",
       " ('au', 25738),\n",
       " ('aimé', 24842),\n",
       " ('fruit', 23626),\n",
       " ('préféré', 22886),\n",
       " ('agréable', 17751),\n",
       " ('froid', 16794),\n",
       " ('son', 16496),\n",
       " ('chaud', 16405),\n",
       " ('de', 15070),\n",
       " ('plus', 14934),\n",
       " ('automne', 14727),\n",
       " ('mois', 14350),\n",
       " ('à', 13870),\n",
       " ('elle', 12056),\n",
       " ('citrons', 11679),\n",
       " ('paris', 11334),\n",
       " ('inde', 11277),\n",
       " ('états-unis', 11210),\n",
       " ('france', 11170),\n",
       " ('jersey', 11052),\n",
       " ('new', 11047),\n",
       " ('chine', 10936),\n",
       " ('pendant', 10741),\n",
       " ('pamplemousse', 10140),\n",
       " ('mon', 9403),\n",
       " ('votre', 9368),\n",
       " ('juin', 9133),\n",
       " ('printemps', 9100),\n",
       " ('janvier', 9090),\n",
       " ('hiver', 9038),\n",
       " ('mars', 9023),\n",
       " ('été', 8999),\n",
       " ('mai', 8995),\n",
       " ('septembre', 8958),\n",
       " ('juillet', 8956),\n",
       " ('avril', 8954),\n",
       " ('novembre', 8951),\n",
       " ('décembre', 8945),\n",
       " ('février', 8942),\n",
       " ('octobre', 8911),\n",
       " ('aime', 8870),\n",
       " ('août', 8789),\n",
       " ('merveilleux', 8704),\n",
       " ('relaxant', 8458),\n",
       " ('doux', 8458),\n",
       " ('humide', 8446),\n",
       " ('notre', 8319),\n",
       " ('californie', 8189),\n",
       " ('sec', 7957),\n",
       " ('leur', 7855),\n",
       " ('occupé', 7782),\n",
       " ('pluvieux', 7658),\n",
       " ('calme', 7256),\n",
       " ('beau', 6387),\n",
       " ('habituellement', 6215),\n",
       " ('pommes', 5844),\n",
       " ('pêches', 5844),\n",
       " ('oranges', 5844),\n",
       " ('poires', 5844),\n",
       " ('fraises', 5844),\n",
       " ('bananes', 5844),\n",
       " ('verts', 5835),\n",
       " ('raisins', 5780),\n",
       " ('mangues', 5774),\n",
       " (\"d'\", 5100),\n",
       " ('mangue', 4899),\n",
       " ('gel', 4886),\n",
       " ('raisin', 4852),\n",
       " ('pomme', 4848),\n",
       " (\"l'orange\", 4848),\n",
       " ('citron', 4848),\n",
       " ('chaux', 4848),\n",
       " ('banane', 4848),\n",
       " ('poire', 4848),\n",
       " ('fraise', 4848),\n",
       " ('pêche', 4848),\n",
       " ('pas', 4495),\n",
       " ('enneigée', 4008),\n",
       " ('favori', 3857),\n",
       " ('déteste', 3743),\n",
       " ('gèle', 3622),\n",
       " ('fruits', 3566),\n",
       " ('voiture', 3510),\n",
       " (\"l'automne\", 3411),\n",
       " ('ils', 3185),\n",
       " (\"n'aime\", 3131),\n",
       " ('california', 3061),\n",
       " ('neige', 3016),\n",
       " ('fait', 2916),\n",
       " ('belle', 2726),\n",
       " ('ne', 2715),\n",
       " ('nous', 2520),\n",
       " ('vous', 2517),\n",
       " ('des', 2435),\n",
       " ('animal', 2248),\n",
       " ('camion', 1944),\n",
       " ('cours', 1927),\n",
       " ('neigeux', 1867),\n",
       " ('conduit', 1706),\n",
       " ('prochain', 1666),\n",
       " ('je', 1548),\n",
       " ('ce', 1465),\n",
       " ('tranquille', 1437),\n",
       " ('a', 1356),\n",
       " ('cher', 1308),\n",
       " ('une', 1278),\n",
       " ('cette', 1239),\n",
       " ('était', 1198),\n",
       " ('aller', 1180),\n",
       " ('chaude', 1124),\n",
       " ('aiment', 1116),\n",
       " ('aimons', 1111),\n",
       " (\"n'aiment\", 1111),\n",
       " (\"n'aimez\", 1094),\n",
       " ('leurs', 1072),\n",
       " ('aimez', 1053),\n",
       " ('sont', 1018),\n",
       " ('aimé.', 1010),\n",
       " ('détestons', 1001),\n",
       " ('jaune', 972),\n",
       " ('rouge', 972),\n",
       " (\"j'aime\", 966),\n",
       " ('visiter', 908),\n",
       " ('sèche', 837),\n",
       " ('occupée', 836),\n",
       " ('frisquet', 834),\n",
       " ('?', 811),\n",
       " ('préférée', 770),\n",
       " ('animaux', 768),\n",
       " ('dernier', 757),\n",
       " ('aimait', 707),\n",
       " ('un', 698),\n",
       " ('conduisait', 673),\n",
       " ('que', 667),\n",
       " ('nouvelle', 648),\n",
       " ('vieille', 647),\n",
       " ('vu', 645),\n",
       " ('verte', 628),\n",
       " ('petite', 615),\n",
       " ('nos', 613),\n",
       " ('noire', 602),\n",
       " ('brillant', 587),\n",
       " ('blanche', 579),\n",
       " ('redouté', 576),\n",
       " ('pleut', 562),\n",
       " (\"n'aimait\", 561),\n",
       " ('pamplemousses', 552),\n",
       " ('pense', 540),\n",
       " ('entre', 540),\n",
       " ('bleue', 504),\n",
       " ('nouveau', 502),\n",
       " ('traduire', 501),\n",
       " ('rouillée', 486),\n",
       " ('bleu', 468),\n",
       " ('se', 461),\n",
       " ('grande', 459),\n",
       " ('rouillé', 454),\n",
       " ('préféré.', 419),\n",
       " ('ses', 402),\n",
       " (\"qu'il\", 393),\n",
       " ('blanc', 393),\n",
       " ('aux', 392),\n",
       " ('brillante', 385),\n",
       " ('préférés', 383),\n",
       " ('noir', 370),\n",
       " ('pluies', 367),\n",
       " ('envisage', 360),\n",
       " ('étaient', 357),\n",
       " ('va', 355),\n",
       " ('rendre', 350),\n",
       " ('vert', 344),\n",
       " ('-', 328),\n",
       " ('vieux', 325),\n",
       " ('petit', 324),\n",
       " ('espagnol', 312),\n",
       " ('portugais', 312),\n",
       " ('chinois', 312),\n",
       " ('anglais', 312),\n",
       " ('français', 312),\n",
       " ('glaciales', 307),\n",
       " ('mes', 297),\n",
       " ('cet', 286),\n",
       " ('automobile', 278),\n",
       " ('traduction', 277),\n",
       " ('mouillé', 273),\n",
       " ('difficile', 260),\n",
       " ('amusant', 260),\n",
       " ('facile', 260),\n",
       " ('comme', 259),\n",
       " ('gros', 258),\n",
       " ('souris', 256),\n",
       " ('pourrait', 252),\n",
       " ('voulait', 252),\n",
       " ('veut', 252),\n",
       " ('pourquoi', 240),\n",
       " ('aimés', 237),\n",
       " ('prévois', 233),\n",
       " ('prévoyons', 232),\n",
       " ('vos', 225),\n",
       " ('intention', 206),\n",
       " ('clémentes', 200),\n",
       " ('ont', 194),\n",
       " ('chat', 192),\n",
       " ('requin', 192),\n",
       " ('cheval', 192),\n",
       " ('chien', 192),\n",
       " ('singe', 192),\n",
       " ('lion', 192),\n",
       " ('ours', 192),\n",
       " ('lapin', 192),\n",
       " ('serpent', 192),\n",
       " ('redoutés', 190),\n",
       " ('allé', 187),\n",
       " ('grosse', 185),\n",
       " ('pluie', 174),\n",
       " ('trop', 173),\n",
       " ('monde', 173),\n",
       " ('maillot', 173),\n",
       " ('vont', 168),\n",
       " ('volant', 165),\n",
       " ('avez', 162),\n",
       " ('i', 150),\n",
       " ('allés', 150),\n",
       " ('allée', 150),\n",
       " ('quand', 144),\n",
       " ('oiseau', 128),\n",
       " ('éléphant', 128),\n",
       " ('pourraient', 126),\n",
       " ('voulaient', 126),\n",
       " ('veulent', 126),\n",
       " ('détendre', 111),\n",
       " ('aimée', 105),\n",
       " ('magnifique', 104),\n",
       " (\"l'automobile\", 100),\n",
       " (\"n'aimons\", 97),\n",
       " ('-ce', 95),\n",
       " ('gelé', 94),\n",
       " ('détestait', 87),\n",
       " ('grand', 81),\n",
       " ('bien', 77),\n",
       " ('vers', 76),\n",
       " ('prévoient', 75),\n",
       " ('prévoit', 75),\n",
       " ('lui', 70),\n",
       " ('visite', 68),\n",
       " ('comment', 67),\n",
       " ('éléphants', 64),\n",
       " ('chevaux', 64),\n",
       " ('chiens', 64),\n",
       " (\"l'éléphant\", 64),\n",
       " (\"l'oiseau\", 64),\n",
       " ('requins', 64),\n",
       " (\"l'ours\", 64),\n",
       " ('serpents', 64),\n",
       " ('chats', 64),\n",
       " ('lapins', 64),\n",
       " ('singes', 64),\n",
       " ('oiseaux', 64),\n",
       " ('lions', 64),\n",
       " ('légère', 63),\n",
       " ('cépage', 60),\n",
       " ('pensez', 60),\n",
       " ('États-unis', 57),\n",
       " ('tour', 57),\n",
       " ('eiffel', 57),\n",
       " (\"l'épicerie\", 57),\n",
       " ('terrain', 57),\n",
       " ('football', 57),\n",
       " ('lac', 57),\n",
       " (\"l'école\", 57),\n",
       " (\"l'animal\", 56),\n",
       " (\"n'est\", 47),\n",
       " ('allons', 45),\n",
       " ('allez', 45),\n",
       " ('peu', 41),\n",
       " ('pousse', 41),\n",
       " ('du', 39),\n",
       " ('-il', 36),\n",
       " ('temps', 33),\n",
       " ('at', 32),\n",
       " ('rouille', 32),\n",
       " ('sur', 28),\n",
       " (\"qu'elle\", 26),\n",
       " ('-ils', 26),\n",
       " ('petites', 26),\n",
       " ('-elle', 24),\n",
       " ('dernière', 24),\n",
       " ('êtes-vous', 24),\n",
       " ('vais', 24),\n",
       " ('voudrait', 24),\n",
       " ('proches', 20),\n",
       " ('frais', 20),\n",
       " ('manguiers', 19),\n",
       " ('avons', 19),\n",
       " ('t', 18),\n",
       " ('porcelaine', 17),\n",
       " ('détestez', 17),\n",
       " (\"c'est\", 17),\n",
       " ('grandes', 16),\n",
       " ('préférées', 16),\n",
       " ('douce', 14),\n",
       " ('durant', 14),\n",
       " ('congélation', 14),\n",
       " ('plaît', 13),\n",
       " ('où', 12),\n",
       " ('dans', 12),\n",
       " ('est-ce', 12),\n",
       " ('voulez', 12),\n",
       " ('aimeraient', 12),\n",
       " (\"n'a\", 12),\n",
       " ('petits', 10),\n",
       " ('aiment-ils', 10),\n",
       " ('grands', 9),\n",
       " ('limes', 9),\n",
       " ('envisagent', 9),\n",
       " ('grosses', 8),\n",
       " ('bénigne', 8),\n",
       " ('mouillée', 7),\n",
       " ('enneigé', 7),\n",
       " ('moindres', 7),\n",
       " ('conduite', 6),\n",
       " ('gelés', 5),\n",
       " ('tout', 4),\n",
       " ('etats-unis', 3),\n",
       " (\"n'êtes\", 3),\n",
       " ('vit', 3),\n",
       " ('ressort', 2),\n",
       " ('détend', 2),\n",
       " ('redoutée', 2),\n",
       " ('qui', 2),\n",
       " ('traduis', 2),\n",
       " ('apprécié', 2),\n",
       " ('allions', 1),\n",
       " ('trouvé', 1),\n",
       " ('as-tu', 1),\n",
       " ('faire', 1),\n",
       " ('favoris', 1),\n",
       " ('souvent', 1),\n",
       " ('es-tu', 1),\n",
       " ('moteur', 1)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_fr_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the dataset is pretty small, we may want to get a bigger data set, but we'll see how this one does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are building an embedding matrix of pretrained word vectors. The word embeddings used here were downloaded from the fastText repository. These embeddings have 300 dimensions. To start we will add a few token embeddings for our specific case. We want a token to signal the start of the sentence, A token for words that we do not have an embedding for, and a token to pad sentences so all the sentences we use have the same length. This will allow us to train the model on batches of sentences rather than one at a time.\n",
    "\n",
    "After this step we will have a dictionary and an embedding matrix for each language. The dictionary will map words to an index value in the embedding matrix where its' corresponding embedding vector is stored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Embeddings for the English data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings load from .npy file\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('data/en_words.npy') and os.path.exists('data/en_vectors.npy'):\n",
    "    en_words = np.load('data/en_words.npy')\n",
    "    en_vectors = np.load('data/en_vectors.npy')\n",
    "    print('Embeddings load from .npy file')\n",
    "else:\n",
    "    # make a dict with the top 100,000 words\n",
    "    en_words = ['<pad>', # Padding Token\n",
    "                '<s>', # Start of sentence token\n",
    "                '<unk>'# Unknown word token\n",
    "               ]\n",
    "\n",
    "    en_vectors = list(np.random.randn(3, 300))\n",
    "    en_vectors[0] *= 0 # make the padding vector zeros\n",
    "\n",
    "    with open('data/wiki.en.vec', \"r\") as f:\n",
    "        f.readline()\n",
    "        for _ in range(100000):\n",
    "            en_vecs = f.readline()\n",
    "            word = en_vecs.split()[0]\n",
    "            vector = np.float32(en_vecs.split()[1:])\n",
    "\n",
    "            # skip lines that don't have 300 dim\n",
    "            if len(vector) != 300:\n",
    "                continue\n",
    "\n",
    "            if word not in en_words:\n",
    "                en_words.append(word)\n",
    "                en_vectors.append(vector)\n",
    "        print(word, vector[:10]) # Last word embedding read from the file\n",
    "\n",
    "    # Save the arrays so we don't have to load the full word embedding file\n",
    "    np.save('data/en_words.npy', en_words)\n",
    "    np.save('data/en_vectors.npy', en_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_word2idx = {word:index for index, word in enumerate(en_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index for word hemophilia: 99996 \n",
      "vector for word hemophilia:\n",
      " [ 0.16189    -0.056121   -0.65560001  0.21569    -0.11878    -0.02066\n",
      "  0.37613001 -0.24117    -0.098989   -0.010058  ]\n"
     ]
    }
   ],
   "source": [
    "hemophilia_idx = en_word2idx['hemophilia']\n",
    "print('index for word hemophilia:', hemophilia_idx, \n",
    "      '\\nvector for word hemophilia:\\n',en_vectors[hemophilia_idx][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word embedding for hemophilia matches the one read from the file, so it looks like everything worked properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Embeddings for the Frech data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings load from .npy file\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('data/fr_words.npy') and os.path.exists('data/fr_vectors.npy'):\n",
    "    fr_words = np.load('data/fr_words.npy')\n",
    "    fr_vectors = np.load('data/fr_vectors.npy')\n",
    "    print('Embeddings load from .npy file')\n",
    "else:\n",
    "    # make a dict with the top 100,000 words\n",
    "    fr_words = ['<pad>',\n",
    "                '<s>',\n",
    "                '<unk>']\n",
    "\n",
    "    fr_vectors = list(np.random.randn(3, 300))\n",
    "    fr_vectors[0] = np.zeros(300) # make the padding vector zeros\n",
    "\n",
    "    np.load()\n",
    "    with open('data/wiki.fr.vec', \"r\") as f:\n",
    "        f.readline()\n",
    "        for _ in range(100000):\n",
    "            fr_vecs = f.readline()\n",
    "            word = fr_vecs.split()[0]\n",
    "            try:\n",
    "                vector = np.float32(fr_vecs.split()[1:])\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "             # skip lines that don't have 300 dim\n",
    "            if len(vector) != 300:\n",
    "                continue\n",
    "\n",
    "            if word not in fr_words:\n",
    "                fr_words.append(word)\n",
    "                fr_vectors.append(vector)\n",
    "        print(word, vector[:10])\n",
    "    # Save the arrays so we don't have to load the full word embedding file\n",
    "    np.save('data/fr_words.npy', fr_words)\n",
    "    np.save('data/fr_vectors.npy', fr_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_word2idx = {word:index for index, word in enumerate(fr_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index for word chabeuil: 99783 \n",
      "vector for word chabeuil:\n",
      " [-0.18058001 -0.24758001  0.075607    0.17299999  0.24116001 -0.11223\n",
      " -0.28173     0.27373999  0.37997001  0.48008999]\n"
     ]
    }
   ],
   "source": [
    "chabeuil_idx = fr_word2idx['chabeuil']\n",
    "print('index for word chabeuil:', chabeuil_idx, \n",
    "      '\\nvector for word chabeuil:\\n',fr_vectors[chabeuil_idx][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word embedding for chabeuil matches as well so everything worked correctly for the french vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrays already saved\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('data/fr_words.npy') and os.path.exists('data/en_words.npy'):\n",
    "    print('Arrays already saved')\n",
    "else:\n",
    "    np.save('data/fr_words.npy', fr_words)\n",
    "    np.save('data/fr_vectors.npy', fr_vectors)\n",
    "    np.save('data/en_words.npy', en_words)\n",
    "    np.save('data/en_vectors.npy', en_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Embedding layer in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately, we want to take a batch of sentences, convert them to indices which are then converted to embeddings. This will be in the form of a tensor where the dimesions are:\n",
    "\n",
    "(batch_size, sequence_length, embedding dimesions)\n",
    "\n",
    "This is the shape we need for the encoder LSTM layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_embeddings = len(en_vectors)\n",
    "embedding_dim = 300\n",
    "embeds = nn.Embedding(num_embeddings, embedding_dim)  # 99996 words in vocab, 300 dimensional embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-1.1912,  0.7294, -0.3587,  ..., -0.2856,  0.4691,  1.5383],\n",
       "        [ 0.1248,  0.5451, -0.5377,  ...,  0.9908, -0.4892,  1.1032],\n",
       "        ...,\n",
       "        [-0.4209,  0.0920, -0.3235,  ..., -0.1302, -0.6792,  0.3011],\n",
       "        [-0.3881, -0.0339,  0.0141,  ...,  0.2241,  0.0517, -0.1321],\n",
       "        [-0.1806, -0.2476,  0.0756,  ..., -0.2007,  0.1597,  0.1842]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds.weight.data.copy_(torch.from_numpy(en_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1,  2,  3,  4,  5,  6,  7,  8]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "lookup_tensor = torch.tensor([[1,2,3,4],[5,6,7,8]], dtype=torch.long)\n",
    "print(lookup_tensor.shape)\n",
    "lookup_tensor.view(1, 1, -1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2,  3,  4],\n",
       "        [ 5,  6,  7,  8]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 300])\n"
     ]
    }
   ],
   "source": [
    "hello_embed = embeds(lookup_tensor)\n",
    "print(hello_embed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so we have all the pieces needed to take words and convert them into word embeddings. These word embeddings already have a lot of useful information about how words relate since we loaded the pre-trained word embeddings. Now we can build the translation model with the embedding matrices built in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Visualizing the Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# use PCA and t-SNE to reduce the dimensionality of the word embeddings to 3 dimensions\n",
    "\n",
    "# Plot 3-d representation of several word embeddings and the words in plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up PyTorch Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than organizing all the data from a file and storing it in a list or some other data structure, PyTorch allows us to create a dataset object. To get an example from a dataset we just index the dataset object like we would a list. However, all our processing can be contained in the objects initialization or indexing process.\n",
    "\n",
    "This will also make training easier when we want to iterate through batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class French2EnglishDataset(Dataset):\n",
    "    '''\n",
    "        French and associated English sentences.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, fr_sentences, en_sentences, fr_word2idx, en_word2vec, seq_length):\n",
    "        self.fr_sentences = fr_sentences\n",
    "        self.en_sentences = en_sentences\n",
    "        self.fr_word2idx = fr_word2idx\n",
    "        self.en_word2idx = en_word2idx\n",
    "        self.seq_length = seq_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(french_sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "            Returns a pair of tensors containing word indices\n",
    "            for the specified sentence pair in the dataset.\n",
    "        '''\n",
    "        \n",
    "        # init torch tensors, note that 0 is the padding index\n",
    "        french_tensor = torch.zeros(self.seq_length, dtype=torch.long)\n",
    "        english_tensor = torch.zeros(self.seq_length, dtype=torch.long)\n",
    "        \n",
    "        # Get sentence pair\n",
    "        french_sentence = self.fr_sentences[idx]\n",
    "        english_sentence = self.en_sentences[idx]\n",
    "        \n",
    "        # Add <EOS> tags\n",
    "        \n",
    "        # Load word indices\n",
    "        for i, word in enumerate(french_sentence.split()):\n",
    "            if word in fr_word2idx:\n",
    "                french_tensor[i] = fr_word2idx[word]\n",
    "            else:\n",
    "                french_tensor[i] = fr_word2idx['<unk>']\n",
    "        \n",
    "        for i, word in enumerate(english_sentence.split()):\n",
    "            if word in en_word2idx:\n",
    "                english_tensor[i] = en_word2idx[word]\n",
    "            else:\n",
    "                english_tensor[i] = en_word2idx['<unk>']\n",
    "            \n",
    "        sample = {'french_tensor': french_tensor, 'french_sentence': french_sentence,\n",
    "                  'english_tensor': english_tensor, 'english_sentence': english_sentence}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_english_dataset = French2EnglishDataset(french_sentences,\n",
    "                                               english_sentences,\n",
    "                                               fr_word2idx,\n",
    "                                               en_word2idx,\n",
    "                                               seq_length = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = french_english_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([   270,   3707,     21,    583,   6232,    219,      2,   2539,\n",
       "             4,     11,     24,     21,  52237,     16,    186,      7,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(test_sample['french_sentence'])\n",
    "test_sample['french_tensor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(french_english_dataset, batch_size=64,\n",
    "                        shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([64, 30]) torch.Size([64, 30])\n",
      "1 torch.Size([64, 30]) torch.Size([64, 30])\n",
      "2 torch.Size([64, 30]) torch.Size([64, 30])\n",
      "3 torch.Size([64, 30]) torch.Size([64, 30])\n",
      "4 torch.Size([64, 30]) torch.Size([64, 30])\n",
      "5 torch.Size([64, 30]) torch.Size([64, 30])\n",
      "6 torch.Size([64, 30]) torch.Size([64, 30])\n",
      "7 torch.Size([64, 30]) torch.Size([64, 30])\n",
      "8 torch.Size([64, 30]) torch.Size([64, 30])\n",
      "9 torch.Size([64, 30]) torch.Size([64, 30])\n",
      "10 torch.Size([64, 30]) torch.Size([64, 30])\n"
     ]
    }
   ],
   "source": [
    "for i_batch, sample_batched in enumerate(dataloader):\n",
    "    print(i_batch, sample_batched['french_tensor'].shape,\n",
    "          sample_batched['english_tensor'].shape)\n",
    "    if i_batch == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-Directional LSTM Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBiLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size, pretrained_embeddings):\n",
    "        super(EncoderBiLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_dim = pretrained_embeddings.shape[1]\n",
    "        self.vocab_size = pretrained_embeddings.shape[0]\n",
    "        self.num_layers = 1\n",
    "        self.dropout = 0\n",
    "        self.bidirectional = True\n",
    "        \n",
    "        \n",
    "        # Construct the layers\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(pretrained_embeddings))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        self.lstm = nn.LSTM(self.embedding_dim,\n",
    "                            self.hidden_size,\n",
    "                            batch_first = True,\n",
    "                            dropout=self.dropout,\n",
    "                            bidirectional=self.bidirectional)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input)\n",
    "        output = self.lstm(embedded, hidden)\n",
    "        return output\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "        \n",
    "        hidden_state = torch.zeros(self.num_layers*(2 if self.bidirectional else 1),\n",
    "                                   batch_size,\n",
    "                                   self.hidden_size, \n",
    "                                   device=device)\n",
    "        \n",
    "        cell_state = torch.zeros(self.num_layers*(2 if self.bidirectional else 1),\n",
    "                                 batch_size,\n",
    "                                 self.hidden_size, \n",
    "                                 device=device)\n",
    "        \n",
    "        return (hidden_state, cell_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final output of the BiLSTM Encoder on our test input is: \n",
      "\n",
      " torch.Size([3, 4, 6])\n",
      "\n",
      "\n",
      "Encoder output tensor: \n",
      "\n",
      " tensor([[[ 0.0160,  0.1536,  0.2898, -0.1098, -0.0247,  0.2115],\n",
      "         [-0.0121,  0.0254, -0.0687, -0.1311, -0.0263,  0.2953],\n",
      "         [-0.0301,  0.0734,  0.0342, -0.0889,  0.0229,  0.2013],\n",
      "         [-0.0101,  0.0600, -0.0423, -0.0110,  0.0847,  0.1791]],\n",
      "\n",
      "        [[-0.0742,  0.0503,  0.0917, -0.0825,  0.0028,  0.4024],\n",
      "         [-0.0110,  0.1777,  0.3906, -0.2996, -0.0360,  0.1253],\n",
      "         [-0.0031,  0.1669, -0.1167, -0.1098, -0.0638,  0.3660],\n",
      "         [ 0.0115,  0.0661,  0.1623, -0.2038,  0.1183,  0.3278]],\n",
      "\n",
      "        [[-0.0118,  0.0740,  0.0507, -0.3665,  0.0316,  0.1321],\n",
      "         [-0.0099,  0.0814, -0.0398, -0.0282,  0.0439,  0.2290],\n",
      "         [ 0.0441,  0.0661,  0.1803, -0.0055, -0.0625,  0.0833],\n",
      "         [ 0.0233,  0.0225,  0.2432,  0.1095, -0.2604,  0.0952]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Test the encoder on a sample input, input tensor has dimensions (batch_size, seq_length)\n",
    "\n",
    "batch_size = 3\n",
    "seq_length = 4\n",
    "hidden_size = 3\n",
    "encoder = EncoderBiLSTM(hidden_size, np_en_vectors).to(device)\n",
    "hidden = encoder.initHidden(batch_size)\n",
    "\n",
    "inputs = torch.randint(0, 50, (batch_size, seq_length), dtype=torch.long, device=device)\n",
    "\n",
    "encoder_output, hidden_state = encoder.forward(inputs, hidden)\n",
    "\n",
    "print(\"The final output of the BiLSTM Encoder on our test input is: \\n\\n\", encoder_output.shape)\n",
    "\n",
    "print('\\n\\nEncoder output tensor: \\n\\n', encoder_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Attention\n",
    "Let's take a moment to go over how attention is being modeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights:\n",
      " tensor([[[ 1.,  0.,  0.,  0.]],\n",
      "\n",
      "        [[ 1.,  0.,  0.,  0.]],\n",
      "\n",
      "        [[ 1.,  0.,  0.,  0.]]], device='cuda:0')\n",
      "\n",
      "Encoder Output after attention is applied: \n",
      " tensor([[ 0.0160,  0.1536,  0.2898, -0.1098, -0.0247,  0.2115],\n",
      "        [-0.0742,  0.0503,  0.0917, -0.0825,  0.0028,  0.4024],\n",
      "        [-0.0118,  0.0740,  0.0507, -0.3665,  0.0316,  0.1321]], device='cuda:0')\n",
      "\n",
      " torch.Size([3, 6])\n"
     ]
    }
   ],
   "source": [
    "# Initialize attention weights to one\n",
    "attn_weights = torch.ones((batch_size, seq_length),device=device)\n",
    "\n",
    "# Set all weights except the weights associated with the first sequence item equal to zero\n",
    "# This would represent full attention on the first word in the sequence\n",
    "attn_weights[:, 1:] = 0\n",
    "\n",
    "attn_weights.unsqueeze_(1) # Add dimension for batch matrix multiplication\n",
    "attn_applied = torch.bmm(attn_weights, encoder_output)\n",
    "attn_applied.squeeze_(1) # Remove extra dimension\n",
    "\n",
    "print('Attention weights:\\n', attn_weights)\n",
    "print('\\nEncoder Output after attention is applied: \\n', attn_applied)\n",
    "print('\\n', attn_applied.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2,  3,  1,  2,  3],\n",
       "        [ 4,  5,  6,  4,  5,  6]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((torch.tensor([[1,2,3],[4,5,6]]), torch.tensor([[1,2,3],[4,5,6]])), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Decoder with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderLSTM(nn.Module):\n",
    "    def __init__(self, decoder_hidden_size, pretrained_embeddings, seq_length, encoder_hidden_dim):\n",
    "        super(AttnDecoderLSTM, self).__init__()\n",
    "        # Embedding parameters\n",
    "        self.embedding_dim = pretrained_embeddings.shape[1]\n",
    "        self.output_vocab_size = pretrained_embeddings.shape[0]\n",
    "        \n",
    "        # LSTM parameters\n",
    "        self.decoder_hidden_size = decoder_hidden_size\n",
    "        self.num_layers = 1 # Potentially add more layers to LSTM later\n",
    "        self.dropout = 0.0 # Potentially add dropout later\n",
    "        \n",
    "        # Attention parameters\n",
    "        self.seq_length = seq_length\n",
    "        self.encoder_hidden_dim = encoder_hidden_dim\n",
    "        \n",
    "        # Construct embedding layer for output language\n",
    "        self.embedding = nn.Embedding(self.output_vocab_size, self.embedding_dim)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(pretrained_embeddings))\n",
    "        self.embedding.weight.requires_grad = False # we don't want to train the embedding weights\n",
    "        \n",
    "        # Construct layer that calculates attentional weights\n",
    "        self.attn = nn.Linear(2 * self.decoder_hidden_size + self.embedding_dim, self.seq_length)\n",
    "        \n",
    "        # Construct layer that compresses the combined matrix of the input embeddings\n",
    "        # and the encoder inputs after attention has been applied\n",
    "        self.attn_with_input = nn.Linear(self.embedding_dim + self.encoder_hidden_dim, self.embedding_dim)\n",
    "        \n",
    "        # LSTM for Decoder\n",
    "        self.lstm = nn.LSTM(self.embedding_dim,\n",
    "                            self.decoder_hidden_size,\n",
    "                            dropout=self.dropout)\n",
    "        \n",
    "        # Output layer\n",
    "        self.out = nn.Linear(self.decoder_hidden_size, self.output_vocab_size)\n",
    "    \n",
    "    def forward(self, input, hidden, encoder_output):\n",
    "        # Input word indices, should have dim(1, batch_size), output will be (1, batch_size, embedding_dim)\n",
    "        embedded = self.embedding(input)\n",
    "        \n",
    "        # Calculate Attention weights\n",
    "        attn_weights = F.softmax(self.attn(torch.cat((hidden[1][0], hidden[0][0], embedded[0]), 1)), dim=1)\n",
    "        attn_weights.unsqueeze_(1) # Add dimension for batch matrix multiplication\n",
    "        \n",
    "        # Apply Attention weights\n",
    "        attn_applied = torch.bmm(attn_weights, encoder_output)\n",
    "        attn_applied.squeeze_(1) # Remove extra dimension, dim are now (batch_size, encoder_hidden_size)\n",
    "        \n",
    "        # Prepare LSTM input tensor\n",
    "        attn_combined = torch.cat((embedded[0], attn_applied), 1) # Combine embedding input and attn_applied,\n",
    "        lstm_input = F.relu(self.attn_with_input(attn_combined)) # pass through fully connected with ReLU\n",
    "        lstm_input = lstm_input.unsqueeze_(0) # Add seq dimension so tensor has expected dimensions for lstm\n",
    "        \n",
    "        output, hidden = self.lstm(lstm_input, hidden) # Output dim = (1, batch_size, decoder_hidden_size)\n",
    "        output = F.softmax(self.out(output[0]), dim=1) # softmax over all words in vocab\n",
    "        output_idx = output.topk(1, dim=1)[1].view(1,-1) # Get the index of highest probability\n",
    "        output_embedded = self.embedding(output_idx)\n",
    "        return output_idx, output_embedded, hidden, attn_weights\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "        \n",
    "        hidden_state = torch.zeros(self.num_layers,\n",
    "                                   batch_size,\n",
    "                                   self.decoder_hidden_size, \n",
    "                                   device=device)\n",
    "        \n",
    "        cell_state = torch.zeros(self.num_layers,\n",
    "                                 batch_size,\n",
    "                                 self.decoder_hidden_size, \n",
    "                                 device=device)\n",
    "        \n",
    "        return (hidden_state, cell_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the decoder on sample inputs to check that the dimensions of everything is correct\n",
    "decoder_hidden_size = 5\n",
    "\n",
    "decoder = AttnDecoderLSTM(decoder_hidden_size, np.vstack(fr_vectors), seq_length, 2*hidden_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_idx = torch.tensor([fr_word2idx['<s>']]*batch_size, dtype=torch.long, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_idx = input_idx.unsqueeze_(0)\n",
    "decoder_hidden = decoder.initHidden(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "o, o_e, h, a = decoder.forward(input_idx, decoder_hidden, encoder_output)\n",
    "print(o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 6\n",
    "attention_input = torch.randn((batch_size, embedding_dim))\n",
    "attn = nn.Linear(embedding_dim, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7529,  0.1128,  0.8237],\n",
       "        [-0.4670,  0.9860, -0.1091],\n",
       "        [-0.0454,  0.2469, -0.2652],\n",
       "        [-0.0920, -0.6958, -0.6729]])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.t(attn(attention_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Testing LSTM in pytorch(old code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "seq_len = 5\n",
    "batch_size = 3\n",
    "embidding_dim = 10\n",
    "hidden_size = 3\n",
    "hidden_layers = 1\n",
    "inputs = torch.randn((seq_len, batch_size, embedding_dim))  # make a sequence of length 5, 1 batch, input 10dim vector\n",
    "\n",
    "# initialize the hidden state. hidden layers have 3 nodes\n",
    "hidden = (torch.randn(hidden_layers, batch_size, hidden_size),\n",
    "            torch.randn((hidden_layers, batch_size, hidden_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "lstm = nn.LSTM(embedding_dim, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "out, hidden = lstm(inputs, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-2.5325e-01,  4.9366e-02,  4.0028e-01],\n",
      "         [ 8.1580e-01, -5.9460e-01, -4.5211e-06],\n",
      "         [ 5.9945e-01,  3.4448e-04,  1.5961e-01]],\n",
      "\n",
      "        [[ 5.9029e-01,  8.7725e-04,  1.1217e-02],\n",
      "         [ 7.1056e-03, -1.5894e-04, -1.4747e-04],\n",
      "         [-2.7879e-03,  1.2264e-06, -1.4473e-02]],\n",
      "\n",
      "        [[ 1.3057e-01,  3.2360e-03,  5.4227e-01],\n",
      "         [-3.9118e-01,  7.1249e-02, -3.7470e-01],\n",
      "         [ 4.5797e-05,  5.1406e-05,  2.7413e-02]],\n",
      "\n",
      "        [[ 9.9916e-02,  4.7488e-01,  9.7807e-03],\n",
      "         [-3.8806e-01, -8.1084e-05,  7.0315e-02],\n",
      "         [ 1.5172e-01,  5.9603e-01, -1.1859e-04]],\n",
      "\n",
      "        [[ 2.7766e-01,  4.5121e-02, -5.6821e-01],\n",
      "         [-7.7895e-02, -4.1036e-01, -6.2259e-03],\n",
      "         [ 9.4735e-02,  1.3790e-04, -1.2973e-04]]])\n",
      "torch.Size([5, 3, 3])\n",
      "torch.Size([1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "# outputs of each hidden node(3) for each item in sequence(5)\n",
    "print(out)\n",
    "print(out.shape)\n",
    "\n",
    "# final hidden state and final cell state of sequence; notice that the hidden state equals the final output\n",
    "print(hidden[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "seq_len = 5\n",
    "batch_size = 1\n",
    "input_dim = 10\n",
    "hidden_size = 3\n",
    "hidden_layers = 1\n",
    "num_dir = 2 # for bidirectional lstm\n",
    "inputs = autograd.Variable(torch.randn((seq_len, batch_size, input_dim)))  # make a sequence of length 5, 1 batch, input 10dim vector\n",
    "\n",
    "# initialize the hidden state. hidden layers have 3 nodes\n",
    "hidden = (autograd.Variable(torch.randn(hidden_layers*num_dir, batch_size, hidden_size)),\n",
    "          autograd.Variable(torch.randn((hidden_layers*num_dir, batch_size, hidden_size))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(input_dim, hidden_size, bidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "out, hidden = lstm(inputs, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      " -0.1693  0.7509 -0.0828 -0.5793  0.5285  0.6648\n",
      "\n",
      "(1 ,.,.) = \n",
      "  0.0218  0.1844 -0.1121 -0.6127  0.2034  0.3170\n",
      "\n",
      "(2 ,.,.) = \n",
      "  0.0731 -0.0683  0.0742 -0.0712  0.1552  0.0725\n",
      "\n",
      "(3 ,.,.) = \n",
      "  0.0098 -0.0052 -0.1215 -0.0741  0.2897  0.0834\n",
      "\n",
      "(4 ,.,.) = \n",
      " -0.0263 -0.3019 -0.1845 -0.1061  0.4653  0.0832\n",
      "[torch.FloatTensor of size 5x1x6]\n",
      "\n",
      "(Variable containing:\n",
      "(0 ,.,.) = \n",
      " -0.0263 -0.3019 -0.1845\n",
      "\n",
      "(1 ,.,.) = \n",
      " -0.5793  0.5285  0.6648\n",
      "[torch.FloatTensor of size 2x1x3]\n",
      ", Variable containing:\n",
      "(0 ,.,.) = \n",
      " -0.3161 -0.5384 -0.4495\n",
      "\n",
      "(1 ,.,.) = \n",
      " -0.7322  0.9997  0.9236\n",
      "[torch.FloatTensor of size 2x1x3]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# outputs of each hidden node in both directions(3*2) for each item in sequence(5)\n",
    "print(out)\n",
    "\n",
    "# final hidden and cell state of model in both directions\n",
    "# notice that the first 3 output of the final item equals the final first hidden state\n",
    "# the second 3 outputs from the first item equals the final second hidden state\n",
    "print(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Training Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Walk through one training iteration with a sample input then build a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Get sample input and target\n",
    "\n",
    "# get encoder, decoder, their optimizers, and the criterion function\n",
    "\n",
    "# init hidden layer\n",
    "\n",
    "# run forward pass through encoder\n",
    "\n",
    "# pass encoder output, hidden state, and decoder input to decoder\n",
    "\n",
    "# compute loss\n",
    "\n",
    "# Back propagate the gradients\n",
    "\n",
    "# Clip the gradients\n",
    "\n",
    "# Step the optimizers\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Using the Model for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Part 5: Visualizing the Attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
