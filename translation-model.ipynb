{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Translation Model\n",
    "by Mac Brennan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we get started we will load all the packages we will need\n",
    "import os\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project we will be building a neural translation model that takes in a sentence in French and outputs a sentence in English. The model that will be used is called an encoder-decoder network. What this means is we have two neural networks:\n",
    "\n",
    "- One called the encoder, that extracts the meaning from the French sentence, representing it as a tensor of numbers.\n",
    "- One called the decoder that converts that tensor of numbers back into a sentence in English\n",
    "\n",
    "Our job is to train the encoder and decoder to learn to do this in a way such that the English sentence output by the decoder has the same meaning as the input French sentence. To give a visual understanding of what is happening, the following illustration shows the model that will be built. Don't worry if it doesn't make complete sense, the details will be explained as we go. The goal is to give you a starting point for visualizing what is happening.\n",
    "\n",
    "<p style='text-align: center !important;'>\n",
    " <img src='https://github.com/macbrennan90/macbrennan90.github.io/blob/master/images/encoder-decoder.png?raw=true'\n",
    "      alt='Translation Model Summary'>\n",
    "</p>\n",
    "\n",
    "\n",
    "This project will be broken up into several parts as follows:\n",
    "\n",
    "__Part 1:__ Preparing the words\n",
    "\n",
    "+ Dataset\n",
    "+ Word Embeddings\n",
    "\n",
    "__Part 2:__ Building the Model\n",
    "\n",
    "+ Bi-Directional LSTM Encoder\n",
    "+ Decoder with Attention\n",
    "\n",
    "__Part 3:__ Training the Model\n",
    "\n",
    "__Part 4:__ Evaluation\n",
    "\n",
    "__Part 5:__ Vizualize Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Preparing the words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset that will be used is a text file of english sentences and the corresponding french sentences.\n",
    "\n",
    "Each sentence is on a new line. The sentences will be split into a list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data\n",
    "The data will be stored in two lists where each item is a sentence. The lists are:\n",
    "+ english_sentences\n",
    "+ french_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/small_vocab_en', \"r\") as f:\n",
    "    data1 = f.read()\n",
    "with open('data/small_vocab_fr', \"r\") as f:\n",
    "    data2 = f.read()\n",
    "    \n",
    "# The data is just in a text file with each sentence on its own line\n",
    "english_sentences = data1.split('\\n')\n",
    "french_sentences = data2.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of English sentences: 137861 \n",
      "Number of French sentences: 137861 \n",
      "\n",
      "Example/Target pair:\n",
      "\n",
      "  california is usually quiet during march , and it is usually hot in june .\n",
      "  california est généralement calme en mars , et il est généralement chaud en juin .\n"
     ]
    }
   ],
   "source": [
    "print('Number of English sentences:', len(english_sentences), \n",
    "      '\\nNumber of French sentences:', len(french_sentences),'\\n')\n",
    "print('Example/Target pair:\\n')\n",
    "print('  '+english_sentences[2])\n",
    "print('  '+french_sentences[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vocabulary\n",
    "Let's take a closer look at the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['california',\n",
       " 'is',\n",
       " 'usually',\n",
       " 'quiet',\n",
       " 'during',\n",
       " 'march',\n",
       " ',',\n",
       " 'and',\n",
       " 'it',\n",
       " 'is',\n",
       " 'usually',\n",
       " 'hot',\n",
       " 'in',\n",
       " 'june',\n",
       " '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_sentences[2].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest english sentence in our dataset is: 17\n"
     ]
    }
   ],
   "source": [
    "max_en_length = 0\n",
    "for sentence in english_sentences:\n",
    "    length = len(sentence.split())\n",
    "    max_en_length = max(max_en_length, length)\n",
    "print(\"The longest english sentence in our dataset is:\", max_en_length)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest english sentence in our dataset is: 23\n"
     ]
    }
   ],
   "source": [
    "max_fr_length = 0\n",
    "for sentence in french_sentences:\n",
    "    length = len(sentence.split())\n",
    "    max_fr_length = max(max_fr_length, length)\n",
    "print(\"The longest english sentence in our dataset is:\", max_fr_length)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_word_count = {}\n",
    "fr_word_count = {}\n",
    "\n",
    "for sentence in english_sentences:\n",
    "    for word in sentence.split():\n",
    "        if word in en_word_count:\n",
    "            en_word_count[word] +=1\n",
    "        else:\n",
    "            en_word_count[word] = 1\n",
    "            \n",
    "for sentence in french_sentences:\n",
    "    for word in sentence.split():\n",
    "        if word in fr_word_count:\n",
    "            fr_word_count[word] +=1\n",
    "        else:\n",
    "            fr_word_count[word] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique English words: 227\n",
      "Number of unique French words: 355\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique English words:', len(en_word_count))\n",
    "print('Number of unique French words:', len(fr_word_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value(items_tuple):\n",
    "    return items_tuple[1]\n",
    "\n",
    "sorted_en_words= sorted(en_word_count.items(), key=get_value, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('is', 205858),\n",
       " (',', 140897),\n",
       " ('.', 129039),\n",
       " ('in', 75525),\n",
       " ('it', 75137),\n",
       " ('during', 74933),\n",
       " ('the', 67628),\n",
       " ('but', 63987),\n",
       " ('and', 59850),\n",
       " ('sometimes', 37746),\n",
       " ('usually', 37507),\n",
       " ('never', 37500),\n",
       " ('least', 27564),\n",
       " ('favorite', 27371),\n",
       " ('fruit', 27105),\n",
       " ('most', 14934),\n",
       " ('loved', 13666),\n",
       " ('liked', 13546),\n",
       " ('new', 12197),\n",
       " ('paris', 11334),\n",
       " ('india', 11277),\n",
       " ('united', 11270),\n",
       " ('states', 11270),\n",
       " ('california', 11250),\n",
       " ('jersey', 11225),\n",
       " ('france', 11170),\n",
       " ('china', 10953),\n",
       " ('he', 10786),\n",
       " ('she', 10786),\n",
       " ('grapefruit', 10118),\n",
       " ('your', 9734),\n",
       " ('my', 9700),\n",
       " ('his', 9700),\n",
       " ('her', 9700),\n",
       " ('fall', 9134),\n",
       " ('june', 9133),\n",
       " ('spring', 9102),\n",
       " ('january', 9090),\n",
       " ('winter', 9038),\n",
       " ('march', 9023),\n",
       " ('autumn', 9004),\n",
       " ('may', 8995),\n",
       " ('nice', 8984),\n",
       " ('september', 8958),\n",
       " ('july', 8956),\n",
       " ('april', 8954),\n",
       " ('november', 8951),\n",
       " ('summer', 8948),\n",
       " ('december', 8945),\n",
       " ('february', 8942),\n",
       " ('our', 8932),\n",
       " ('their', 8932),\n",
       " ('freezing', 8928),\n",
       " ('pleasant', 8916),\n",
       " ('beautiful', 8915),\n",
       " ('october', 8910),\n",
       " ('snowy', 8898),\n",
       " ('warm', 8890),\n",
       " ('cold', 8878),\n",
       " ('wonderful', 8808),\n",
       " ('dry', 8794),\n",
       " ('busy', 8791),\n",
       " ('august', 8789),\n",
       " ('chilly', 8770),\n",
       " ('rainy', 8761),\n",
       " ('mild', 8743),\n",
       " ('wet', 8726),\n",
       " ('relaxing', 8696),\n",
       " ('quiet', 8693),\n",
       " ('hot', 8639),\n",
       " ('dislikes', 7314),\n",
       " ('likes', 7314),\n",
       " ('limes', 5554),\n",
       " ('mangoes', 5549),\n",
       " ('lemons', 5533),\n",
       " ('grapes', 5525),\n",
       " ('apples', 5452),\n",
       " ('oranges', 5452),\n",
       " ('strawberries', 5452),\n",
       " ('bananas', 5452),\n",
       " ('peaches', 5451),\n",
       " ('pears', 5451),\n",
       " ('to', 5166),\n",
       " ('strawberry', 4715),\n",
       " ('grape', 4703),\n",
       " ('lime', 4680),\n",
       " ('apple', 4652),\n",
       " ('lemon', 4652),\n",
       " ('banana', 4652),\n",
       " ('mango', 4652),\n",
       " ('pear', 4652),\n",
       " ('peach', 4652),\n",
       " ('orange', 4651),\n",
       " ('like', 4588),\n",
       " ('dislike', 4444),\n",
       " ('they', 3222),\n",
       " ('that', 2712),\n",
       " ('i', 2664),\n",
       " ('we', 2532),\n",
       " ('you', 2414),\n",
       " ('animal', 2304),\n",
       " ('a', 1944),\n",
       " ('truck', 1944),\n",
       " ('car', 1944),\n",
       " ('automobile', 1944),\n",
       " ('was', 1867),\n",
       " ('next', 1666),\n",
       " ('go', 1386),\n",
       " ('driving', 1296),\n",
       " ('visit', 1224),\n",
       " ('little', 1016),\n",
       " ('big', 1016),\n",
       " ('old', 972),\n",
       " ('yellow', 972),\n",
       " ('red', 972),\n",
       " ('rusty', 972),\n",
       " ('blue', 972),\n",
       " ('white', 972),\n",
       " ('black', 972),\n",
       " ('green', 972),\n",
       " ('shiny', 972),\n",
       " ('favorite.', 961),\n",
       " ('are', 870),\n",
       " ('?', 811),\n",
       " ('last', 781),\n",
       " ('feared', 768),\n",
       " ('animals', 768),\n",
       " ('this', 768),\n",
       " ('plan', 714),\n",
       " ('going', 666),\n",
       " ('saw', 648),\n",
       " ('disliked', 648),\n",
       " ('drives', 648),\n",
       " ('drove', 648),\n",
       " ('grapefruit.', 574),\n",
       " ('between', 540),\n",
       " ('liked.', 500),\n",
       " ('loved.', 500),\n",
       " ('translate', 480),\n",
       " ('plans', 476),\n",
       " ('peaches.', 393),\n",
       " ('pears.', 393),\n",
       " ('bananas.', 392),\n",
       " ('oranges.', 392),\n",
       " ('apples.', 392),\n",
       " ('strawberries.', 392),\n",
       " ('were', 384),\n",
       " ('went', 378),\n",
       " ('might', 378),\n",
       " ('wanted', 378),\n",
       " ('thinks', 360),\n",
       " ('grapes.', 319),\n",
       " ('spanish', 312),\n",
       " ('portuguese', 312),\n",
       " ('chinese', 312),\n",
       " ('english', 312),\n",
       " ('french', 312),\n",
       " ('lemons.', 311),\n",
       " ('translating', 300),\n",
       " ('mangoes.', 295),\n",
       " ('limes.', 290),\n",
       " ('difficult', 260),\n",
       " ('fun', 260),\n",
       " ('easy', 260),\n",
       " ('wants', 252),\n",
       " ('think', 240),\n",
       " ('why', 240),\n",
       " (\"it's\", 240),\n",
       " ('did', 204),\n",
       " ('orange.', 197),\n",
       " ('mango.', 196),\n",
       " ('banana.', 196),\n",
       " ('peach.', 196),\n",
       " ('lemon.', 196),\n",
       " ('pear.', 196),\n",
       " ('apple.', 196),\n",
       " ('cat', 192),\n",
       " ('shark', 192),\n",
       " ('bird', 192),\n",
       " ('mouse', 192),\n",
       " ('horse', 192),\n",
       " ('elephant', 192),\n",
       " ('dog', 192),\n",
       " ('monkey', 192),\n",
       " ('lion', 192),\n",
       " ('bear', 192),\n",
       " ('rabbit', 192),\n",
       " ('snake', 192),\n",
       " ('lime.', 168),\n",
       " ('grape.', 145),\n",
       " ('when', 144),\n",
       " ('strawberry.', 133),\n",
       " ('want', 126),\n",
       " ('fruit.', 87),\n",
       " ('do', 84),\n",
       " ('how', 67),\n",
       " ('elephants', 64),\n",
       " ('horses', 64),\n",
       " ('dogs', 64),\n",
       " ('sharks', 64),\n",
       " ('snakes', 64),\n",
       " ('cats', 64),\n",
       " ('rabbits', 64),\n",
       " ('monkeys', 64),\n",
       " ('bears', 64),\n",
       " ('birds', 64),\n",
       " ('lions', 64),\n",
       " ('mice', 64),\n",
       " (\"didn't\", 60),\n",
       " ('eiffel', 57),\n",
       " ('tower', 57),\n",
       " ('grocery', 57),\n",
       " ('store', 57),\n",
       " ('football', 57),\n",
       " ('field', 57),\n",
       " ('lake', 57),\n",
       " ('school', 57),\n",
       " ('would', 48),\n",
       " (\"aren't\", 36),\n",
       " ('been', 36),\n",
       " ('weather', 33),\n",
       " ('does', 24),\n",
       " ('has', 24),\n",
       " (\"isn't\", 24),\n",
       " ('am', 24),\n",
       " ('where', 12),\n",
       " ('have', 12)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_en_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_fr_words = sorted(fr_word_count.items(), key=get_value, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('est', 196809),\n",
       " ('.', 135619),\n",
       " (',', 123135),\n",
       " ('en', 105768),\n",
       " ('il', 84079),\n",
       " ('les', 65255),\n",
       " ('mais', 63987),\n",
       " ('et', 59851),\n",
       " ('la', 49861),\n",
       " ('parfois', 37746),\n",
       " ('jamais', 37215),\n",
       " ('le', 35306),\n",
       " (\"l'\", 32917),\n",
       " ('généralement', 31292),\n",
       " ('moins', 27557),\n",
       " ('au', 25738),\n",
       " ('aimé', 24842),\n",
       " ('fruit', 23626),\n",
       " ('préféré', 22886),\n",
       " ('agréable', 17751),\n",
       " ('froid', 16794),\n",
       " ('son', 16496),\n",
       " ('chaud', 16405),\n",
       " ('de', 15070),\n",
       " ('plus', 14934),\n",
       " ('automne', 14727),\n",
       " ('mois', 14350),\n",
       " ('à', 13870),\n",
       " ('elle', 12056),\n",
       " ('citrons', 11679),\n",
       " ('paris', 11334),\n",
       " ('inde', 11277),\n",
       " ('états-unis', 11210),\n",
       " ('france', 11170),\n",
       " ('jersey', 11052),\n",
       " ('new', 11047),\n",
       " ('chine', 10936),\n",
       " ('pendant', 10741),\n",
       " ('pamplemousse', 10140),\n",
       " ('mon', 9403),\n",
       " ('votre', 9368),\n",
       " ('juin', 9133),\n",
       " ('printemps', 9100),\n",
       " ('janvier', 9090),\n",
       " ('hiver', 9038),\n",
       " ('mars', 9023),\n",
       " ('été', 8999),\n",
       " ('mai', 8995),\n",
       " ('septembre', 8958),\n",
       " ('juillet', 8956),\n",
       " ('avril', 8954),\n",
       " ('novembre', 8951),\n",
       " ('décembre', 8945),\n",
       " ('février', 8942),\n",
       " ('octobre', 8911),\n",
       " ('aime', 8870),\n",
       " ('août', 8789),\n",
       " ('merveilleux', 8704),\n",
       " ('relaxant', 8458),\n",
       " ('doux', 8458),\n",
       " ('humide', 8446),\n",
       " ('notre', 8319),\n",
       " ('californie', 8189),\n",
       " ('sec', 7957),\n",
       " ('leur', 7855),\n",
       " ('occupé', 7782),\n",
       " ('pluvieux', 7658),\n",
       " ('calme', 7256),\n",
       " ('beau', 6387),\n",
       " ('habituellement', 6215),\n",
       " ('pommes', 5844),\n",
       " ('pêches', 5844),\n",
       " ('oranges', 5844),\n",
       " ('poires', 5844),\n",
       " ('fraises', 5844),\n",
       " ('bananes', 5844),\n",
       " ('verts', 5835),\n",
       " ('raisins', 5780),\n",
       " ('mangues', 5774),\n",
       " (\"d'\", 5100),\n",
       " ('mangue', 4899),\n",
       " ('gel', 4886),\n",
       " ('raisin', 4852),\n",
       " ('pomme', 4848),\n",
       " (\"l'orange\", 4848),\n",
       " ('citron', 4848),\n",
       " ('chaux', 4848),\n",
       " ('banane', 4848),\n",
       " ('poire', 4848),\n",
       " ('fraise', 4848),\n",
       " ('pêche', 4848),\n",
       " ('pas', 4495),\n",
       " ('enneigée', 4008),\n",
       " ('favori', 3857),\n",
       " ('déteste', 3743),\n",
       " ('gèle', 3622),\n",
       " ('fruits', 3566),\n",
       " ('voiture', 3510),\n",
       " (\"l'automne\", 3411),\n",
       " ('ils', 3185),\n",
       " (\"n'aime\", 3131),\n",
       " ('california', 3061),\n",
       " ('neige', 3016),\n",
       " ('fait', 2916),\n",
       " ('belle', 2726),\n",
       " ('ne', 2715),\n",
       " ('nous', 2520),\n",
       " ('vous', 2517),\n",
       " ('des', 2435),\n",
       " ('animal', 2248),\n",
       " ('camion', 1944),\n",
       " ('cours', 1927),\n",
       " ('neigeux', 1867),\n",
       " ('conduit', 1706),\n",
       " ('prochain', 1666),\n",
       " ('je', 1548),\n",
       " ('ce', 1465),\n",
       " ('tranquille', 1437),\n",
       " ('a', 1356),\n",
       " ('cher', 1308),\n",
       " ('une', 1278),\n",
       " ('cette', 1239),\n",
       " ('était', 1198),\n",
       " ('aller', 1180),\n",
       " ('chaude', 1124),\n",
       " ('aiment', 1116),\n",
       " ('aimons', 1111),\n",
       " (\"n'aiment\", 1111),\n",
       " (\"n'aimez\", 1094),\n",
       " ('leurs', 1072),\n",
       " ('aimez', 1053),\n",
       " ('sont', 1018),\n",
       " ('aimé.', 1010),\n",
       " ('détestons', 1001),\n",
       " ('jaune', 972),\n",
       " ('rouge', 972),\n",
       " (\"j'aime\", 966),\n",
       " ('visiter', 908),\n",
       " ('sèche', 837),\n",
       " ('occupée', 836),\n",
       " ('frisquet', 834),\n",
       " ('?', 811),\n",
       " ('préférée', 770),\n",
       " ('animaux', 768),\n",
       " ('dernier', 757),\n",
       " ('aimait', 707),\n",
       " ('un', 698),\n",
       " ('conduisait', 673),\n",
       " ('que', 667),\n",
       " ('nouvelle', 648),\n",
       " ('vieille', 647),\n",
       " ('vu', 645),\n",
       " ('verte', 628),\n",
       " ('petite', 615),\n",
       " ('nos', 613),\n",
       " ('noire', 602),\n",
       " ('brillant', 587),\n",
       " ('blanche', 579),\n",
       " ('redouté', 576),\n",
       " ('pleut', 562),\n",
       " (\"n'aimait\", 561),\n",
       " ('pamplemousses', 552),\n",
       " ('pense', 540),\n",
       " ('entre', 540),\n",
       " ('bleue', 504),\n",
       " ('nouveau', 502),\n",
       " ('traduire', 501),\n",
       " ('rouillée', 486),\n",
       " ('bleu', 468),\n",
       " ('se', 461),\n",
       " ('grande', 459),\n",
       " ('rouillé', 454),\n",
       " ('préféré.', 419),\n",
       " ('ses', 402),\n",
       " (\"qu'il\", 393),\n",
       " ('blanc', 393),\n",
       " ('aux', 392),\n",
       " ('brillante', 385),\n",
       " ('préférés', 383),\n",
       " ('noir', 370),\n",
       " ('pluies', 367),\n",
       " ('envisage', 360),\n",
       " ('étaient', 357),\n",
       " ('va', 355),\n",
       " ('rendre', 350),\n",
       " ('vert', 344),\n",
       " ('-', 328),\n",
       " ('vieux', 325),\n",
       " ('petit', 324),\n",
       " ('espagnol', 312),\n",
       " ('portugais', 312),\n",
       " ('chinois', 312),\n",
       " ('anglais', 312),\n",
       " ('français', 312),\n",
       " ('glaciales', 307),\n",
       " ('mes', 297),\n",
       " ('cet', 286),\n",
       " ('automobile', 278),\n",
       " ('traduction', 277),\n",
       " ('mouillé', 273),\n",
       " ('difficile', 260),\n",
       " ('amusant', 260),\n",
       " ('facile', 260),\n",
       " ('comme', 259),\n",
       " ('gros', 258),\n",
       " ('souris', 256),\n",
       " ('pourrait', 252),\n",
       " ('voulait', 252),\n",
       " ('veut', 252),\n",
       " ('pourquoi', 240),\n",
       " ('aimés', 237),\n",
       " ('prévois', 233),\n",
       " ('prévoyons', 232),\n",
       " ('vos', 225),\n",
       " ('intention', 206),\n",
       " ('clémentes', 200),\n",
       " ('ont', 194),\n",
       " ('chat', 192),\n",
       " ('requin', 192),\n",
       " ('cheval', 192),\n",
       " ('chien', 192),\n",
       " ('singe', 192),\n",
       " ('lion', 192),\n",
       " ('ours', 192),\n",
       " ('lapin', 192),\n",
       " ('serpent', 192),\n",
       " ('redoutés', 190),\n",
       " ('allé', 187),\n",
       " ('grosse', 185),\n",
       " ('pluie', 174),\n",
       " ('trop', 173),\n",
       " ('monde', 173),\n",
       " ('maillot', 173),\n",
       " ('vont', 168),\n",
       " ('volant', 165),\n",
       " ('avez', 162),\n",
       " ('i', 150),\n",
       " ('allés', 150),\n",
       " ('allée', 150),\n",
       " ('quand', 144),\n",
       " ('oiseau', 128),\n",
       " ('éléphant', 128),\n",
       " ('pourraient', 126),\n",
       " ('voulaient', 126),\n",
       " ('veulent', 126),\n",
       " ('détendre', 111),\n",
       " ('aimée', 105),\n",
       " ('magnifique', 104),\n",
       " (\"l'automobile\", 100),\n",
       " (\"n'aimons\", 97),\n",
       " ('-ce', 95),\n",
       " ('gelé', 94),\n",
       " ('détestait', 87),\n",
       " ('grand', 81),\n",
       " ('bien', 77),\n",
       " ('vers', 76),\n",
       " ('prévoient', 75),\n",
       " ('prévoit', 75),\n",
       " ('lui', 70),\n",
       " ('visite', 68),\n",
       " ('comment', 67),\n",
       " ('éléphants', 64),\n",
       " ('chevaux', 64),\n",
       " ('chiens', 64),\n",
       " (\"l'éléphant\", 64),\n",
       " (\"l'oiseau\", 64),\n",
       " ('requins', 64),\n",
       " (\"l'ours\", 64),\n",
       " ('serpents', 64),\n",
       " ('chats', 64),\n",
       " ('lapins', 64),\n",
       " ('singes', 64),\n",
       " ('oiseaux', 64),\n",
       " ('lions', 64),\n",
       " ('légère', 63),\n",
       " ('cépage', 60),\n",
       " ('pensez', 60),\n",
       " ('États-unis', 57),\n",
       " ('tour', 57),\n",
       " ('eiffel', 57),\n",
       " (\"l'épicerie\", 57),\n",
       " ('terrain', 57),\n",
       " ('football', 57),\n",
       " ('lac', 57),\n",
       " (\"l'école\", 57),\n",
       " (\"l'animal\", 56),\n",
       " (\"n'est\", 47),\n",
       " ('allons', 45),\n",
       " ('allez', 45),\n",
       " ('peu', 41),\n",
       " ('pousse', 41),\n",
       " ('du', 39),\n",
       " ('-il', 36),\n",
       " ('temps', 33),\n",
       " ('at', 32),\n",
       " ('rouille', 32),\n",
       " ('sur', 28),\n",
       " (\"qu'elle\", 26),\n",
       " ('-ils', 26),\n",
       " ('petites', 26),\n",
       " ('-elle', 24),\n",
       " ('dernière', 24),\n",
       " ('êtes-vous', 24),\n",
       " ('vais', 24),\n",
       " ('voudrait', 24),\n",
       " ('proches', 20),\n",
       " ('frais', 20),\n",
       " ('manguiers', 19),\n",
       " ('avons', 19),\n",
       " ('t', 18),\n",
       " ('porcelaine', 17),\n",
       " ('détestez', 17),\n",
       " (\"c'est\", 17),\n",
       " ('grandes', 16),\n",
       " ('préférées', 16),\n",
       " ('douce', 14),\n",
       " ('durant', 14),\n",
       " ('congélation', 14),\n",
       " ('plaît', 13),\n",
       " ('où', 12),\n",
       " ('dans', 12),\n",
       " ('est-ce', 12),\n",
       " ('voulez', 12),\n",
       " ('aimeraient', 12),\n",
       " (\"n'a\", 12),\n",
       " ('petits', 10),\n",
       " ('aiment-ils', 10),\n",
       " ('grands', 9),\n",
       " ('limes', 9),\n",
       " ('envisagent', 9),\n",
       " ('grosses', 8),\n",
       " ('bénigne', 8),\n",
       " ('mouillée', 7),\n",
       " ('enneigé', 7),\n",
       " ('moindres', 7),\n",
       " ('conduite', 6),\n",
       " ('gelés', 5),\n",
       " ('tout', 4),\n",
       " ('etats-unis', 3),\n",
       " (\"n'êtes\", 3),\n",
       " ('vit', 3),\n",
       " ('ressort', 2),\n",
       " ('détend', 2),\n",
       " ('redoutée', 2),\n",
       " ('qui', 2),\n",
       " ('traduis', 2),\n",
       " ('apprécié', 2),\n",
       " ('allions', 1),\n",
       " ('trouvé', 1),\n",
       " ('as-tu', 1),\n",
       " ('faire', 1),\n",
       " ('favoris', 1),\n",
       " ('souvent', 1),\n",
       " ('es-tu', 1),\n",
       " ('moteur', 1)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_fr_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the dataset is pretty small, we may want to get a bigger data set, but we'll see how this one does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are building an embedding matrix of pretrained word vectors. The word embeddings used here were downloaded from the fastText repository. These embeddings have 300 dimensions. To start we will add a few token embeddings for our specific case. We want a token to signal the start of the sentence, A token for words that we do not have an embedding for, and a token to pad sentences so all the sentences we use have the same length. This will allow us to train the model on batches of sentences rather than one at a time.\n",
    "\n",
    "After this step we will have a dictionary and an embedding matrix for each language. The dictionary will map words to an index value in the embedding matrix where its' corresponding embedding vector is stored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For the English data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hemophilia [ 0.16189  -0.056121 -0.6556    0.21569  -0.11878  -0.02066   0.37613\n",
      " -0.24117  -0.098989 -0.010058]\n"
     ]
    }
   ],
   "source": [
    "# make a dict with the top 100,000 words\n",
    "en_words = ['<pad>', # Padding Token\n",
    "            '<s>', # Start of sentence token\n",
    "            '<unk>'# Unknown word token\n",
    "           ]\n",
    "\n",
    "en_vectors = list(np.random.randn(3, 300))\n",
    "en_vectors[0] *= 0 # make the padding vector zeros\n",
    "\n",
    "with open('data/wiki.en.vec', \"r\") as f:\n",
    "    f.readline()\n",
    "    for _ in range(100000):\n",
    "        en_vecs = f.readline()\n",
    "        word = en_vecs.split()[0]\n",
    "        vector = np.float32(en_vecs.split()[1:])\n",
    "        \n",
    "        # skip lines that don't have 300 dim\n",
    "        if len(vector) != 300:\n",
    "            continue\n",
    "        \n",
    "        if word not in en_words:\n",
    "            en_words.append(word)\n",
    "            en_vectors.append(vector)\n",
    "    print(word, vector[:10]) # Last word embedding read from the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_word2idx = {word:index for index, word in enumerate(en_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index for word hemophilia: 99996 \n",
      "vector for word hemophilia:\n",
      " [ 0.16189  -0.056121 -0.6556    0.21569  -0.11878  -0.02066   0.37613\n",
      " -0.24117  -0.098989 -0.010058]\n"
     ]
    }
   ],
   "source": [
    "hemophilia_idx = en_word2idx['hemophilia']\n",
    "print('index for word hemophilia:', hemophilia_idx, \n",
    "      '\\nvector for word hemophilia:\\n',en_vectors[hemophilia_idx][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word embedding for hemophilia matches the one read from the file, so it looks like everything worked properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For the Frech data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chabeuil [-0.18058  -0.24758   0.075607  0.173     0.24116  -0.11223  -0.28173\n",
      "  0.27374   0.37997   0.48009 ]\n"
     ]
    }
   ],
   "source": [
    "# make a dict with the top 100,000 words\n",
    "fr_words = ['<pad>',\n",
    "            '<s>',\n",
    "            '<unk>']\n",
    "\n",
    "fr_vectors = list(np.random.randn(3, 300))\n",
    "fr_vectors[0] = np.zeros(300) # make the padding vector zeros\n",
    "\n",
    "with open('data/wiki.fr.vec', \"r\") as f:\n",
    "    f.readline()\n",
    "    for _ in range(100000):\n",
    "        fr_vecs = f.readline()\n",
    "        word = fr_vecs.split()[0]\n",
    "        try:\n",
    "            vector = np.float32(fr_vecs.split()[1:])\n",
    "        except ValueError:\n",
    "            continue\n",
    "            \n",
    "         # skip lines that don't have 300 dim\n",
    "        if len(vector) != 300:\n",
    "            continue\n",
    "        \n",
    "        if word not in fr_words:\n",
    "            fr_words.append(word)\n",
    "            fr_vectors.append(vector)\n",
    "    print(word, vector[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_word2idx = {word:index for index, word in enumerate(fr_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index for word chabeuil: 99783 \n",
      "vector for word chabeuil:\n",
      " [-0.18058  -0.24758   0.075607  0.173     0.24116  -0.11223  -0.28173\n",
      "  0.27374   0.37997   0.48009 ]\n"
     ]
    }
   ],
   "source": [
    "chabeuil_idx = fr_word2idx['chabeuil']\n",
    "print('index for word chabeuil:', chabeuil_idx, \n",
    "      '\\nvector for word chabeuil:\\n',fr_vectors[chabeuil_idx][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word embedding for chabeuil matches as well so everything worked correctly for the french vocab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example Embedding layer in PyTorch\n",
    "Ultimately, we want to take a batch of sentences, convert them to indices which are then converted to embeddings. This will be in the form of a tensor where the dimesions are:\n",
    "\n",
    "(sequence length, batch size, embedding dimesions)\n",
    "\n",
    "This is the shape expected by the LSTM layer of the Encoder Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_embeddings = len(en_vectors)\n",
    "embedding_dim = 300\n",
    "embeds = nn.Embedding(num_embeddings, embedding_dim)  # 100001 words in vocab, 300 dimensional embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_en_vectors = np.vstack(en_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0000,  0.0000, -0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 1.2913, -0.3440,  0.1565,  ...,  0.7169,  0.4956, -2.3075],\n",
       "        [ 0.6535,  0.5977,  0.2801,  ...,  0.1021, -0.0557, -0.4644],\n",
       "        ...,\n",
       "        [-0.1793,  0.2280, -0.2962,  ...,  0.1251,  0.6385, -0.2883],\n",
       "        [-0.3869,  0.2369, -0.0565,  ...,  0.0601,  0.2860,  0.6931],\n",
       "        [ 0.1619, -0.0561, -0.6556,  ..., -0.0777,  0.0579, -0.4724]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds.weight.data.copy_(torch.from_numpy(np_en_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1,  2,  3,  4,  5,  6,  7,  8]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup_tensor = torch.tensor([[1,2,3,4],[5,6,7,8]], dtype=torch.long)\n",
    "print(lookup_tensor.shape)\n",
    "lookup_tensor.view(1, 1, -1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2,  3,  4],\n",
       "        [ 5,  6,  7,  8]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 300])\n"
     ]
    }
   ],
   "source": [
    "hello_embed = embeds(lookup_tensor)\n",
    "print(hello_embed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so we have all the pieces needed to take words and convert them into word embeddings. These word embeddings already have a lot of useful information about how words relate since we loaded the pre-trained word embeddings. Now we can build the translation model with the embedding matrices built in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-Directional LSTM Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBiLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size, pretrained_embeddings):\n",
    "        super(EncoderBiLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_dim = pretrained_embeddings.shape[1]\n",
    "        self.vocab_size = pretrained_embeddings.shape[0]\n",
    "        self.num_layers = 1\n",
    "        self.dropout = 0\n",
    "        self.bidirectional = True\n",
    "        \n",
    "        \n",
    "        # Construct the layers\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(pretrained_embeddings))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        self.lstm = nn.LSTM(self.embedding_dim,\n",
    "                            self.hidden_size,\n",
    "                            dropout=self.dropout,\n",
    "                            bidirectional=self.bidirectional)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input)\n",
    "        output = self.lstm(embedded, hidden)\n",
    "        return output\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "        \n",
    "        hidden_state = torch.zeros(self.num_layers*(2 if self.bidirectional else 1),\n",
    "                                   batch_size,\n",
    "                                   self.hidden_size, \n",
    "                                   device=device)\n",
    "        \n",
    "        cell_state = torch.zeros(self.num_layers*(2 if self.bidirectional else 1),\n",
    "                                 batch_size,\n",
    "                                 self.hidden_size, \n",
    "                                 device=device)\n",
    "        \n",
    "        return (hidden_state, cell_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final output of the BiLSTM Encoder is: \n",
      "\n",
      " torch.Size([4, 3, 6])\n"
     ]
    }
   ],
   "source": [
    "# Test the encoder on a sample input, input tensor has dimensions (sequence_length, batch_size)\n",
    "\n",
    "batch_size = 3\n",
    "seq_length = 4\n",
    "hidden_size = 3\n",
    "encoder = EncoderBiLSTM(hidden_size, np_en_vectors)\n",
    "encoder.to(device)\n",
    "\n",
    "hidden = encoder.initHidden(batch_size)\n",
    "inputs = torch.randint(0, 50, (seq_length, batch_size), dtype=torch.long, device=device)\n",
    "\n",
    "context, hidden_state = encoder.forward(inputs, hidden)\n",
    "\n",
    "print(\"The final output of the BiLSTM Encoder is: \\n\\n\",context.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the output still has a sequence length of 4 and a batch size of 3. The BiLSTM outputs a value from each hidden node\n",
    "twice (once in each direction) and concatenates them to give 6 output values for each sequence/batch item.\n",
    "\n",
    "This output tensor represents the context for the input sentence. The decoder network will scan this tensor when generating the output sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1716,  0.3041,  0.0727,  0.2710,  0.5424, -0.0902],\n",
      "         [ 0.3359,  0.0791,  0.0660,  0.2077,  0.5786, -0.3195],\n",
      "         [ 0.2293,  0.2636, -0.2116,  0.0644,  0.4420, -0.1041]],\n",
      "\n",
      "        [[ 0.4976,  0.3006,  0.0712,  0.1986,  0.5871, -0.4084],\n",
      "         [ 0.5360,  0.4418, -0.0443,  0.2865,  0.1685, -0.1923],\n",
      "         [ 0.5103,  0.2843, -0.0728,  0.1470,  0.4493, -0.4269]],\n",
      "\n",
      "        [[ 0.3212,  0.4243, -0.0966,  0.1773,  0.3163, -0.3653],\n",
      "         [ 0.4594,  0.4607, -0.2657,  0.0782,  0.0822, -0.2024],\n",
      "         [-0.5598, -0.0426, -0.0030,  0.0004, -0.0050, -0.0579]],\n",
      "\n",
      "        [[ 0.8241,  0.0178, -0.0005, -0.0001,  0.0001, -0.4976],\n",
      "         [ 0.5656,  0.2699,  0.1018, -0.0439,  0.3380, -0.1443],\n",
      "         [-0.2454, -0.0080, -0.2131, -0.1138,  0.2160, -0.0424]]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1716,  0.3041,  0.0727,  0.2710,  0.5424, -0.0902], device='cuda:0')"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(context)\n",
    "context[0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention\n",
    "The idea behind attention is that the encoder output will be weighted based on the input to the decoder and the previous hidden state. This gives the effect of being able to focus in on a specific part of the encoder context. This weighted context is then passed to the decoder along with the input and the hidden state where it computes the next item in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of context tensor: torch.Size([4, 3, 6])\n",
      "Shape of attention weight matrix: torch.Size([4, 3]) \n",
      "\n",
      "Note that the attn_applied tensor is the same shape as the context tensor as expected.\n",
      "Shape of attn_applied tensor: torch.Size([4, 3, 6]) \n",
      "\n",
      "Encoder output after attention weights have been applied:\n",
      "\n",
      " tensor([[[ 0.0564,  0.1000,  0.0239,  0.0891,  0.1784, -0.0297],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000],\n",
      "         [-0.1394, -0.1603,  0.1287, -0.0392, -0.2688,  0.0633]],\n",
      "\n",
      "        [[ 0.1668,  0.1007,  0.0239,  0.0666,  0.1968, -0.1369],\n",
      "         [-0.7007, -0.5777,  0.0579, -0.3745, -0.2203,  0.2513],\n",
      "         [-0.3599, -0.2005,  0.0513, -0.1037, -0.3168,  0.3010]],\n",
      "\n",
      "        [[ 0.2691,  0.3554, -0.0809,  0.1485,  0.2650, -0.3060],\n",
      "         [ 0.7113,  0.7133, -0.4114,  0.1211,  0.1273, -0.3135],\n",
      "         [ 0.3682,  0.0280,  0.0020, -0.0003,  0.0033,  0.0381]],\n",
      "\n",
      "        [[ 1.4803,  0.0320, -0.0009, -0.0002,  0.0001, -0.8938],\n",
      "         [-0.1229, -0.0586, -0.0221,  0.0095, -0.0734,  0.0313],\n",
      "         [-0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.randn((seq_length, batch_size),device=device)\n",
    "print('Shape of context tensor:', context.shape)\n",
    "print('Shape of attention weight matrix:',attn_weights.shape, '\\n')\n",
    "\n",
    "# WE have a random set of attn weights, lets set the weight for the 1st sequence \n",
    "# item in the second batch example to 0\n",
    "attn_weights[0][1] = 0\n",
    "\n",
    "# Here we set the weights of the 4th seq item of the 3 example to zero\n",
    "attn_weights[3][2] = 0\n",
    "\n",
    "attn_applied = torch.mul(context, torch.unsqueeze(attn_weights, 2))\n",
    "print('Note that the attn_applied tensor is the same shape as the context tensor as expected.')\n",
    "print('Shape of attn_applied tensor:', attn_applied.shape, '\\n')\n",
    "\n",
    "print('Encoder output after attention weights have been applied:\\n\\n', attn_applied)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the attention weights have been applied, we can see that the encoder output values associated with \n",
    "the 1st sequence item of the 2nd batch example and the 4th sequence item of the 3rd batch example \n",
    "are set to zero as expected when we set the attention weights associated with those positions to zero.\n",
    "\n",
    "So, we know the shape of the attention weight matrix and how to apply it to the encoder output. We now need to generate that attention weight matrix for the batch. To do this we will use a fully connected layer that takes in the decoder input(previous decoder output or target for that sequence step) and the previous hidden state and cell state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0564,  0.1000,  0.0239,  0.0891,  0.1784, -0.0297],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000],\n",
       "         [-0.1394, -0.1603,  0.1287, -0.0392, -0.2688,  0.0633]],\n",
       "\n",
       "        [[ 0.1668,  0.1007,  0.0239,  0.0666,  0.1968, -0.1369],\n",
       "         [-0.7007, -0.5777,  0.0579, -0.3745, -0.2203,  0.2513],\n",
       "         [-0.3599, -0.2005,  0.0513, -0.1037, -0.3168,  0.3010]],\n",
       "\n",
       "        [[ 0.2691,  0.3554, -0.0809,  0.1485,  0.2650, -0.3060],\n",
       "         [ 0.7113,  0.7133, -0.4114,  0.1211,  0.1273, -0.3135],\n",
       "         [ 0.3682,  0.0280,  0.0020, -0.0003,  0.0033,  0.0381]],\n",
       "\n",
       "        [[ 1.4803,  0.0320, -0.0009, -0.0002,  0.0001, -0.8938],\n",
       "         [-0.1229, -0.0586, -0.0221,  0.0095, -0.0734,  0.0313],\n",
       "         [-0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000]]], device='cuda:0')"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_applied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Decoder with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size, pretrained_embeddings, encoder_output_length):\n",
    "        super(AttnDecoderLSTM, self).__init__()\n",
    "        self.num_layers = 1\n",
    "        self.hidden_size = hidden_size\n",
    "        self.encoder_output_length = encoder_output_length\n",
    "        self.embedding_dim = pretrained_embeddings.shape[1]\n",
    "        self.output_vocab_size = pretrained_embeddings.shape[0]\n",
    "        \n",
    "        # Embedding layer for output language\n",
    "        self.embedding = nn.Embedding(self.output_vocab_size, self.embedding_dim)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(pretrained_embeddings))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        # Attention layer, fully connected layer\n",
    "        self.attn = nn.Linear(self.hidden_size + self.embedding_dim, self.encoder_output_length)\n",
    "        self\n",
    "    \n",
    "    def forward(self, input, hidden, encoder_output):\n",
    "        pass\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "        \n",
    "        hidden_state = torch.zeros(self.num_layers,\n",
    "                                   batch_size,\n",
    "                                   self.hidden_size, \n",
    "                                   device=device)\n",
    "        \n",
    "        cell_state = torch.zeros(self.num_layers,\n",
    "                                 batch_size,\n",
    "                                 self.hidden_size, \n",
    "                                 device=device)\n",
    "        \n",
    "        return (hidden_state, cell_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 6\n",
    "attention_input = torch.randn((batch_size, embedding_dim))\n",
    "attn = nn.Linear(embedding_dim, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7529,  0.1128,  0.8237],\n",
       "        [-0.4670,  0.9860, -0.1091],\n",
       "        [-0.0454,  0.2469, -0.2652],\n",
       "        [-0.0920, -0.6958, -0.6729]])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.t(attn(attention_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing LSTM in pytorch(old code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 5\n",
    "batch_size = 3\n",
    "embidding_dim = 10\n",
    "hidden_size = 3\n",
    "hidden_layers = 1\n",
    "inputs = torch.randn((seq_len, batch_size, embedding_dim))  # make a sequence of length 5, 1 batch, input 10dim vector\n",
    "\n",
    "# initialize the hidden state. hidden layers have 3 nodes\n",
    "hidden = (torch.randn(hidden_layers, batch_size, hidden_size),\n",
    "            torch.randn((hidden_layers, batch_size, hidden_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lstm = nn.LSTM(embedding_dim, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, hidden = lstm(inputs, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-2.5325e-01,  4.9366e-02,  4.0028e-01],\n",
      "         [ 8.1580e-01, -5.9460e-01, -4.5211e-06],\n",
      "         [ 5.9945e-01,  3.4448e-04,  1.5961e-01]],\n",
      "\n",
      "        [[ 5.9029e-01,  8.7725e-04,  1.1217e-02],\n",
      "         [ 7.1056e-03, -1.5894e-04, -1.4747e-04],\n",
      "         [-2.7879e-03,  1.2264e-06, -1.4473e-02]],\n",
      "\n",
      "        [[ 1.3057e-01,  3.2360e-03,  5.4227e-01],\n",
      "         [-3.9118e-01,  7.1249e-02, -3.7470e-01],\n",
      "         [ 4.5797e-05,  5.1406e-05,  2.7413e-02]],\n",
      "\n",
      "        [[ 9.9916e-02,  4.7488e-01,  9.7807e-03],\n",
      "         [-3.8806e-01, -8.1084e-05,  7.0315e-02],\n",
      "         [ 1.5172e-01,  5.9603e-01, -1.1859e-04]],\n",
      "\n",
      "        [[ 2.7766e-01,  4.5121e-02, -5.6821e-01],\n",
      "         [-7.7895e-02, -4.1036e-01, -6.2259e-03],\n",
      "         [ 9.4735e-02,  1.3790e-04, -1.2973e-04]]])\n",
      "torch.Size([5, 3, 3])\n",
      "torch.Size([1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "# outputs of each hidden node(3) for each item in sequence(5)\n",
    "print(out)\n",
    "print(out.shape)\n",
    "\n",
    "# final hidden state and final cell state of sequence; notice that the hidden state equals the final output\n",
    "print(hidden[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 5\n",
    "batch_size = 1\n",
    "input_dim = 10\n",
    "hidden_size = 3\n",
    "hidden_layers = 1\n",
    "num_dir = 2 # for bidirectional lstm\n",
    "inputs = autograd.Variable(torch.randn((seq_len, batch_size, input_dim)))  # make a sequence of length 5, 1 batch, input 10dim vector\n",
    "\n",
    "# initialize the hidden state. hidden layers have 3 nodes\n",
    "hidden = (autograd.Variable(torch.randn(hidden_layers*num_dir, batch_size, hidden_size)),\n",
    "          autograd.Variable(torch.randn((hidden_layers*num_dir, batch_size, hidden_size))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(input_dim, hidden_size, bidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, hidden = lstm(inputs, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      " -0.1693  0.7509 -0.0828 -0.5793  0.5285  0.6648\n",
      "\n",
      "(1 ,.,.) = \n",
      "  0.0218  0.1844 -0.1121 -0.6127  0.2034  0.3170\n",
      "\n",
      "(2 ,.,.) = \n",
      "  0.0731 -0.0683  0.0742 -0.0712  0.1552  0.0725\n",
      "\n",
      "(3 ,.,.) = \n",
      "  0.0098 -0.0052 -0.1215 -0.0741  0.2897  0.0834\n",
      "\n",
      "(4 ,.,.) = \n",
      " -0.0263 -0.3019 -0.1845 -0.1061  0.4653  0.0832\n",
      "[torch.FloatTensor of size 5x1x6]\n",
      "\n",
      "(Variable containing:\n",
      "(0 ,.,.) = \n",
      " -0.0263 -0.3019 -0.1845\n",
      "\n",
      "(1 ,.,.) = \n",
      " -0.5793  0.5285  0.6648\n",
      "[torch.FloatTensor of size 2x1x3]\n",
      ", Variable containing:\n",
      "(0 ,.,.) = \n",
      " -0.3161 -0.5384 -0.4495\n",
      "\n",
      "(1 ,.,.) = \n",
      " -0.7322  0.9997  0.9236\n",
      "[torch.FloatTensor of size 2x1x3]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# outputs of each hidden node in both directions(3*2) for each item in sequence(5)\n",
    "print(out)\n",
    "\n",
    "# final hidden and cell state of model in both directions\n",
    "# notice that the first 3 output of the final item equals the final first hidden state\n",
    "# the second 3 outputs from the first item equals the final second hidden state\n",
    "print(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Visualizing Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
