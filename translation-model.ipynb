{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Neural Translation Model in PyTorch</h1>\n",
    "by Mac Brennan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align: center !important;'>\n",
    " <img src='https://github.com/macbrennan90/macbrennan90.github.io/blob/master/images/encoder-decoder.png?raw=true'\n",
    "      alt='Translation Model Summary'>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project will be broken up into several parts as follows:\n",
    "\n",
    "__Part 1:__ Preparing the words\n",
    "\n",
    "+ Inspecting the Dataset\n",
    "+ Using Word Embeddings\n",
    "+ Organizing the Data\n",
    "\n",
    "__Part 2:__ Building the Model\n",
    "\n",
    "+ Bi-Directional Encoder\n",
    "+ Building Attention\n",
    "+ Decoder with Attention\n",
    "\n",
    "__Part 3:__ Training the Model\n",
    "\n",
    "+ Training Function\n",
    "+ Training Loop\n",
    "\n",
    "__Part 4:__ Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we get started we will load all the packages we will need\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import os.path\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "\n",
    "# Use gpu if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Preparing the Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset that will be used is a text file of english sentences and the corresponding french sentences.\n",
    "\n",
    "Each sentence is on a new line. The sentences will be split into a list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Load the data\n",
    "The data will be stored in two lists where each item is a sentence. The lists are:\n",
    "+ english_sentences\n",
    "+ french_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open('data/small_vocab_en', \"r\") as f:\n",
    "    data1 = f.read()\n",
    "with open('data/small_vocab_fr', \"r\") as f:\n",
    "    data2 = f.read()\n",
    "    \n",
    "# The data is just in a text file with each sentence on its own line\n",
    "english_sentences = data1.split('\\n')\n",
    "french_sentences = data2.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of English sentences: 137861 \n",
      "Number of French sentences: 137861 \n",
      "\n",
      "Example/Target pair:\n",
      "\n",
      "  california is usually quiet during march , and it is usually hot in june .\n",
      "  california est généralement calme en mars , et il est généralement chaud en juin .\n"
     ]
    }
   ],
   "source": [
    "print('Number of English sentences:', len(english_sentences), \n",
    "      '\\nNumber of French sentences:', len(french_sentences),'\\n')\n",
    "print('Example/Target pair:\\n')\n",
    "print('  '+english_sentences[2])\n",
    "print('  '+french_sentences[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Vocabulary\n",
    "Let's take a closer look at the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['california',\n",
       " 'is',\n",
       " 'usually',\n",
       " 'quiet',\n",
       " 'during',\n",
       " 'march',\n",
       " ',',\n",
       " 'and',\n",
       " 'it',\n",
       " 'is',\n",
       " 'usually',\n",
       " 'hot',\n",
       " 'in',\n",
       " 'june',\n",
       " '.']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_sentences[2].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest english sentence in our dataset is: 17\n"
     ]
    }
   ],
   "source": [
    "max_en_length = 0\n",
    "for sentence in english_sentences:\n",
    "    length = len(sentence.split())\n",
    "    max_en_length = max(max_en_length, length)\n",
    "print(\"The longest english sentence in our dataset is:\", max_en_length)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest french sentence in our dataset is: 23\n"
     ]
    }
   ],
   "source": [
    "max_fr_length = 0\n",
    "for sentence in french_sentences:\n",
    "    length = len(sentence.split())\n",
    "    max_fr_length = max(max_fr_length, length)\n",
    "print(\"The longest french sentence in our dataset is:\", max_fr_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "seq_length = max(max_fr_length, max_en_length) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "en_word_count = {}\n",
    "fr_word_count = {}\n",
    "\n",
    "for sentence in english_sentences:\n",
    "    for word in sentence.split():\n",
    "        if word in en_word_count:\n",
    "            en_word_count[word] +=1\n",
    "        else:\n",
    "            en_word_count[word] = 1\n",
    "            \n",
    "for sentence in french_sentences:\n",
    "    for word in sentence.split():\n",
    "        if word in fr_word_count:\n",
    "            fr_word_count[word] +=1\n",
    "        else:\n",
    "            fr_word_count[word] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Add end of sentence token to word count dict\n",
    "en_word_count['</s>'] = len(english_sentences)\n",
    "fr_word_count['</s>'] = len(english_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique English words: 228\n",
      "Number of unique French words: 356\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique English words:', len(en_word_count))\n",
    "print('Number of unique French words:', len(fr_word_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_value(items_tuple):\n",
    "    return items_tuple[1]\n",
    "\n",
    "# Sort the word counts to see what words or most/least common\n",
    "sorted_en_words= sorted(en_word_count.items(), key=get_value, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('is', 205858),\n",
       " (',', 140897),\n",
       " ('</s>', 137861),\n",
       " ('.', 129039),\n",
       " ('in', 75525),\n",
       " ('it', 75137),\n",
       " ('during', 74933),\n",
       " ('the', 67628),\n",
       " ('but', 63987),\n",
       " ('and', 59850)]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_en_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sorted_fr_words = sorted(fr_word_count.items(), key=get_value, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('est', 196809),\n",
       " ('</s>', 137861),\n",
       " ('.', 135619),\n",
       " (',', 123135),\n",
       " ('en', 105768),\n",
       " ('il', 84079),\n",
       " ('les', 65255),\n",
       " ('mais', 63987),\n",
       " ('et', 59851),\n",
       " ('la', 49861)]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_fr_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So the dataset is pretty small, we may want to get a bigger data set, but we'll see how this one does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternate Dataset\n",
    "Skip this section for now. You can come back and try training on this second dataset later. It is more diverse so it takes longer to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/fra.txt', \"r\") as f:\n",
    "    data1 = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = data1.split('\\n')\n",
    "english_sentences = []\n",
    "french_sentences = []\n",
    "for i, pair in enumerate(pairs):\n",
    "    pair_split = pair.split('\\t')\n",
    "    if len(pair_split)!= 2:\n",
    "        continue\n",
    "    english = pair_split[0].lower()\n",
    "    french = pair_split[1].lower()\n",
    "    \n",
    "    # Remove punctuation and limit sentence length\n",
    "    max_sent_length = 10\n",
    "    punctuation_table = english.maketrans({i:None for i in string.punctuation})\n",
    "    english = english.translate(punctuation_table)\n",
    "    french = french.translate(punctuation_table)\n",
    "    if len(english.split()) >= max_sent_length or len(french.split()) >= max_sent_length:\n",
    "        continue\n",
    "       \n",
    "    english_sentences.append(english)\n",
    "    french_sentences.append(french)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'have', 'to', 'hurry']"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_sentences[10000].split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['je', 'dois', 'me', 'dépêcher']"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "french_sentences[10000].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'students', 'have', 'been', 'eagerly', 'awaiting', 'the', 'test', 'results']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['mes',\n",
       " 'étudiants',\n",
       " 'ont',\n",
       " 'attendu',\n",
       " 'avidement',\n",
       " 'les',\n",
       " 'résultats',\n",
       " 'de',\n",
       " 'lépreuve']"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(english_sentences[-100].split())\n",
    "french_sentences[-100].split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest english sentence in our dataset is: 9\n"
     ]
    }
   ],
   "source": [
    "max_en_length = 0\n",
    "for sentence in english_sentences:\n",
    "    length = len(sentence.split())\n",
    "    max_en_length = max(max_en_length, length)\n",
    "print(\"The longest english sentence in our dataset is:\", max_en_length)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest french sentence in our dataset is: 9\n"
     ]
    }
   ],
   "source": [
    "max_fr_length = 0\n",
    "for sentence in french_sentences:\n",
    "    length = len(sentence.split())\n",
    "    max_fr_length = max(max_fr_length, length)\n",
    "print(\"The longest french sentence in our dataset is:\", max_fr_length)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = max(max_fr_length, max_en_length) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_word_count = {}\n",
    "fr_word_count = {}\n",
    "\n",
    "for sentence in english_sentences:\n",
    "    for word in sentence.split():\n",
    "        if word in en_word_count:\n",
    "            en_word_count[word] +=1\n",
    "        else:\n",
    "            en_word_count[word] = 1\n",
    "            \n",
    "for sentence in french_sentences:\n",
    "    for word in sentence.split():\n",
    "        if word in fr_word_count:\n",
    "            fr_word_count[word] +=1\n",
    "        else:\n",
    "            fr_word_count[word] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_word_count['</s>'] = len(english_sentences)\n",
    "fr_word_count['</s>'] = len(english_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique English words: 11997\n",
      "Number of unique French words: 24710\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique English words:', len(en_word_count))\n",
    "print('Number of unique French words:', len(fr_word_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_word2idx = {k:v+3 for v, k in enumerate(fr_word_count.keys())}\n",
    "en_word2idx = {k:v+3 for v, k in enumerate(en_word_count.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_word2idx['<pad>'] = 0\n",
    "fr_word2idx['<s>'] = 1\n",
    "fr_word2idx['<unk>'] = 2\n",
    "\n",
    "en_word2idx['<pad>'] = 0\n",
    "en_word2idx['<s>'] = 1\n",
    "en_word2idx['<unk>'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24713"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fr_word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value(items_tuple):\n",
    "    return items_tuple[1]\n",
    "\n",
    "sorted_en_words= sorted(en_word_count.items(), key=get_value, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('attempting', 1),\n",
       " ('eradicate', 1),\n",
       " ('impossibilities', 1),\n",
       " ('offers', 1),\n",
       " ('profound', 1),\n",
       " ('insights', 1),\n",
       " ('hummer', 1),\n",
       " ('limousines', 1),\n",
       " ('tyrannical', 1),\n",
       " ('imprison', 1)]"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_en_words[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are building an embedding matrix of pretrained word vectors. The word embeddings used here were downloaded from the fastText repository. These embeddings have 300 dimensions. To start we will add a few token embeddings for our specific case. We want a token to signal the start of the sentence, A token for words that we do not have an embedding for, and a token to pad sentences so all the sentences we use have the same length. This will allow us to train the model on batches of sentences that are different lengths, rather than one at a time.\n",
    "\n",
    "After this step we will have a dictionary and an embedding matrix for each language. The dictionary will map words to an index value in the embedding matrix where its' corresponding embedding vector is stored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Embeddings for the English data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings load from .npy file\n"
     ]
    }
   ],
   "source": [
    "# The data file containing the embeddings is very large so once we have the embeddings we want\n",
    "# we will save them as a numpy array. This way we can load this much faster then having to re read from\n",
    "# the large embedding file\n",
    "if os.path.exists('data/en_words.npy') and os.path.exists('data/en_vectors.npy'):\n",
    "    en_words = np.load('data/en_words.npy')\n",
    "    en_vectors = np.load('data/en_vectors.npy')\n",
    "    print('Embeddings load from .npy file')\n",
    "else:\n",
    "    # make a dict with the top 100,000 words\n",
    "    en_words = ['<pad>', # Padding Token\n",
    "                '<s>', # Start of sentence token\n",
    "                '<unk>'# Unknown word token\n",
    "               ]\n",
    "\n",
    "    en_vectors = list(np.random.uniform(-0.1, 0.1, (3, 300)))\n",
    "    en_vectors[0] *= 0 # make the padding vector zeros\n",
    "\n",
    "    with open('data/wiki.en.vec', \"r\") as f:\n",
    "        f.readline()\n",
    "        for _ in range(100000):\n",
    "            en_vecs = f.readline()\n",
    "            word = en_vecs.split()[0]\n",
    "            vector = np.float32(en_vecs.split()[1:])\n",
    "\n",
    "            # skip lines that don't have 300 dim\n",
    "            if len(vector) != 300:\n",
    "                continue\n",
    "\n",
    "            if word not in en_words:\n",
    "                en_words.append(word)\n",
    "                en_vectors.append(vector)\n",
    "        print(word, vector[:10]) # Last word embedding read from the file\n",
    "        en_words = np.array(en_words)\n",
    "        en_vectors = np.array(en_vectors)\n",
    "    # Save the arrays so we don't have to load the full word embedding file\n",
    "    np.save('data/en_words.npy', en_words)\n",
    "    np.save('data/en_vectors.npy', en_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_word2idx = {word:index for index, word in enumerate(en_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index for word hemophilia: 99996 \n",
      "vector for word hemophilia:\n",
      " [ 0.16189    -0.056121   -0.65560001  0.21569    -0.11878    -0.02066\n",
      "  0.37613001 -0.24117    -0.098989   -0.010058  ]\n"
     ]
    }
   ],
   "source": [
    "hemophilia_idx = en_word2idx['hemophilia']\n",
    "print('index for word hemophilia:', hemophilia_idx, \n",
    "      '\\nvector for word hemophilia:\\n',en_vectors[hemophilia_idx][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word embedding for hemophilia matches the one read from the file, so it looks like everything worked properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Embeddings for the Frech data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings load from .npy file\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('data/fr_words.npy') and os.path.exists('data/fr_vectors.npy'):\n",
    "    fr_words = np.load('data/fr_words.npy')\n",
    "    fr_vectors = np.load('data/fr_vectors.npy')\n",
    "    print('Embeddings load from .npy file')\n",
    "else:\n",
    "    # make a dict with the top 100,000 words\n",
    "    fr_words = ['<pad>',\n",
    "                '<s>',\n",
    "                '<unk>']\n",
    "\n",
    "    fr_vectors = list(np.random.uniform(-0.1, 0.1, (3, 300)))\n",
    "    fr_vectors[0] = np.zeros(300) # make the padding vector zeros\n",
    "\n",
    "    with open('data/wiki.fr.vec', \"r\") as f:\n",
    "        f.readline()\n",
    "        for _ in range(100000):\n",
    "            fr_vecs = f.readline()\n",
    "            word = fr_vecs.split()[0]\n",
    "            try:\n",
    "                vector = np.float32(fr_vecs.split()[1:])\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "             # skip lines that don't have 300 dim\n",
    "            if len(vector) != 300:\n",
    "                continue\n",
    "\n",
    "            if word not in fr_words:\n",
    "                fr_words.append(word)\n",
    "                fr_vectors.append(vector)\n",
    "        print(word, vector[:10])\n",
    "        fr_words = np.array(fr_words)\n",
    "        fr_vectors = np.array(fr_vectors)\n",
    "    # Save the arrays so we don't have to load the full word embedding file\n",
    "    np.save('data/fr_words.npy', fr_words)\n",
    "    np.save('data/fr_vectors.npy', fr_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_word2idx = {word:index for index, word in enumerate(fr_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index for word chabeuil: 99783 \n",
      "vector for word chabeuil:\n",
      " [-0.18058001 -0.24758001  0.075607    0.17299999  0.24116001 -0.11223\n",
      " -0.28173     0.27373999  0.37997001  0.48008999]\n"
     ]
    }
   ],
   "source": [
    "chabeuil_idx = fr_word2idx['chabeuil']\n",
    "print('index for word chabeuil:', chabeuil_idx, \n",
    "      '\\nvector for word chabeuil:\\n',fr_vectors[chabeuil_idx][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99783"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_word2idx[\"chabeuil\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word embedding for chabeuil matches as well so everything worked correctly for the french vocab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so we have all the pieces needed to take words and convert them into word embeddings. These word embeddings already have a lot of useful information about how words relate since we loaded the pre-trained word embeddings. Now we can build the translation model with the embedding matrices built in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up PyTorch Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than organizing all the data from a file and storing it in a list or some other data structure, PyTorch allows us to create a dataset object. To get an example from a dataset we just index the dataset object like we would a list. However, all our processing can be contained in the objects initialization or indexing process.\n",
    "\n",
    "This will also make training easier when we want to iterate through batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "class French2EnglishDataset(Dataset):\n",
    "    '''\n",
    "        French and associated English sentences.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, fr_sentences, en_sentences, fr_word2idx, en_word2idx, seq_length):\n",
    "        self.fr_sentences = fr_sentences\n",
    "        self.en_sentences = en_sentences\n",
    "        self.fr_word2idx = fr_word2idx\n",
    "        self.en_word2idx = en_word2idx\n",
    "        self.seq_length = seq_length\n",
    "        self.unk_en = set()\n",
    "        self.unk_fr = set()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(french_sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "            Returns a pair of tensors containing word indices\n",
    "            for the specified sentence pair in the dataset.\n",
    "        '''\n",
    "        \n",
    "        # init torch tensors, note that 0 is the padding index\n",
    "        french_tensor = torch.zeros(self.seq_length, dtype=torch.long)\n",
    "        english_tensor = torch.zeros(self.seq_length, dtype=torch.long)\n",
    "        \n",
    "        # Get sentence pair\n",
    "        french_sentence = self.fr_sentences[idx].split()\n",
    "        english_sentence = self.en_sentences[idx].split()\n",
    "        \n",
    "        # Add <EOS> tags\n",
    "        french_sentence.append('</s>')\n",
    "        english_sentence.append('</s>')\n",
    "        \n",
    "        # Load word indices\n",
    "        for i, word in enumerate(french_sentence):\n",
    "            if word in fr_word2idx and fr_word_count[word] > 5:\n",
    "                french_tensor[i] = fr_word2idx[word]\n",
    "            else:\n",
    "                french_tensor[i] = fr_word2idx['<unk>']\n",
    "                self.unk_fr.add(word)\n",
    "        \n",
    "        for i, word in enumerate(english_sentence):\n",
    "            if word in en_word2idx and en_word_count[word] > 5:\n",
    "                english_tensor[i] = en_word2idx[word]\n",
    "            else:\n",
    "                english_tensor[i] = en_word2idx['<unk>']\n",
    "                self.unk_en.add(word)\n",
    "            \n",
    "        sample = {'french_tensor': french_tensor, 'french_sentence': self.fr_sentences[idx],\n",
    "                  'english_tensor': english_tensor, 'english_sentence': self.en_sentences[idx]}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_english_dataset = French2EnglishDataset(french_sentences,\n",
    "                                               english_sentences,\n",
    "                                               fr_word2idx,\n",
    "                                               en_word2idx,\n",
    "                                               seq_length = seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example output of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = french_english_dataset[-10] # get 13th item in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tout le monde sattendait à ce que lexpérience échoue\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  121,    10,   129,     2,    12,    39,    33,     2,  6300,\n",
       "            3])"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(test_sample['french_sentence'])\n",
    "test_sample['french_tensor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_word2idx['</s>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataloader to check how the batching works\n",
    "dataloader = DataLoader(french_english_dataset, batch_size=64,\n",
    "                        shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([64, 10]) torch.Size([64, 10])\n",
      "1 torch.Size([64, 10]) torch.Size([64, 10])\n",
      "2 torch.Size([64, 10]) torch.Size([64, 10])\n",
      "3 torch.Size([64, 10]) torch.Size([64, 10])\n",
      "4 torch.Size([64, 10]) torch.Size([64, 10])\n",
      "5 torch.Size([64, 10]) torch.Size([64, 10])\n",
      "6 torch.Size([64, 10]) torch.Size([64, 10])\n",
      "7 torch.Size([64, 10]) torch.Size([64, 10])\n",
      "8 torch.Size([64, 10]) torch.Size([64, 10])\n",
      "9 torch.Size([64, 10]) torch.Size([64, 10])\n",
      "10 torch.Size([64, 10]) torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "# Prints out 10 batches from the dataloader\n",
    "for i_batch, sample_batched in enumerate(dataloader):\n",
    "    print(i_batch, sample_batched['french_tensor'].shape,\n",
    "          sample_batched['english_tensor'].shape)\n",
    "    if i_batch == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-Directional LSTM Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBiLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size, pretrained_embeddings):\n",
    "        super(EncoderBiLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_dim = pretrained_embeddings.shape[1]\n",
    "        self.vocab_size = pretrained_embeddings.shape[0]\n",
    "        self.num_layers = 2\n",
    "        self.dropout = 0.1 if self.num_layers > 1 else 0\n",
    "        self.bidirectional = True\n",
    "        \n",
    "        \n",
    "        # Construct the layers\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(pretrained_embeddings))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        self.lstm = nn.LSTM(self.embedding_dim,\n",
    "                            self.hidden_size,\n",
    "                            self.num_layers,\n",
    "                            batch_first = True,\n",
    "                            dropout=self.dropout,\n",
    "                            bidirectional=self.bidirectional)\n",
    "        \n",
    "        # Initialize hidden to hidden weights in LSTM to the Identity matrix\n",
    "        # PyTorch LSTM has 4 different hidden to hidden weights stacked in one matrix\n",
    "        identity_init = torch.eye(self.hidden_size)\n",
    "        self.lstm.weight_hh_l0.data.copy_(torch.cat([identity_init]*4, dim=0))\n",
    "        self.lstm.weight_hh_l0_reverse.data.copy_(torch.cat([identity_init]*4, dim=0))\n",
    "        self.lstm.weight_hh_l1.data.copy_(torch.cat([identity_init]*4, dim=0))\n",
    "        self.lstm.weight_hh_l1_reverse.data.copy_(torch.cat([identity_init]*4, dim=0))\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input)\n",
    "        output = self.lstm(embedded, hidden)\n",
    "        return output\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "        \n",
    "        hidden_state = torch.zeros(self.num_layers*(2 if self.bidirectional else 1),\n",
    "                                   batch_size,\n",
    "                                   self.hidden_size, \n",
    "                                   device=device)\n",
    "        \n",
    "        cell_state = torch.zeros(self.num_layers*(2 if self.bidirectional else 1),\n",
    "                                 batch_size,\n",
    "                                 self.hidden_size, \n",
    "                                 device=device)\n",
    "        \n",
    "        return (hidden_state, cell_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBiGRU(nn.Module):\n",
    "    def __init__(self, hidden_size, pretrained_embeddings):\n",
    "        super(EncoderBiGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_dim = pretrained_embeddings.shape[1]\n",
    "        self.vocab_size = pretrained_embeddings.shape[0]\n",
    "        self.num_layers = 2\n",
    "        self.dropout = 0.1 if self.num_layers > 1 else 0\n",
    "        self.bidirectional = True\n",
    "        \n",
    "        \n",
    "        # Construct the layers\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(pretrained_embeddings))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        self.gru = nn.GRU(self.embedding_dim,\n",
    "                            self.hidden_size,\n",
    "                            self.num_layers,\n",
    "                            batch_first = True,\n",
    "                            dropout=self.dropout,\n",
    "                            bidirectional=self.bidirectional)\n",
    "        \n",
    "        # Initialize hidden to hidden weights in GRU to the Identity matrix\n",
    "        # PyTorch GRU has 3 different hidden to hidden weights stacked in one matrix\n",
    "        identity_init = torch.eye(self.hidden_size)\n",
    "        self.gru.weight_hh_l0.data.copy_(torch.cat([identity_init]*3, dim=0))\n",
    "        self.gru.weight_hh_l0_reverse.data.copy_(torch.cat([identity_init]*3, dim=0))\n",
    "        self.gru.weight_hh_l1.data.copy_(torch.cat([identity_init]*3, dim=0))\n",
    "        self.gru.weight_hh_l1_reverse.data.copy_(torch.cat([identity_init]*3, dim=0))\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input)\n",
    "        output = self.gru(embedded, hidden)\n",
    "        return output\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "        \n",
    "        hidden_state = torch.zeros(self.num_layers*(2 if self.bidirectional else 1),\n",
    "                                   batch_size,\n",
    "                                   self.hidden_size, \n",
    "                                   device=device)\n",
    "        \n",
    "        return hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final output of the BiLSTM Encoder on our test input is: \n",
      "\n",
      " torch.Size([1, 3, 10])\n",
      "\n",
      "\n",
      "Encoder output tensor: \n",
      "\n",
      " tensor([[[ 0.0086,  0.0373,  0.0601, -0.0059,  0.0364,  0.1941,  0.0409,\n",
      "          -0.1233, -0.1199,  0.0149],\n",
      "         [ 0.0197,  0.0708,  0.1135, -0.0156,  0.0518,  0.1375,  0.0249,\n",
      "          -0.0826, -0.0825,  0.0017],\n",
      "         [ 0.0386,  0.1402,  0.2047, -0.0223,  0.0343,  0.0880,  0.0323,\n",
      "          -0.0698, -0.0657,  0.0421]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Test the encoder on a sample input, input tensor has dimensions (batch_size, seq_length)\n",
    "\n",
    "batch_size = 1\n",
    "seq_length = 3\n",
    "hidden_size = 5\n",
    "encoder = EncoderBiLSTM(hidden_size, fr_vectors).to(device)\n",
    "hidden = encoder.initHidden(batch_size)\n",
    "\n",
    "# Create an input tensor of random indices\n",
    "inputs = torch.randint(0, 50, (batch_size, seq_length), dtype=torch.long, device=device)\n",
    "\n",
    "encoder_output, hidden_state = encoder.forward(inputs, hidden)\n",
    "\n",
    "print(\"The final output of the BiLSTM Encoder on our test input is: \\n\\n\", encoder_output.shape)\n",
    "\n",
    "print('\\n\\nEncoder output tensor: \\n\\n', encoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.2758,  0.0220,  0.0257, -0.0053, -0.0084]],\n",
       " \n",
       "         [[ 0.0670,  0.3896, -0.3120,  0.5056,  0.0540]],\n",
       " \n",
       "         [[ 0.0259,  0.0108,  0.0382,  0.3557,  0.2149]],\n",
       " \n",
       "         [[-0.1834,  0.0832,  0.1996,  0.1796, -0.1747]]], device='cuda:0'),\n",
       " tensor([[[ 0.5685,  0.0715,  0.3894, -0.0150, -0.0221]],\n",
       " \n",
       "         [[ 0.2641,  0.7460, -0.9657,  1.3272,  0.1121]],\n",
       " \n",
       "         [[ 0.0575,  0.0171,  0.0801,  0.6920,  0.6083]],\n",
       " \n",
       "         [[-0.4367,  0.1599,  0.3130,  0.4168, -0.3858]]], device='cuda:0'))"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_state# Tuple where first item is the hidden states, second item is the cell states.\n",
    "\n",
    "# The lstm has 2 layers, each layer has a forward and backward pass giving 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0670,  0.3896, -0.3120,  0.5056,  0.0540]],\n",
       "\n",
       "        [[-0.1834,  0.0832,  0.1996,  0.1796, -0.1747]]], device='cuda:0')"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_state[0][::2] # Hidden states from forward pass for both lstm layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_gru = EncoderBiGRU(hidden_size, fr_vectors).to(device)\n",
    "hidden = encoder_gru.initHidden(batch_size)\n",
    "o,h = encoder_gru(inputs, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0655,  0.1603, -0.0524, -0.0657, -0.1401,  0.3061,  0.2582,\n",
       "           0.0695, -0.2592,  0.5090],\n",
       "         [ 0.0141, -0.0763, -0.0059, -0.0959, -0.1177,  0.2653,  0.1059,\n",
       "          -0.0114, -0.2287,  0.4691],\n",
       "         [ 0.0666, -0.1238, -0.0988, -0.0869, -0.2081,  0.1008,  0.0713,\n",
       "           0.0013, -0.1661,  0.2408]]], device='cuda:0')"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.5315, -0.1505,  0.1251,  0.5621,  0.2153]],\n",
      "\n",
      "        [[ 0.0974,  0.6534, -0.4232,  0.2435, -0.3140]],\n",
      "\n",
      "        [[ 0.0666, -0.1238, -0.0988, -0.0869, -0.2081]],\n",
      "\n",
      "        [[ 0.3061,  0.2582,  0.0695, -0.2592,  0.5090]]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0974,  0.6534, -0.4232,  0.2435, -0.3140]],\n",
       "\n",
       "        [[ 0.3061,  0.2582,  0.0695, -0.2592,  0.5090]]], device='cuda:0')"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(h)\n",
    "h[1::2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Attention\n",
    "Let's take a moment to go over how attention is being modeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights:\n",
      " tensor([[[ 1.,  0.,  0.]]], device='cuda:0')\n",
      "\n",
      "First sequence item in Encoder output: \n",
      " tensor([[ 0.0148, -0.0507,  0.0720,  0.1213, -0.0374,  0.2630, -0.0217,\n",
      "         -0.1557,  0.1702, -0.1133]], device='cuda:0')\n",
      "\n",
      "Encoder Output after attention is applied: \n",
      " tensor([ 0.0148, -0.0507,  0.0720,  0.1213, -0.0374,  0.2630, -0.0217,\n",
      "        -0.1557,  0.1702, -0.1133], device='cuda:0')\n",
      "\n",
      " torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# Initialize attention weights to one\n",
    "attn_weights = torch.ones((batch_size, seq_length),device=device)\n",
    "\n",
    "# Set all weights except the weights associated with the first sequence item equal to zero\n",
    "# This would represent full attention on the first word in the sequence\n",
    "attn_weights[:, 1:] = 0\n",
    "\n",
    "attn_weights.unsqueeze_(1) # Add dimension for batch matrix multiplication\n",
    "attn_applied = torch.bmm(attn_weights, encoder_output)\n",
    "attn_applied.squeeze_() # Remove extra dimension\n",
    "\n",
    "print('Attention weights:\\n', attn_weights)\n",
    "print('\\nFirst sequence item in Encoder output: \\n',encoder_output[:,0,:])\n",
    "print('\\nEncoder Output after attention is applied: \\n', attn_applied)\n",
    "print('\\n', attn_applied.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Decoder with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderLSTM(nn.Module):\n",
    "    def __init__(self, decoder_hidden_size, pretrained_embeddings, seq_length, encoder_hidden_dim):\n",
    "        super(AttnDecoderLSTM, self).__init__()\n",
    "        # Embedding parameters\n",
    "        self.embedding_dim = pretrained_embeddings.shape[1]\n",
    "        self.output_vocab_size = pretrained_embeddings.shape[0]\n",
    "        \n",
    "        # LSTM parameters\n",
    "        self.decoder_hidden_size = decoder_hidden_size\n",
    "        self.num_layers = 2 # Potentially add more layers to LSTM later\n",
    "        self.dropout = 0.1 if self.num_layers > 1 else 0 # Potentially add dropout later\n",
    "        \n",
    "        # Attention parameters\n",
    "        self.seq_length = seq_length\n",
    "        self.encoder_hidden_dim = encoder_hidden_dim\n",
    "        \n",
    "        # Construct embedding layer for output language\n",
    "        self.embedding = nn.Embedding(self.output_vocab_size, self.embedding_dim)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(pretrained_embeddings))\n",
    "        self.embedding.weight.requires_grad = False # we don't want to train the embedding weights\n",
    "        \n",
    "        # Construct layer that calculates attentional weights\n",
    "        self.attn = nn.Linear((2*self.decoder_hidden_size + self.embedding_dim), self.seq_length)\n",
    "        \n",
    "        # Construct layer that compresses the combined matrix of the input embeddings\n",
    "        # and the encoder inputs after attention has been applied\n",
    "        self.attn_with_input = nn.Linear(self.embedding_dim + self.encoder_hidden_dim, self.embedding_dim)\n",
    "        \n",
    "        # LSTM for Decoder\n",
    "        self.lstm = nn.LSTM(self.embedding_dim,\n",
    "                            self.decoder_hidden_size,\n",
    "                            self.num_layers,\n",
    "                            dropout=self.dropout)\n",
    "        \n",
    "        # Initialize hidden to hidden weights in LSTM to the Identity matrix\n",
    "        # PyTorch LSTM has 4 different hidden to hidden weights stacked in one matrix\n",
    "        identity_init = torch.eye(self.decoder_hidden_size)\n",
    "        self.lstm.weight_hh_l0.data.copy_(torch.cat([identity_init]*4, dim=0))\n",
    "        self.lstm.weight_hh_l1.data.copy_(torch.cat([identity_init]*4, dim=0))\n",
    "        \n",
    "        # Output layer\n",
    "        self.out = nn.Linear(self.decoder_hidden_size, self.output_vocab_size)\n",
    "    \n",
    "    def forward(self, input, hidden, encoder_output):\n",
    "        # Input word indices, should have dim(1, batch_size), output will be (1, batch_size, embedding_dim)\n",
    "        embedded = self.embedding(input)\n",
    "        \n",
    "        # Calculate Attention weights\n",
    "        attn_weights = F.softmax(self.attn(torch.cat((hidden[1][1], hidden[0][1], embedded[0]), 1)), dim=1)\n",
    "        attn_weights = attn_weights.unsqueeze(1) # Add dimension for batch matrix multiplication\n",
    "        \n",
    "        # Apply Attention weights\n",
    "        attn_applied = torch.bmm(attn_weights, encoder_output)\n",
    "        attn_applied = attn_applied.squeeze(1) # Remove extra dimension, dim are now (batch_size, encoder_hidden_size)\n",
    "        \n",
    "        # Prepare LSTM input tensor\n",
    "\n",
    "        attn_combined = torch.cat((embedded[0], attn_applied), 1) # Combine embedding input and attn_applied,\n",
    "        lstm_input = F.relu(self.attn_with_input(attn_combined)) # pass through fully connected with ReLU\n",
    "        lstm_input = lstm_input.unsqueeze(0) # Add seq dimension so tensor has expected dimensions for lstm\n",
    "        \n",
    "        output, hidden = self.lstm(lstm_input, hidden) # Output dim = (1, batch_size, decoder_hidden_size)\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1) # softmax over all words in vocab\n",
    "        \n",
    "        \n",
    "        return output, hidden, attn_weights\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "        \n",
    "        hidden_state = torch.zeros(self.num_layers,\n",
    "                                   batch_size,\n",
    "                                   self.decoder_hidden_size, \n",
    "                                   device=device)\n",
    "        \n",
    "        cell_state = torch.zeros(self.num_layers,\n",
    "                                 batch_size,\n",
    "                                 self.decoder_hidden_size, \n",
    "                                 device=device)\n",
    "        \n",
    "        return (hidden_state, cell_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderGRU(nn.Module):\n",
    "    def __init__(self, decoder_hidden_size, pretrained_embeddings, seq_length, encoder_hidden_dim):\n",
    "        super(AttnDecoderGRU, self).__init__()\n",
    "        # Embedding parameters\n",
    "        self.embedding_dim = pretrained_embeddings.shape[1]\n",
    "        self.output_vocab_size = pretrained_embeddings.shape[0]\n",
    "        \n",
    "        # GRU parameters\n",
    "        self.decoder_hidden_size = decoder_hidden_size\n",
    "        self.num_layers = 2 # Potentially add more layers to LSTM later\n",
    "        self.dropout = 0.1 if self.num_layers > 1 else 0 # Potentially add dropout later\n",
    "        \n",
    "        # Attention parameters\n",
    "        self.seq_length = seq_length\n",
    "        self.encoder_hidden_dim = encoder_hidden_dim\n",
    "        \n",
    "        # Construct embedding layer for output language\n",
    "        self.embedding = nn.Embedding(self.output_vocab_size, self.embedding_dim)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(pretrained_embeddings))\n",
    "        self.embedding.weight.requires_grad = False # we don't want to train the embedding weights\n",
    "        \n",
    "        # Construct layer that calculates attentional weights\n",
    "        self.attn = nn.Linear(self.decoder_hidden_size + self.embedding_dim, self.seq_length)\n",
    "        \n",
    "        # Construct layer that compresses the combined matrix of the input embeddings\n",
    "        # and the encoder inputs after attention has been applied\n",
    "        self.attn_with_input = nn.Linear(self.embedding_dim + self.encoder_hidden_dim, self.embedding_dim)\n",
    "        \n",
    "        # gru for Decoder\n",
    "        self.gru = nn.GRU(self.embedding_dim,\n",
    "                            self.decoder_hidden_size,\n",
    "                            self.num_layers,\n",
    "                            dropout=self.dropout)\n",
    "        \n",
    "        # Initialize hidden to hidden weights in GRU to the Identity matrix\n",
    "        # PyTorch GRU has 3 different hidden to hidden weights stacked in one matrix\n",
    "        identity_init = torch.eye(self.decoder_hidden_size)\n",
    "        self.gru.weight_hh_l0.data.copy_(torch.cat([identity_init]*3, dim=0))\n",
    "        self.gru.weight_hh_l1.data.copy_(torch.cat([identity_init]*3, dim=0))\n",
    "        \n",
    "        # Output layer\n",
    "        self.out = nn.Linear(self.decoder_hidden_size, self.output_vocab_size)\n",
    "    \n",
    "    def forward(self, input, hidden, encoder_output):\n",
    "        # Input word indices, should have dim(1, batch_size), output will be (1, batch_size, embedding_dim)\n",
    "        embedded = self.embedding(input)\n",
    "        \n",
    "        # Calculate Attention weights\n",
    "        attn_weights = F.softmax(self.attn(torch.cat((hidden[0], embedded[0]), 1)), dim=1)\n",
    "        attn_weights = attn_weights.unsqueeze(1) # Add dimension for batch matrix multiplication\n",
    "        \n",
    "        # Apply Attention weights\n",
    "        attn_applied = torch.bmm(attn_weights, encoder_output)\n",
    "        attn_applied = attn_applied.squeeze(1) # Remove extra dimension, dim are now (batch_size, encoder_hidden_size)\n",
    "        \n",
    "        # Prepare GRU input tensor\n",
    "\n",
    "        attn_combined = torch.cat((embedded[0], attn_applied), 1) # Combine embedding input and attn_applied,\n",
    "        gru_input = F.relu(self.attn_with_input(attn_combined)) # pass through fully connected with ReLU\n",
    "        gru_input = gru_input.unsqueeze(0) # Add seq dimension so tensor has expected dimensions for lstm\n",
    "        \n",
    "        output, hidden = self.gru(gru_input, hidden) # Output dim = (1, batch_size, decoder_hidden_size)\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1) # softmax over all words in vocab\n",
    "        \n",
    "        return output, hidden, attn_weights\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "        \n",
    "        hidden_state = torch.zeros(self.num_layers,\n",
    "                                   batch_size,\n",
    "                                   self.decoder_hidden_size, \n",
    "                                   device=device)\n",
    "        \n",
    "        return hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Testing the Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Test the decoder on sample inputs to check that the dimensions of everything is correct\n",
    "decoder_hidden_size = 5\n",
    "\n",
    "decoder = AttnDecoderLSTM(decoder_hidden_size, en_vectors, seq_length, 2*hidden_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "input_idx = torch.tensor([fr_word2idx['<s>']]*batch_size, dtype=torch.long, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "input_idx = input_idx.unsqueeze_(0)\n",
    "decoder_hidden = decoder.initHidden(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 99997])\n"
     ]
    }
   ],
   "source": [
    "output, hidden, attention = decoder.forward(input_idx, decoder_hidden, encoder_output)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 5])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_hidden[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, encoder, decoder,\n",
    "          encoder_optimizer, decoder_optimizer, criterion):\n",
    "    \n",
    "    # Initialize encoder hidden state\n",
    "    encoder_hidden = encoder.initHidden(input_tensor.shape[0])\n",
    "    \n",
    "    # clear the gradients in the optimizers\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    # run forward pass through encoder on entire sequence\n",
    "    encoder_output, encoder_hidden = encoder.forward(input_tensor, encoder_hidden)\n",
    "    \n",
    "    # Initialize decoder input(Start of Sentence tag) and hidden state from encoder\n",
    "    decoder_input =  torch.tensor([en_word2idx['<s>']]*input_tensor.shape[0], dtype=torch.long, device=device).unsqueeze(0)\n",
    "    \n",
    "    # Use correct initial hidden state dimensions depending on type of RNN\n",
    "    try:\n",
    "        encoder.lstm\n",
    "        decoder_hidden = (encoder_hidden[0][1::2].contiguous(), encoder_hidden[1][1::2].contiguous())\n",
    "    except AttributeError:\n",
    "        decoder_hidden = encoder_hidden[1::2].contiguous()\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    use_teacher_forcing = True if random.random() < 0.5 else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        for di in range(seq_length):\n",
    "            output, decoder_hidden, attn_weights = decoder(decoder_input,\n",
    "                                                           decoder_hidden,\n",
    "                                                           encoder_output)\n",
    "            \n",
    "            # Feed target as input to next item in the sequence\n",
    "            decoder_input = target_tensor[di].unsqueeze(0)\n",
    "            loss += criterion(output, target_tensor[di])\n",
    "    else:\n",
    "        for di in range(seq_length):\n",
    "            output, decoder_hidden, attn_weights = decoder(decoder_input,\n",
    "                                                           decoder_hidden,\n",
    "                                                           encoder_output)\n",
    "            \n",
    "            # Feed output as input to next item in the sequence\n",
    "            decoder_input = output.topk(1)[1].view(1,-1).detach()\n",
    "            loss += criterion(output, target_tensor[di])\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    # Clip the gradients\n",
    "    nn.utils.clip_grad_norm_(encoder.parameters(), 25)\n",
    "    nn.utils.clip_grad_norm_(decoder.parameters(), 25)\n",
    "    \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, dataloader, epochs, print_every_n_batches=100, learning_rate=0.01):\n",
    "    \n",
    "    # keep track of losses\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "\n",
    "    # Initialize Encoder Optimizer\n",
    "    encoder_parameters = filter(lambda p: p.requires_grad, encoder.parameters())\n",
    "    encoder_optimizer = optim.SGD(encoder_parameters, lr=learning_rate)\n",
    "    \n",
    "    # Initialize Decoder Optimizer\n",
    "    decoder_parameters = filter(lambda p: p.requires_grad, decoder.parameters())\n",
    "    decoder_optimizer = optim.SGD(decoder_parameters, lr=learning_rate)\n",
    "\n",
    "    \n",
    "    criterion = nn.NLLLoss(ignore_index=0)\n",
    "    \n",
    "    # Cycle through epochs\n",
    "    for epoch in range(epochs):\n",
    "        loss_avg = 0\n",
    "        print(f'Epoch {epoch + 1}/{epochs}')\n",
    "        # Cycle through batches\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            \n",
    "            input_tensor = batch['french_tensor'].to(device)\n",
    "            target_tensor = batch['english_tensor'].transpose(1,0).to(device)\n",
    "            \n",
    "\n",
    "            loss = train(input_tensor, target_tensor, encoder, decoder,\n",
    "                         encoder_optimizer, decoder_optimizer, criterion)\n",
    "            \n",
    "            loss_avg += loss\n",
    "            if i % print_every_n_batches == 0 and i != 0:\n",
    "                loss_avg /= print_every_n_batches\n",
    "                print(f'After {i} batches, average loss/{print_every_n_batches} batches: {loss_avg}')\n",
    "                plot_losses.append(loss)\n",
    "                loss_avg = 0\n",
    "    return plot_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "batch_size = 16\n",
    "dataloader = DataLoader(french_english_dataset, batch_size=batch_size,\n",
    "                        shuffle=True, num_workers=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_lstm = EncoderBiLSTM(hidden_size, fr_vectors).to(device)\n",
    "decoder_lstm = AttnDecoderLSTM(hidden_size, en_vectors, seq_length, 2*hidden_size).to(device)\n",
    "\n",
    "encoder_gru = EncoderBiGRU(hidden_size, fr_vectors).to(device)\n",
    "decoder_gru = AttnDecoderGRU(hidden_size, en_vectors, seq_length, 2*hidden_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model from scratch.\n"
     ]
    }
   ],
   "source": [
    "from_scratch = True # Set to False if you have saved weights and want to load them\n",
    "\n",
    "if not from_scratch:\n",
    "    # Load weights from earlier model\n",
    "    encoder_state_dict = torch.load('models/encoder1_lstm.pth')\n",
    "    decoder_state_dict = torch.load('models/decoder1_lstm.pth')\n",
    "\n",
    "    encoder.load_state_dict(encoder_state_dict)\n",
    "    decoder.load_state_dict(decoder_state_dict)\n",
    "else:\n",
    "    print('Training model from scratch.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "After 100 batches, average loss/100 batches: 70.84892837524414\n",
      "After 200 batches, average loss/100 batches: 56.20391784667969\n",
      "After 300 batches, average loss/100 batches: 52.12953670501709\n",
      "After 400 batches, average loss/100 batches: 50.59972053527832\n",
      "After 500 batches, average loss/100 batches: 49.44401634216309\n",
      "After 600 batches, average loss/100 batches: 48.30097923278809\n",
      "After 700 batches, average loss/100 batches: 48.17358085632324\n",
      "After 800 batches, average loss/100 batches: 46.290966262817385\n",
      "After 900 batches, average loss/100 batches: 46.28448665618897\n",
      "After 1000 batches, average loss/100 batches: 46.135090370178226\n",
      "After 1100 batches, average loss/100 batches: 45.47230705261231\n",
      "After 1200 batches, average loss/100 batches: 45.524223976135254\n",
      "After 1300 batches, average loss/100 batches: 43.72259780883789\n",
      "After 1400 batches, average loss/100 batches: 44.74284992218018\n",
      "After 1500 batches, average loss/100 batches: 43.75370372772217\n",
      "After 1600 batches, average loss/100 batches: 43.30781524658203\n",
      "After 1700 batches, average loss/100 batches: 43.07711017608643\n",
      "After 1800 batches, average loss/100 batches: 43.14129711151123\n",
      "After 1900 batches, average loss/100 batches: 42.72362838745117\n",
      "After 2000 batches, average loss/100 batches: 42.60274080276489\n",
      "After 2100 batches, average loss/100 batches: 41.65464324951172\n",
      "After 2200 batches, average loss/100 batches: 41.02450344085693\n",
      "After 2300 batches, average loss/100 batches: 40.64088264465332\n",
      "After 2400 batches, average loss/100 batches: 41.49496780395508\n",
      "After 2500 batches, average loss/100 batches: 40.79719640731812\n",
      "After 2600 batches, average loss/100 batches: 39.57684587478638\n",
      "After 2700 batches, average loss/100 batches: 39.42157249450683\n",
      "After 2800 batches, average loss/100 batches: 40.235213413238526\n",
      "After 2900 batches, average loss/100 batches: 38.85758533477783\n",
      "After 3000 batches, average loss/100 batches: 39.94518911361694\n",
      "After 3100 batches, average loss/100 batches: 39.6174582862854\n",
      "After 3200 batches, average loss/100 batches: 39.450867404937746\n",
      "After 3300 batches, average loss/100 batches: 39.33531774520874\n",
      "After 3400 batches, average loss/100 batches: 39.31129940032959\n",
      "After 3500 batches, average loss/100 batches: 38.76560224533081\n",
      "After 3600 batches, average loss/100 batches: 39.04179334640503\n",
      "After 3700 batches, average loss/100 batches: 38.745208168029784\n",
      "After 3800 batches, average loss/100 batches: 39.364486618041994\n",
      "After 3900 batches, average loss/100 batches: 38.71720184326172\n",
      "After 4000 batches, average loss/100 batches: 38.50134750366211\n",
      "After 4100 batches, average loss/100 batches: 37.889464111328124\n",
      "After 4200 batches, average loss/100 batches: 37.780152034759524\n",
      "After 4300 batches, average loss/100 batches: 37.38370374679565\n",
      "After 4400 batches, average loss/100 batches: 38.69120790481568\n",
      "After 4500 batches, average loss/100 batches: 37.21965404510498\n",
      "After 4600 batches, average loss/100 batches: 36.73969875335693\n",
      "After 4700 batches, average loss/100 batches: 36.82999975204468\n",
      "After 4800 batches, average loss/100 batches: 36.885032157897946\n",
      "After 4900 batches, average loss/100 batches: 37.4495899772644\n",
      "After 5000 batches, average loss/100 batches: 36.528059310913086\n",
      "After 5100 batches, average loss/100 batches: 36.6356075668335\n",
      "After 5200 batches, average loss/100 batches: 36.14664258956909\n",
      "After 5300 batches, average loss/100 batches: 36.5854615020752\n",
      "After 5400 batches, average loss/100 batches: 36.98127613067627\n",
      "After 5500 batches, average loss/100 batches: 35.495919685363766\n",
      "After 5600 batches, average loss/100 batches: 36.2814736366272\n",
      "After 5700 batches, average loss/100 batches: 36.04048011779785\n",
      "After 5800 batches, average loss/100 batches: 35.19065607070923\n",
      "After 5900 batches, average loss/100 batches: 34.98036432266235\n",
      "After 6000 batches, average loss/100 batches: 34.970888977050784\n",
      "After 6100 batches, average loss/100 batches: 35.05963247299194\n",
      "After 6200 batches, average loss/100 batches: 34.60176494598389\n",
      "After 6300 batches, average loss/100 batches: 34.35575653076172\n",
      "After 6400 batches, average loss/100 batches: 34.941947803497314\n",
      "After 6500 batches, average loss/100 batches: 34.18226821899414\n",
      "After 6600 batches, average loss/100 batches: 34.13661434173584\n",
      "After 6700 batches, average loss/100 batches: 34.378994922637936\n",
      "After 6800 batches, average loss/100 batches: 34.417833919525144\n",
      "After 6900 batches, average loss/100 batches: 33.49362489700317\n",
      "After 7000 batches, average loss/100 batches: 33.56222354888916\n",
      "After 7100 batches, average loss/100 batches: 34.24269937515259\n",
      "After 7200 batches, average loss/100 batches: 33.864410724639896\n",
      "After 7300 batches, average loss/100 batches: 33.41859956741333\n",
      "After 7400 batches, average loss/100 batches: 32.96862707138062\n",
      "After 7500 batches, average loss/100 batches: 33.19680990219116\n",
      "After 7600 batches, average loss/100 batches: 32.98583869934082\n",
      "After 7700 batches, average loss/100 batches: 32.94396654129028\n",
      "After 7800 batches, average loss/100 batches: 32.50608169555664\n",
      "After 7900 batches, average loss/100 batches: 32.22494878768921\n",
      "After 8000 batches, average loss/100 batches: 32.32095376968384\n",
      "After 8100 batches, average loss/100 batches: 32.26022382736206\n",
      "After 8200 batches, average loss/100 batches: 31.266114044189454\n",
      "Epoch 2/10\n",
      "After 100 batches, average loss/100 batches: 31.4108473777771\n",
      "After 200 batches, average loss/100 batches: 31.13138366699219\n",
      "After 300 batches, average loss/100 batches: 31.11695686340332\n",
      "After 400 batches, average loss/100 batches: 30.967767486572267\n",
      "After 500 batches, average loss/100 batches: 31.478490905761717\n",
      "After 600 batches, average loss/100 batches: 32.310352096557615\n",
      "After 700 batches, average loss/100 batches: 31.356378345489503\n",
      "After 800 batches, average loss/100 batches: 30.906618824005125\n",
      "After 900 batches, average loss/100 batches: 30.958041534423828\n",
      "After 1000 batches, average loss/100 batches: 30.418845653533936\n",
      "After 1100 batches, average loss/100 batches: 30.55808506011963\n",
      "After 1200 batches, average loss/100 batches: 30.687881984710693\n",
      "After 1300 batches, average loss/100 batches: 30.870260257720947\n",
      "After 1400 batches, average loss/100 batches: 30.808542671203615\n",
      "After 1500 batches, average loss/100 batches: 30.870167922973632\n",
      "After 1600 batches, average loss/100 batches: 29.690409812927246\n",
      "After 1700 batches, average loss/100 batches: 29.38851890563965\n",
      "After 1800 batches, average loss/100 batches: 30.070160503387452\n",
      "After 1900 batches, average loss/100 batches: 28.997328777313232\n",
      "After 2000 batches, average loss/100 batches: 30.26089307785034\n",
      "After 2100 batches, average loss/100 batches: 29.42094476699829\n",
      "After 2200 batches, average loss/100 batches: 29.35486925125122\n",
      "After 2300 batches, average loss/100 batches: 29.847478828430177\n",
      "After 2400 batches, average loss/100 batches: 28.781799449920655\n",
      "After 2500 batches, average loss/100 batches: 29.291007862091064\n",
      "After 2600 batches, average loss/100 batches: 29.77453821182251\n",
      "After 2700 batches, average loss/100 batches: 29.558228816986084\n",
      "After 2800 batches, average loss/100 batches: 28.74669479370117\n",
      "After 2900 batches, average loss/100 batches: 28.907560386657714\n",
      "After 3000 batches, average loss/100 batches: 28.803913288116455\n",
      "After 3100 batches, average loss/100 batches: 29.163746547698974\n",
      "After 3200 batches, average loss/100 batches: 27.855855598449708\n",
      "After 3300 batches, average loss/100 batches: 28.76882423400879\n",
      "After 3400 batches, average loss/100 batches: 28.603962478637694\n",
      "After 3500 batches, average loss/100 batches: 28.999854774475097\n",
      "After 3600 batches, average loss/100 batches: 29.646580448150633\n",
      "After 3700 batches, average loss/100 batches: 28.878990879058836\n",
      "After 3800 batches, average loss/100 batches: 28.404275207519532\n",
      "After 3900 batches, average loss/100 batches: 28.433203506469727\n",
      "After 4000 batches, average loss/100 batches: 28.193983364105225\n",
      "After 4100 batches, average loss/100 batches: 27.49620569229126\n",
      "After 4200 batches, average loss/100 batches: 28.608213558197022\n",
      "After 4300 batches, average loss/100 batches: 27.742420959472657\n",
      "After 4400 batches, average loss/100 batches: 27.923011798858642\n",
      "After 4500 batches, average loss/100 batches: 28.498850765228273\n",
      "After 4600 batches, average loss/100 batches: 27.217646293640136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 4700 batches, average loss/100 batches: 27.808196697235108\n",
      "After 4800 batches, average loss/100 batches: 26.922648754119873\n",
      "After 4900 batches, average loss/100 batches: 27.470580921173095\n",
      "After 5000 batches, average loss/100 batches: 27.70908952713013\n",
      "After 5100 batches, average loss/100 batches: 26.849361209869386\n",
      "After 5200 batches, average loss/100 batches: 27.789777812957762\n",
      "After 5300 batches, average loss/100 batches: 28.633908100128174\n",
      "After 5400 batches, average loss/100 batches: 27.72856439590454\n",
      "After 5500 batches, average loss/100 batches: 26.91353359222412\n",
      "After 5600 batches, average loss/100 batches: 28.227961139678953\n",
      "After 5700 batches, average loss/100 batches: 26.683748264312744\n",
      "After 5800 batches, average loss/100 batches: 27.44314612388611\n",
      "After 5900 batches, average loss/100 batches: 26.298358116149902\n",
      "After 6000 batches, average loss/100 batches: 26.0166868019104\n",
      "After 6100 batches, average loss/100 batches: 26.593118095397948\n",
      "After 6200 batches, average loss/100 batches: 26.945027904510496\n",
      "After 6300 batches, average loss/100 batches: 27.01284002304077\n",
      "After 6400 batches, average loss/100 batches: 25.67169921875\n",
      "After 6500 batches, average loss/100 batches: 26.585481452941895\n",
      "After 6600 batches, average loss/100 batches: 26.17305944442749\n",
      "After 6700 batches, average loss/100 batches: 26.250172691345213\n",
      "After 6800 batches, average loss/100 batches: 25.611663970947266\n",
      "After 6900 batches, average loss/100 batches: 25.92744065284729\n",
      "After 7000 batches, average loss/100 batches: 26.195594959259033\n",
      "After 7100 batches, average loss/100 batches: 26.526592407226563\n",
      "After 7200 batches, average loss/100 batches: 26.30643479347229\n",
      "After 7300 batches, average loss/100 batches: 26.351072711944582\n",
      "After 7400 batches, average loss/100 batches: 26.32102210998535\n",
      "After 7500 batches, average loss/100 batches: 25.704148635864257\n",
      "After 7600 batches, average loss/100 batches: 25.305573387145998\n",
      "After 7700 batches, average loss/100 batches: 25.445787410736084\n",
      "After 7800 batches, average loss/100 batches: 25.64649271965027\n",
      "After 7900 batches, average loss/100 batches: 25.404485759735106\n",
      "After 8000 batches, average loss/100 batches: 25.682192211151122\n",
      "After 8100 batches, average loss/100 batches: 25.345013666152955\n",
      "After 8200 batches, average loss/100 batches: 24.82312615394592\n",
      "Epoch 3/10\n",
      "After 100 batches, average loss/100 batches: 24.817444286346436\n",
      "After 200 batches, average loss/100 batches: 24.625902318954466\n",
      "After 300 batches, average loss/100 batches: 25.502489738464355\n",
      "After 400 batches, average loss/100 batches: 24.546373538970947\n",
      "After 500 batches, average loss/100 batches: 24.19201985359192\n",
      "After 600 batches, average loss/100 batches: 24.75858555793762\n",
      "After 700 batches, average loss/100 batches: 25.037120885849\n",
      "After 800 batches, average loss/100 batches: 24.561997909545898\n",
      "After 900 batches, average loss/100 batches: 24.109040098190306\n",
      "After 1000 batches, average loss/100 batches: 24.127687463760378\n",
      "After 1100 batches, average loss/100 batches: 23.489580192565917\n",
      "After 1200 batches, average loss/100 batches: 24.354082422256468\n",
      "After 1300 batches, average loss/100 batches: 24.485110912322998\n",
      "After 1400 batches, average loss/100 batches: 23.99964593887329\n",
      "After 1500 batches, average loss/100 batches: 23.994997539520263\n",
      "After 1600 batches, average loss/100 batches: 24.347322549819946\n",
      "After 1700 batches, average loss/100 batches: 23.74715856552124\n",
      "After 1800 batches, average loss/100 batches: 24.66825176239014\n",
      "After 1900 batches, average loss/100 batches: 23.693246288299562\n",
      "After 2000 batches, average loss/100 batches: 24.633848114013674\n",
      "After 2100 batches, average loss/100 batches: 23.555168495178222\n",
      "After 2200 batches, average loss/100 batches: 23.5270228767395\n",
      "After 2300 batches, average loss/100 batches: 23.694850902557373\n",
      "After 2400 batches, average loss/100 batches: 23.332102308273317\n",
      "After 2500 batches, average loss/100 batches: 23.443945827484132\n",
      "After 2600 batches, average loss/100 batches: 23.947516174316405\n",
      "After 2700 batches, average loss/100 batches: 23.5919313621521\n",
      "After 2800 batches, average loss/100 batches: 23.84130126953125\n",
      "After 2900 batches, average loss/100 batches: 24.257329998016356\n",
      "After 3000 batches, average loss/100 batches: 23.65717710494995\n",
      "After 3100 batches, average loss/100 batches: 24.42811961174011\n",
      "After 3200 batches, average loss/100 batches: 23.833957929611206\n",
      "After 3300 batches, average loss/100 batches: 23.998384857177733\n",
      "After 3400 batches, average loss/100 batches: 23.53880849838257\n",
      "After 3500 batches, average loss/100 batches: 23.597099742889405\n",
      "After 3600 batches, average loss/100 batches: 23.05614130973816\n",
      "After 3700 batches, average loss/100 batches: 22.947999114990235\n",
      "After 3800 batches, average loss/100 batches: 23.03766303062439\n",
      "After 3900 batches, average loss/100 batches: 22.859876165390016\n",
      "After 4000 batches, average loss/100 batches: 22.977498264312743\n",
      "After 4100 batches, average loss/100 batches: 23.17572325706482\n",
      "After 4200 batches, average loss/100 batches: 23.17324239730835\n",
      "After 4300 batches, average loss/100 batches: 23.233673286437988\n",
      "After 4400 batches, average loss/100 batches: 22.25908770561218\n",
      "After 4500 batches, average loss/100 batches: 22.508345518112183\n",
      "After 4600 batches, average loss/100 batches: 21.72648995399475\n",
      "After 4700 batches, average loss/100 batches: 22.82502875328064\n",
      "After 4800 batches, average loss/100 batches: 23.796309909820557\n",
      "After 4900 batches, average loss/100 batches: 22.808521518707277\n",
      "After 5000 batches, average loss/100 batches: 22.59786473274231\n",
      "After 5100 batches, average loss/100 batches: 22.838642768859863\n",
      "After 5200 batches, average loss/100 batches: 23.079669218063355\n",
      "After 5300 batches, average loss/100 batches: 22.19831105232239\n",
      "After 5400 batches, average loss/100 batches: 22.606025600433348\n",
      "After 5500 batches, average loss/100 batches: 22.54777479171753\n",
      "After 5600 batches, average loss/100 batches: 22.382285318374635\n",
      "After 5700 batches, average loss/100 batches: 22.39068106651306\n",
      "After 5800 batches, average loss/100 batches: 22.96243209838867\n",
      "After 5900 batches, average loss/100 batches: 21.662426404953003\n",
      "After 6000 batches, average loss/100 batches: 22.98169720649719\n",
      "After 6100 batches, average loss/100 batches: 22.14407935142517\n",
      "After 6200 batches, average loss/100 batches: 22.082691383361816\n",
      "After 6300 batches, average loss/100 batches: 21.95507179260254\n",
      "After 6400 batches, average loss/100 batches: 22.025720624923707\n",
      "After 6500 batches, average loss/100 batches: 21.8442559337616\n",
      "After 6600 batches, average loss/100 batches: 22.165752363204955\n",
      "After 6700 batches, average loss/100 batches: 22.70620349884033\n",
      "After 6800 batches, average loss/100 batches: 21.776283178329468\n",
      "After 6900 batches, average loss/100 batches: 22.486924991607665\n",
      "After 7000 batches, average loss/100 batches: 22.846998691558838\n",
      "After 7100 batches, average loss/100 batches: 22.171115255355836\n",
      "After 7200 batches, average loss/100 batches: 21.964980688095093\n",
      "After 7300 batches, average loss/100 batches: 21.869951028823852\n",
      "After 7400 batches, average loss/100 batches: 22.596653938293457\n",
      "After 7500 batches, average loss/100 batches: 22.045767879486085\n",
      "After 7600 batches, average loss/100 batches: 21.680752210617065\n",
      "After 7700 batches, average loss/100 batches: 21.9109321308136\n",
      "After 7800 batches, average loss/100 batches: 22.07176951408386\n",
      "After 7900 batches, average loss/100 batches: 21.449566497802735\n",
      "After 8000 batches, average loss/100 batches: 22.570698776245116\n",
      "After 8100 batches, average loss/100 batches: 21.70784511566162\n",
      "After 8200 batches, average loss/100 batches: 21.207288932800292\n",
      "Epoch 4/10\n",
      "After 100 batches, average loss/100 batches: 20.868059511184693\n",
      "After 200 batches, average loss/100 batches: 20.470288066864015\n",
      "After 300 batches, average loss/100 batches: 20.41172715187073\n",
      "After 400 batches, average loss/100 batches: 21.0994317817688\n",
      "After 500 batches, average loss/100 batches: 20.848273878097533\n",
      "After 600 batches, average loss/100 batches: 20.402528352737427\n",
      "After 700 batches, average loss/100 batches: 20.596380853652953\n",
      "After 800 batches, average loss/100 batches: 20.528888454437254\n",
      "After 900 batches, average loss/100 batches: 20.932952299118043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 1000 batches, average loss/100 batches: 20.369552659988404\n",
      "After 1100 batches, average loss/100 batches: 20.379046001434325\n",
      "After 1200 batches, average loss/100 batches: 20.57981785774231\n",
      "After 1300 batches, average loss/100 batches: 20.98960750579834\n",
      "After 1400 batches, average loss/100 batches: 20.101998100280763\n",
      "After 1500 batches, average loss/100 batches: 19.92871313095093\n",
      "After 1600 batches, average loss/100 batches: 22.095501136779784\n",
      "After 1700 batches, average loss/100 batches: 20.211403703689577\n",
      "After 1800 batches, average loss/100 batches: 20.292973289489748\n",
      "After 1900 batches, average loss/100 batches: 20.650492973327637\n",
      "After 2000 batches, average loss/100 batches: 21.00032368659973\n",
      "After 2100 batches, average loss/100 batches: 21.293876905441284\n",
      "After 2200 batches, average loss/100 batches: 20.944240350723266\n",
      "After 2300 batches, average loss/100 batches: 20.73164140701294\n",
      "After 2400 batches, average loss/100 batches: 20.43531295776367\n",
      "After 2500 batches, average loss/100 batches: 21.174272232055664\n",
      "After 2600 batches, average loss/100 batches: 20.094514856338503\n",
      "After 2700 batches, average loss/100 batches: 19.918724880218505\n",
      "After 2800 batches, average loss/100 batches: 19.70640811920166\n",
      "After 2900 batches, average loss/100 batches: 20.679978704452516\n",
      "After 3000 batches, average loss/100 batches: 20.859649362564088\n",
      "After 3100 batches, average loss/100 batches: 20.41897315979004\n",
      "After 3200 batches, average loss/100 batches: 20.484690284729005\n",
      "After 3300 batches, average loss/100 batches: 20.37424738883972\n",
      "After 3400 batches, average loss/100 batches: 20.4797723197937\n",
      "After 3500 batches, average loss/100 batches: 19.671249351501466\n",
      "After 3600 batches, average loss/100 batches: 21.367619533538818\n",
      "After 3700 batches, average loss/100 batches: 20.34032102584839\n",
      "After 3800 batches, average loss/100 batches: 20.03250057220459\n",
      "After 3900 batches, average loss/100 batches: 19.902139949798585\n",
      "After 4000 batches, average loss/100 batches: 19.833234596252442\n",
      "After 4100 batches, average loss/100 batches: 20.006499261856078\n",
      "After 4200 batches, average loss/100 batches: 20.37083507537842\n",
      "After 4300 batches, average loss/100 batches: 20.512783918380737\n",
      "After 4400 batches, average loss/100 batches: 19.910257740020754\n",
      "After 4500 batches, average loss/100 batches: 20.17018430709839\n",
      "After 4600 batches, average loss/100 batches: 20.47891981124878\n",
      "After 4700 batches, average loss/100 batches: 19.855022048950197\n",
      "After 4800 batches, average loss/100 batches: 19.828588790893555\n",
      "After 4900 batches, average loss/100 batches: 19.853844871520995\n",
      "After 5000 batches, average loss/100 batches: 20.050494232177734\n",
      "After 5100 batches, average loss/100 batches: 20.872140703201293\n",
      "After 5200 batches, average loss/100 batches: 19.26544506072998\n",
      "After 5300 batches, average loss/100 batches: 19.766103506088257\n",
      "After 5400 batches, average loss/100 batches: 20.36935435295105\n",
      "After 5500 batches, average loss/100 batches: 19.414527158737183\n",
      "After 5600 batches, average loss/100 batches: 19.148164110183714\n",
      "After 5700 batches, average loss/100 batches: 19.928186855316163\n",
      "After 5800 batches, average loss/100 batches: 19.423066434860228\n",
      "After 5900 batches, average loss/100 batches: 19.83467064857483\n",
      "After 6000 batches, average loss/100 batches: 19.468849487304688\n",
      "After 6100 batches, average loss/100 batches: 19.23107960700989\n",
      "After 6200 batches, average loss/100 batches: 19.94485851287842\n",
      "After 6300 batches, average loss/100 batches: 19.451432876586914\n",
      "After 6400 batches, average loss/100 batches: 19.427557744979858\n",
      "After 6500 batches, average loss/100 batches: 20.02032012939453\n",
      "After 6600 batches, average loss/100 batches: 19.29910342216492\n",
      "After 6700 batches, average loss/100 batches: 19.166203117370607\n",
      "After 6800 batches, average loss/100 batches: 19.169079599380492\n",
      "After 6900 batches, average loss/100 batches: 19.19273341178894\n",
      "After 7000 batches, average loss/100 batches: 19.20460710525513\n",
      "After 7100 batches, average loss/100 batches: 20.06724347114563\n",
      "After 7200 batches, average loss/100 batches: 19.671343431472778\n",
      "After 7300 batches, average loss/100 batches: 19.7931233215332\n",
      "After 7400 batches, average loss/100 batches: 19.674555768966673\n",
      "After 7500 batches, average loss/100 batches: 18.446241779327394\n",
      "After 7600 batches, average loss/100 batches: 19.388602199554445\n",
      "After 7700 batches, average loss/100 batches: 19.361538810729982\n",
      "After 7800 batches, average loss/100 batches: 19.23802523612976\n",
      "After 7900 batches, average loss/100 batches: 19.552493600845338\n",
      "After 8000 batches, average loss/100 batches: 19.12506013870239\n",
      "After 8100 batches, average loss/100 batches: 19.574246797561646\n",
      "After 8200 batches, average loss/100 batches: 19.671017656326296\n",
      "Epoch 5/10\n",
      "After 100 batches, average loss/100 batches: 18.626513109207153\n",
      "After 200 batches, average loss/100 batches: 18.485113620758057\n",
      "After 300 batches, average loss/100 batches: 17.29722946166992\n",
      "After 400 batches, average loss/100 batches: 18.84920431137085\n",
      "After 500 batches, average loss/100 batches: 18.805860748291014\n",
      "After 600 batches, average loss/100 batches: 17.825752992630004\n",
      "After 700 batches, average loss/100 batches: 17.92921516418457\n",
      "After 800 batches, average loss/100 batches: 18.23414866447449\n",
      "After 900 batches, average loss/100 batches: 18.82104166984558\n",
      "After 1000 batches, average loss/100 batches: 18.0602938747406\n",
      "After 1100 batches, average loss/100 batches: 18.308344707489013\n",
      "After 1200 batches, average loss/100 batches: 18.49010022163391\n",
      "After 1300 batches, average loss/100 batches: 18.92164451599121\n",
      "After 1400 batches, average loss/100 batches: 18.232331285476686\n",
      "After 1500 batches, average loss/100 batches: 19.10342908859253\n",
      "After 1600 batches, average loss/100 batches: 18.51902633666992\n",
      "After 1700 batches, average loss/100 batches: 18.10031699180603\n",
      "After 1800 batches, average loss/100 batches: 18.24190770149231\n",
      "After 1900 batches, average loss/100 batches: 18.669940671920777\n",
      "After 2000 batches, average loss/100 batches: 18.89039695739746\n",
      "After 2100 batches, average loss/100 batches: 18.814238653182983\n",
      "After 2200 batches, average loss/100 batches: 18.58767737388611\n",
      "After 2300 batches, average loss/100 batches: 18.306386861801148\n",
      "After 2400 batches, average loss/100 batches: 17.807398538589478\n",
      "After 2500 batches, average loss/100 batches: 18.110189342498778\n",
      "After 2600 batches, average loss/100 batches: 18.455706119537354\n",
      "After 2700 batches, average loss/100 batches: 17.956792650222777\n",
      "After 2800 batches, average loss/100 batches: 18.82161255836487\n",
      "After 2900 batches, average loss/100 batches: 17.765721588134767\n",
      "After 3000 batches, average loss/100 batches: 17.630662546157836\n",
      "After 3100 batches, average loss/100 batches: 18.50685778617859\n",
      "After 3200 batches, average loss/100 batches: 19.33397692680359\n",
      "After 3300 batches, average loss/100 batches: 18.371686067581177\n",
      "After 3400 batches, average loss/100 batches: 18.205058145523072\n",
      "After 3500 batches, average loss/100 batches: 18.359795923233033\n",
      "After 3600 batches, average loss/100 batches: 17.84905821800232\n",
      "After 3700 batches, average loss/100 batches: 18.313965435028077\n",
      "After 3800 batches, average loss/100 batches: 18.12226900100708\n",
      "After 3900 batches, average loss/100 batches: 19.128253421783448\n",
      "After 4000 batches, average loss/100 batches: 18.634520797729493\n",
      "After 4100 batches, average loss/100 batches: 17.601408576965333\n",
      "After 4200 batches, average loss/100 batches: 18.184265298843385\n",
      "After 4300 batches, average loss/100 batches: 18.09010850906372\n",
      "After 4400 batches, average loss/100 batches: 17.412037773132326\n",
      "After 4500 batches, average loss/100 batches: 18.16931357383728\n",
      "After 4600 batches, average loss/100 batches: 17.13132826805115\n",
      "After 4700 batches, average loss/100 batches: 18.835934619903565\n",
      "After 4800 batches, average loss/100 batches: 17.935710983276365\n",
      "After 4900 batches, average loss/100 batches: 18.34509355545044\n",
      "After 5000 batches, average loss/100 batches: 17.97318223953247\n",
      "After 5100 batches, average loss/100 batches: 17.745017023086547\n",
      "After 5200 batches, average loss/100 batches: 17.744319868087768\n",
      "After 5300 batches, average loss/100 batches: 17.333269805908202\n",
      "After 5400 batches, average loss/100 batches: 18.172865657806398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 5500 batches, average loss/100 batches: 17.8042631816864\n",
      "After 5600 batches, average loss/100 batches: 17.867477712631224\n",
      "After 5700 batches, average loss/100 batches: 17.070243806838988\n",
      "After 5800 batches, average loss/100 batches: 17.28203899383545\n",
      "After 5900 batches, average loss/100 batches: 17.72084030151367\n",
      "After 6000 batches, average loss/100 batches: 17.588882598876953\n",
      "After 6100 batches, average loss/100 batches: 17.751462669372557\n",
      "After 6200 batches, average loss/100 batches: 16.89946078300476\n",
      "After 6300 batches, average loss/100 batches: 17.365011253356933\n",
      "After 6400 batches, average loss/100 batches: 17.625730199813844\n",
      "After 6500 batches, average loss/100 batches: 18.22607973098755\n",
      "After 6600 batches, average loss/100 batches: 18.35557053565979\n",
      "After 6700 batches, average loss/100 batches: 18.126824655532836\n",
      "After 6800 batches, average loss/100 batches: 17.78028694152832\n",
      "After 6900 batches, average loss/100 batches: 17.80104702949524\n",
      "After 7000 batches, average loss/100 batches: 17.702787790298462\n",
      "After 7100 batches, average loss/100 batches: 18.286849479675293\n",
      "After 7200 batches, average loss/100 batches: 17.51845262527466\n",
      "After 7300 batches, average loss/100 batches: 17.015184926986695\n",
      "After 7400 batches, average loss/100 batches: 17.784195885658264\n",
      "After 7500 batches, average loss/100 batches: 17.60562659263611\n",
      "After 7600 batches, average loss/100 batches: 17.50326051712036\n",
      "After 7700 batches, average loss/100 batches: 17.237272090911866\n",
      "After 7800 batches, average loss/100 batches: 17.35951626777649\n",
      "After 7900 batches, average loss/100 batches: 17.550553998947144\n",
      "After 8000 batches, average loss/100 batches: 17.620441284179687\n",
      "After 8100 batches, average loss/100 batches: 17.390174865722656\n",
      "After 8200 batches, average loss/100 batches: 17.31842971801758\n",
      "Epoch 6/10\n",
      "After 100 batches, average loss/100 batches: 16.737015013694762\n",
      "After 200 batches, average loss/100 batches: 16.471535301208498\n",
      "After 300 batches, average loss/100 batches: 16.160692043304444\n",
      "After 400 batches, average loss/100 batches: 16.821349401474\n",
      "After 500 batches, average loss/100 batches: 16.644249238967895\n",
      "After 600 batches, average loss/100 batches: 16.046114482879638\n",
      "After 700 batches, average loss/100 batches: 16.48588619709015\n",
      "After 800 batches, average loss/100 batches: 16.199470133781432\n",
      "After 900 batches, average loss/100 batches: 16.758453855514528\n",
      "After 1000 batches, average loss/100 batches: 17.476990160942076\n",
      "After 1100 batches, average loss/100 batches: 16.723801794052125\n",
      "After 1200 batches, average loss/100 batches: 16.478001761436463\n",
      "After 1300 batches, average loss/100 batches: 16.87770308494568\n",
      "After 1400 batches, average loss/100 batches: 16.97031502723694\n",
      "After 1500 batches, average loss/100 batches: 16.31327744960785\n",
      "After 1600 batches, average loss/100 batches: 16.373209753036498\n",
      "After 1700 batches, average loss/100 batches: 16.63425838470459\n",
      "After 1800 batches, average loss/100 batches: 17.05207685470581\n",
      "After 1900 batches, average loss/100 batches: 16.266835346221924\n",
      "After 2000 batches, average loss/100 batches: 16.708298206329346\n",
      "After 2100 batches, average loss/100 batches: 16.242014365196226\n",
      "After 2200 batches, average loss/100 batches: 16.25634925842285\n",
      "After 2300 batches, average loss/100 batches: 16.825610055923462\n",
      "After 2400 batches, average loss/100 batches: 16.479915347099304\n",
      "After 2500 batches, average loss/100 batches: 16.308289422988892\n",
      "After 2600 batches, average loss/100 batches: 16.525624094009398\n",
      "After 2700 batches, average loss/100 batches: 16.961934165954588\n",
      "After 2800 batches, average loss/100 batches: 16.615768456459044\n",
      "After 2900 batches, average loss/100 batches: 16.89021855354309\n",
      "After 3000 batches, average loss/100 batches: 16.970334434509276\n",
      "After 3100 batches, average loss/100 batches: 17.326572332382202\n",
      "After 3200 batches, average loss/100 batches: 15.967260847091675\n",
      "After 3300 batches, average loss/100 batches: 16.935527162551878\n",
      "After 3400 batches, average loss/100 batches: 16.303839988708496\n",
      "After 3500 batches, average loss/100 batches: 16.500969076156615\n",
      "After 3600 batches, average loss/100 batches: 17.25956551551819\n",
      "After 3700 batches, average loss/100 batches: 16.4188223361969\n",
      "After 3800 batches, average loss/100 batches: 16.705710372924806\n",
      "After 3900 batches, average loss/100 batches: 15.590874423980713\n",
      "After 4000 batches, average loss/100 batches: 16.55720579624176\n",
      "After 4100 batches, average loss/100 batches: 16.676353702545168\n",
      "After 4200 batches, average loss/100 batches: 16.252292861938475\n",
      "After 4300 batches, average loss/100 batches: 16.943242807388305\n",
      "After 4400 batches, average loss/100 batches: 16.355246863365174\n",
      "After 4500 batches, average loss/100 batches: 16.741603260040282\n",
      "After 4600 batches, average loss/100 batches: 16.68995656967163\n",
      "After 4700 batches, average loss/100 batches: 15.820655169486999\n",
      "After 4800 batches, average loss/100 batches: 16.683492527008056\n",
      "After 4900 batches, average loss/100 batches: 16.653812837600707\n",
      "After 5000 batches, average loss/100 batches: 16.026448130607605\n",
      "After 5100 batches, average loss/100 batches: 16.99265350341797\n",
      "After 5200 batches, average loss/100 batches: 16.431818256378175\n",
      "After 5300 batches, average loss/100 batches: 17.290218896865845\n",
      "After 5400 batches, average loss/100 batches: 16.387206897735595\n",
      "After 5500 batches, average loss/100 batches: 15.98283571243286\n",
      "After 5600 batches, average loss/100 batches: 16.65577341079712\n",
      "After 5700 batches, average loss/100 batches: 16.51198829174042\n",
      "After 5800 batches, average loss/100 batches: 16.72041055679321\n",
      "After 5900 batches, average loss/100 batches: 16.151235342025757\n",
      "After 6000 batches, average loss/100 batches: 16.31123878479004\n",
      "After 6100 batches, average loss/100 batches: 16.306591510772705\n",
      "After 6200 batches, average loss/100 batches: 16.725067529678345\n",
      "After 6300 batches, average loss/100 batches: 16.480064868927002\n",
      "After 6400 batches, average loss/100 batches: 16.518388872146605\n",
      "After 6500 batches, average loss/100 batches: 16.075203280448914\n",
      "After 6600 batches, average loss/100 batches: 16.82535601615906\n",
      "After 6700 batches, average loss/100 batches: 17.33317458152771\n",
      "After 6800 batches, average loss/100 batches: 16.34989133834839\n",
      "After 6900 batches, average loss/100 batches: 16.88424521446228\n",
      "After 7000 batches, average loss/100 batches: 16.892227454185488\n",
      "After 7100 batches, average loss/100 batches: 16.331825881004335\n",
      "After 7200 batches, average loss/100 batches: 17.10170959472656\n",
      "After 7300 batches, average loss/100 batches: 16.191214780807496\n",
      "After 7400 batches, average loss/100 batches: 17.075182390213012\n",
      "After 7500 batches, average loss/100 batches: 16.49472584724426\n",
      "After 7600 batches, average loss/100 batches: 15.464888429641723\n",
      "After 7700 batches, average loss/100 batches: 15.503799924850464\n",
      "After 7800 batches, average loss/100 batches: 16.303767828941346\n",
      "After 7900 batches, average loss/100 batches: 16.692547607421876\n",
      "After 8000 batches, average loss/100 batches: 16.003383951187132\n",
      "After 8100 batches, average loss/100 batches: 15.632143988609315\n",
      "After 8200 batches, average loss/100 batches: 16.960995321273803\n",
      "Epoch 7/10\n",
      "After 100 batches, average loss/100 batches: 15.533276300430298\n",
      "After 200 batches, average loss/100 batches: 14.652388010025025\n",
      "After 300 batches, average loss/100 batches: 15.012777457237243\n",
      "After 400 batches, average loss/100 batches: 14.77024154663086\n",
      "After 500 batches, average loss/100 batches: 14.905955147743224\n",
      "After 600 batches, average loss/100 batches: 15.701071195602417\n",
      "After 700 batches, average loss/100 batches: 15.25016300201416\n",
      "After 800 batches, average loss/100 batches: 15.599031114578247\n",
      "After 900 batches, average loss/100 batches: 15.33817021369934\n",
      "After 1000 batches, average loss/100 batches: 14.69265905380249\n",
      "After 1100 batches, average loss/100 batches: 15.284038319587708\n",
      "After 1200 batches, average loss/100 batches: 16.158480281829835\n",
      "After 1300 batches, average loss/100 batches: 15.554364070892333\n",
      "After 1400 batches, average loss/100 batches: 15.185233731269836\n",
      "After 1500 batches, average loss/100 batches: 14.586135964393616\n",
      "After 1600 batches, average loss/100 batches: 15.53525529384613\n",
      "After 1700 batches, average loss/100 batches: 16.160266466140747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 1800 batches, average loss/100 batches: 15.596639957427978\n",
      "After 1900 batches, average loss/100 batches: 15.955131759643555\n",
      "After 2000 batches, average loss/100 batches: 15.194098434448243\n",
      "After 2100 batches, average loss/100 batches: 14.815082054138184\n",
      "After 2200 batches, average loss/100 batches: 15.30640468597412\n",
      "After 2300 batches, average loss/100 batches: 15.716773571968078\n",
      "After 2400 batches, average loss/100 batches: 14.593266930580139\n",
      "After 2500 batches, average loss/100 batches: 14.839728813171387\n",
      "After 2600 batches, average loss/100 batches: 14.666165437698364\n",
      "After 2700 batches, average loss/100 batches: 15.40456503868103\n",
      "After 2800 batches, average loss/100 batches: 14.90892349243164\n",
      "After 2900 batches, average loss/100 batches: 15.350645456314087\n",
      "After 3000 batches, average loss/100 batches: 15.552556447982788\n",
      "After 3100 batches, average loss/100 batches: 14.823820099830627\n",
      "After 3200 batches, average loss/100 batches: 14.905847353935242\n",
      "After 3300 batches, average loss/100 batches: 15.385053052902222\n",
      "After 3400 batches, average loss/100 batches: 15.211951956748962\n",
      "After 3500 batches, average loss/100 batches: 15.381884899139404\n",
      "After 3600 batches, average loss/100 batches: 15.219112434387206\n",
      "After 3700 batches, average loss/100 batches: 15.794488453865052\n",
      "After 3800 batches, average loss/100 batches: 15.531791062355042\n",
      "After 3900 batches, average loss/100 batches: 15.216858477592469\n",
      "After 4000 batches, average loss/100 batches: 15.667241010665894\n",
      "After 4100 batches, average loss/100 batches: 15.131141538619994\n",
      "After 4200 batches, average loss/100 batches: 14.915108122825622\n",
      "After 4300 batches, average loss/100 batches: 15.835973300933837\n",
      "After 4400 batches, average loss/100 batches: 14.895304555892944\n",
      "After 4500 batches, average loss/100 batches: 16.31903154373169\n",
      "After 4600 batches, average loss/100 batches: 15.576103444099425\n",
      "After 4700 batches, average loss/100 batches: 15.06173638343811\n",
      "After 4800 batches, average loss/100 batches: 14.936235780715942\n",
      "After 4900 batches, average loss/100 batches: 15.44943681716919\n",
      "After 5000 batches, average loss/100 batches: 15.782277307510377\n",
      "After 5100 batches, average loss/100 batches: 14.765597267150879\n",
      "After 5200 batches, average loss/100 batches: 15.17748782157898\n",
      "After 5300 batches, average loss/100 batches: 15.037514081001282\n",
      "After 5400 batches, average loss/100 batches: 16.106243448257445\n",
      "After 5500 batches, average loss/100 batches: 15.959812312126159\n",
      "After 5600 batches, average loss/100 batches: 15.437359991073608\n",
      "After 5700 batches, average loss/100 batches: 15.304116926193238\n",
      "After 5800 batches, average loss/100 batches: 16.21989553928375\n",
      "After 5900 batches, average loss/100 batches: 14.810661702156066\n",
      "After 6000 batches, average loss/100 batches: 14.81746455192566\n",
      "After 6100 batches, average loss/100 batches: 15.52499074459076\n",
      "After 6200 batches, average loss/100 batches: 15.355165247917176\n",
      "After 6300 batches, average loss/100 batches: 15.053307943344116\n",
      "After 6400 batches, average loss/100 batches: 15.299549970626831\n",
      "After 6500 batches, average loss/100 batches: 14.825550994873048\n",
      "After 6600 batches, average loss/100 batches: 14.58668472290039\n",
      "After 6700 batches, average loss/100 batches: 14.643267087936401\n",
      "After 6800 batches, average loss/100 batches: 15.258551630973816\n",
      "After 6900 batches, average loss/100 batches: 14.929593563079834\n",
      "After 7000 batches, average loss/100 batches: 14.874868464469909\n",
      "After 7100 batches, average loss/100 batches: 15.665687494277954\n",
      "After 7200 batches, average loss/100 batches: 15.505489835739136\n",
      "After 7300 batches, average loss/100 batches: 15.48017195701599\n",
      "After 7400 batches, average loss/100 batches: 15.297642102241516\n",
      "After 7500 batches, average loss/100 batches: 15.583667469024657\n",
      "After 7600 batches, average loss/100 batches: 14.97952862739563\n",
      "After 7700 batches, average loss/100 batches: 15.702939348220825\n",
      "After 7800 batches, average loss/100 batches: 16.37838500022888\n",
      "After 7900 batches, average loss/100 batches: 15.013744502067565\n",
      "After 8000 batches, average loss/100 batches: 15.298806247711182\n",
      "After 8100 batches, average loss/100 batches: 15.172255244255066\n",
      "After 8200 batches, average loss/100 batches: 14.933492980003358\n",
      "Epoch 8/10\n",
      "After 100 batches, average loss/100 batches: 14.478201231956483\n",
      "After 200 batches, average loss/100 batches: 14.168398690223693\n",
      "After 300 batches, average loss/100 batches: 13.904816427230834\n",
      "After 400 batches, average loss/100 batches: 13.955794100761414\n",
      "After 500 batches, average loss/100 batches: 14.548984303474427\n",
      "After 600 batches, average loss/100 batches: 14.485027675628663\n",
      "After 700 batches, average loss/100 batches: 14.044399914741517\n",
      "After 800 batches, average loss/100 batches: 14.49065728187561\n",
      "After 900 batches, average loss/100 batches: 13.626346859931946\n",
      "After 1000 batches, average loss/100 batches: 14.615527691841125\n",
      "After 1100 batches, average loss/100 batches: 14.911545963287354\n",
      "After 1200 batches, average loss/100 batches: 14.460244617462159\n",
      "After 1300 batches, average loss/100 batches: 14.659786987304688\n",
      "After 1400 batches, average loss/100 batches: 13.769889750480651\n",
      "After 1500 batches, average loss/100 batches: 13.899507231712342\n",
      "After 1600 batches, average loss/100 batches: 14.194953632354736\n",
      "After 1700 batches, average loss/100 batches: 14.189288969039916\n",
      "After 1800 batches, average loss/100 batches: 13.8271808385849\n",
      "After 1900 batches, average loss/100 batches: 14.27351541519165\n",
      "After 2000 batches, average loss/100 batches: 14.28989622116089\n",
      "After 2100 batches, average loss/100 batches: 14.103318786621093\n",
      "After 2200 batches, average loss/100 batches: 14.696705980300903\n",
      "After 2300 batches, average loss/100 batches: 13.557477850914001\n",
      "After 2400 batches, average loss/100 batches: 13.942118301391602\n",
      "After 2500 batches, average loss/100 batches: 14.826935091018676\n",
      "After 2600 batches, average loss/100 batches: 14.177500648498535\n",
      "After 2700 batches, average loss/100 batches: 14.320965185165406\n",
      "After 2800 batches, average loss/100 batches: 13.93301868915558\n",
      "After 2900 batches, average loss/100 batches: 13.703161425590515\n",
      "After 3000 batches, average loss/100 batches: 14.065113263130188\n",
      "After 3100 batches, average loss/100 batches: 14.695704421997071\n",
      "After 3200 batches, average loss/100 batches: 13.881282224655152\n",
      "After 3300 batches, average loss/100 batches: 14.227937607765197\n",
      "After 3400 batches, average loss/100 batches: 14.802900810241699\n",
      "After 3500 batches, average loss/100 batches: 14.135859775543214\n",
      "After 3600 batches, average loss/100 batches: 13.863574676513672\n",
      "After 3700 batches, average loss/100 batches: 14.08372181892395\n",
      "After 3800 batches, average loss/100 batches: 15.112114372253417\n",
      "After 3900 batches, average loss/100 batches: 14.103642296791076\n",
      "After 4000 batches, average loss/100 batches: 14.070143384933472\n",
      "After 4100 batches, average loss/100 batches: 14.439875764846802\n",
      "After 4200 batches, average loss/100 batches: 13.813584775924683\n",
      "After 4300 batches, average loss/100 batches: 13.997994003295899\n",
      "After 4400 batches, average loss/100 batches: 14.184416308403016\n",
      "After 4500 batches, average loss/100 batches: 14.483456926345825\n",
      "After 4600 batches, average loss/100 batches: 14.353737063407898\n",
      "After 4700 batches, average loss/100 batches: 14.514060053825379\n",
      "After 4800 batches, average loss/100 batches: 14.535896949768066\n",
      "After 4900 batches, average loss/100 batches: 14.717772965431214\n",
      "After 5000 batches, average loss/100 batches: 14.120164775848389\n",
      "After 5100 batches, average loss/100 batches: 14.566768255233765\n",
      "After 5200 batches, average loss/100 batches: 14.504021248817445\n",
      "After 5300 batches, average loss/100 batches: 13.679595923423767\n",
      "After 5400 batches, average loss/100 batches: 14.238470191955566\n",
      "After 5500 batches, average loss/100 batches: 14.910218858718872\n",
      "After 5600 batches, average loss/100 batches: 14.534392910003662\n",
      "After 5700 batches, average loss/100 batches: 14.963029360771179\n",
      "After 5800 batches, average loss/100 batches: 13.889465169906616\n",
      "After 5900 batches, average loss/100 batches: 13.747957625389098\n",
      "After 6000 batches, average loss/100 batches: 14.946791710853576\n",
      "After 6100 batches, average loss/100 batches: 14.586817898750304\n",
      "After 6200 batches, average loss/100 batches: 14.318424711227417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 6300 batches, average loss/100 batches: 13.597205133438111\n",
      "After 6400 batches, average loss/100 batches: 14.956294345855714\n",
      "After 6500 batches, average loss/100 batches: 14.146872224807739\n",
      "After 6600 batches, average loss/100 batches: 14.258441867828369\n",
      "After 6700 batches, average loss/100 batches: 13.987531294822693\n",
      "After 6800 batches, average loss/100 batches: 14.427384147644043\n",
      "After 6900 batches, average loss/100 batches: 14.739396891593934\n",
      "After 7000 batches, average loss/100 batches: 14.46384243965149\n",
      "After 7100 batches, average loss/100 batches: 13.323862037658692\n",
      "After 7200 batches, average loss/100 batches: 13.644823455810547\n",
      "After 7300 batches, average loss/100 batches: 14.614376564025878\n",
      "After 7400 batches, average loss/100 batches: 14.341404757499696\n",
      "After 7500 batches, average loss/100 batches: 15.166184086799621\n",
      "After 7600 batches, average loss/100 batches: 13.984834151268005\n",
      "After 7700 batches, average loss/100 batches: 14.94808919429779\n",
      "After 7800 batches, average loss/100 batches: 14.929936637878418\n",
      "After 7900 batches, average loss/100 batches: 14.235620679855346\n",
      "After 8000 batches, average loss/100 batches: 14.272918729782104\n",
      "After 8100 batches, average loss/100 batches: 14.813159756660461\n",
      "After 8200 batches, average loss/100 batches: 14.618440623283385\n",
      "Epoch 9/10\n",
      "After 100 batches, average loss/100 batches: 13.66252730846405\n",
      "After 200 batches, average loss/100 batches: 12.769680643081665\n",
      "After 300 batches, average loss/100 batches: 13.354244222640991\n",
      "After 400 batches, average loss/100 batches: 13.007579474449157\n",
      "After 500 batches, average loss/100 batches: 13.580869812965393\n",
      "After 600 batches, average loss/100 batches: 12.543568458557129\n",
      "After 700 batches, average loss/100 batches: 12.862223916053772\n",
      "After 800 batches, average loss/100 batches: 13.438672652244568\n",
      "After 900 batches, average loss/100 batches: 13.850994086265564\n",
      "After 1000 batches, average loss/100 batches: 13.164213881492614\n",
      "After 1100 batches, average loss/100 batches: 13.081865315437318\n",
      "After 1200 batches, average loss/100 batches: 13.343582944869995\n",
      "After 1300 batches, average loss/100 batches: 13.174230465888977\n",
      "After 1400 batches, average loss/100 batches: 13.064837646484374\n",
      "After 1500 batches, average loss/100 batches: 13.814159274101257\n",
      "After 1600 batches, average loss/100 batches: 13.15807345867157\n",
      "After 1700 batches, average loss/100 batches: 12.993376064300538\n",
      "After 1800 batches, average loss/100 batches: 13.477380948066711\n",
      "After 1900 batches, average loss/100 batches: 13.261941089630128\n",
      "After 2000 batches, average loss/100 batches: 13.286160535812378\n",
      "After 2100 batches, average loss/100 batches: 13.308235769271851\n",
      "After 2200 batches, average loss/100 batches: 13.467620754241944\n",
      "After 2300 batches, average loss/100 batches: 13.206250290870667\n",
      "After 2400 batches, average loss/100 batches: 13.410254340171814\n",
      "After 2500 batches, average loss/100 batches: 13.530601916313172\n",
      "After 2600 batches, average loss/100 batches: 13.54623556137085\n",
      "After 2700 batches, average loss/100 batches: 13.49435052394867\n",
      "After 2800 batches, average loss/100 batches: 13.390606265068055\n",
      "After 2900 batches, average loss/100 batches: 13.611288084983826\n",
      "After 3000 batches, average loss/100 batches: 13.733963747024536\n",
      "After 3100 batches, average loss/100 batches: 12.767961821556092\n",
      "After 3200 batches, average loss/100 batches: 13.572165360450745\n",
      "After 3300 batches, average loss/100 batches: 13.171582932472228\n",
      "After 3400 batches, average loss/100 batches: 13.62127513408661\n",
      "After 3500 batches, average loss/100 batches: 13.690112648010254\n",
      "After 3600 batches, average loss/100 batches: 13.008689141273498\n",
      "After 3700 batches, average loss/100 batches: 12.931034817695618\n",
      "After 3800 batches, average loss/100 batches: 13.51843758583069\n",
      "After 3900 batches, average loss/100 batches: 13.518209981918336\n",
      "After 4000 batches, average loss/100 batches: 14.25153058052063\n",
      "After 4100 batches, average loss/100 batches: 14.21514229297638\n",
      "After 4200 batches, average loss/100 batches: 13.752138447761535\n",
      "After 4300 batches, average loss/100 batches: 14.108841576576232\n",
      "After 4400 batches, average loss/100 batches: 13.040839133262635\n",
      "After 4500 batches, average loss/100 batches: 13.736861429214478\n",
      "After 4600 batches, average loss/100 batches: 13.808018803596497\n",
      "After 4700 batches, average loss/100 batches: 13.440090746879578\n",
      "After 4800 batches, average loss/100 batches: 13.739550127983094\n",
      "After 4900 batches, average loss/100 batches: 13.903842735290528\n",
      "After 5000 batches, average loss/100 batches: 12.9119234085083\n",
      "After 5100 batches, average loss/100 batches: 14.020718960762023\n",
      "After 5200 batches, average loss/100 batches: 14.555308709144592\n",
      "After 5300 batches, average loss/100 batches: 14.06313994884491\n",
      "After 5400 batches, average loss/100 batches: 13.82567759513855\n",
      "After 5500 batches, average loss/100 batches: 14.061568970680236\n",
      "After 5600 batches, average loss/100 batches: 13.858704285621643\n",
      "After 5700 batches, average loss/100 batches: 13.606475772857666\n",
      "After 5800 batches, average loss/100 batches: 13.911090335845948\n",
      "After 5900 batches, average loss/100 batches: 13.909007368087769\n",
      "After 6000 batches, average loss/100 batches: 13.337557783126831\n",
      "After 6100 batches, average loss/100 batches: 12.821558079719544\n",
      "After 6200 batches, average loss/100 batches: 12.113357529640197\n",
      "After 6300 batches, average loss/100 batches: 13.490388340950012\n",
      "After 6400 batches, average loss/100 batches: 13.578552041053772\n",
      "After 6500 batches, average loss/100 batches: 14.068212838172913\n",
      "After 6600 batches, average loss/100 batches: 13.443079595565797\n",
      "After 6700 batches, average loss/100 batches: 13.152816863059998\n",
      "After 6800 batches, average loss/100 batches: 13.818080081939698\n",
      "After 6900 batches, average loss/100 batches: 13.38265016555786\n",
      "After 7000 batches, average loss/100 batches: 14.166985220909119\n",
      "After 7100 batches, average loss/100 batches: 13.92705560207367\n",
      "After 7200 batches, average loss/100 batches: 12.882619919776916\n",
      "After 7300 batches, average loss/100 batches: 13.071358513832092\n",
      "After 7400 batches, average loss/100 batches: 13.084552755355835\n",
      "After 7500 batches, average loss/100 batches: 13.109908442497254\n",
      "After 7600 batches, average loss/100 batches: 13.731437668800353\n",
      "After 7700 batches, average loss/100 batches: 13.272699370384217\n",
      "After 7800 batches, average loss/100 batches: 13.982278890609741\n",
      "After 7900 batches, average loss/100 batches: 13.837851700782776\n",
      "After 8000 batches, average loss/100 batches: 13.778908772468567\n",
      "After 8100 batches, average loss/100 batches: 14.07177288532257\n",
      "After 8200 batches, average loss/100 batches: 13.888894033432006\n",
      "Epoch 10/10\n",
      "After 100 batches, average loss/100 batches: 12.68981656074524\n",
      "After 200 batches, average loss/100 batches: 13.032648735046386\n",
      "After 300 batches, average loss/100 batches: 12.471721253395081\n",
      "After 400 batches, average loss/100 batches: 12.214929838180542\n",
      "After 500 batches, average loss/100 batches: 11.862173089981079\n",
      "After 600 batches, average loss/100 batches: 11.687053952217102\n",
      "After 700 batches, average loss/100 batches: 13.069643115997314\n",
      "After 800 batches, average loss/100 batches: 12.553839859962464\n",
      "After 900 batches, average loss/100 batches: 11.984106788635254\n",
      "After 1000 batches, average loss/100 batches: 12.235192170143128\n",
      "After 1100 batches, average loss/100 batches: 12.569094533920287\n",
      "After 1200 batches, average loss/100 batches: 11.51516290664673\n",
      "After 1300 batches, average loss/100 batches: 11.913183255195618\n",
      "After 1400 batches, average loss/100 batches: 12.244998006820678\n",
      "After 1500 batches, average loss/100 batches: 13.361012692451476\n",
      "After 1600 batches, average loss/100 batches: 12.531801810264588\n",
      "After 1700 batches, average loss/100 batches: 11.867020363807677\n",
      "After 1800 batches, average loss/100 batches: 12.86882884979248\n",
      "After 1900 batches, average loss/100 batches: 12.606568694114685\n",
      "After 2000 batches, average loss/100 batches: 13.018840069770812\n",
      "After 2100 batches, average loss/100 batches: 12.040266852378846\n",
      "After 2200 batches, average loss/100 batches: 12.570527958869935\n",
      "After 2300 batches, average loss/100 batches: 12.515700097084045\n",
      "After 2400 batches, average loss/100 batches: 11.776980485916138\n",
      "After 2500 batches, average loss/100 batches: 12.604896411895751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2600 batches, average loss/100 batches: 12.690297985076905\n",
      "After 2700 batches, average loss/100 batches: 12.869108991622925\n",
      "After 2800 batches, average loss/100 batches: 12.795578441619874\n",
      "After 2900 batches, average loss/100 batches: 12.798071293830871\n",
      "After 3000 batches, average loss/100 batches: 12.22079885005951\n",
      "After 3100 batches, average loss/100 batches: 12.53943714618683\n",
      "After 3200 batches, average loss/100 batches: 13.024232721328735\n",
      "After 3300 batches, average loss/100 batches: 14.159751958847046\n",
      "After 3400 batches, average loss/100 batches: 12.27744749546051\n",
      "After 3500 batches, average loss/100 batches: 13.499307870864868\n",
      "After 3600 batches, average loss/100 batches: 13.6280007314682\n",
      "After 3700 batches, average loss/100 batches: 12.831750931739807\n",
      "After 3800 batches, average loss/100 batches: 12.46674756526947\n",
      "After 3900 batches, average loss/100 batches: 13.071388611793518\n",
      "After 4000 batches, average loss/100 batches: 13.46661591053009\n",
      "After 4100 batches, average loss/100 batches: 12.938979229927064\n",
      "After 4200 batches, average loss/100 batches: 13.580121207237244\n",
      "After 4300 batches, average loss/100 batches: 12.713797507286072\n",
      "After 4400 batches, average loss/100 batches: 12.621109828948974\n",
      "After 4500 batches, average loss/100 batches: 13.000561461448669\n",
      "After 4600 batches, average loss/100 batches: 12.303470602035523\n",
      "After 4700 batches, average loss/100 batches: 12.698165054321288\n",
      "After 4800 batches, average loss/100 batches: 12.338147811889648\n",
      "After 4900 batches, average loss/100 batches: 13.11610011100769\n",
      "After 5000 batches, average loss/100 batches: 12.25605767250061\n",
      "After 5100 batches, average loss/100 batches: 12.823060441017152\n",
      "After 5200 batches, average loss/100 batches: 13.006767182350158\n",
      "After 5300 batches, average loss/100 batches: 12.440754971504212\n",
      "After 5400 batches, average loss/100 batches: 12.932762794494629\n",
      "After 5500 batches, average loss/100 batches: 12.761496071815492\n",
      "After 5600 batches, average loss/100 batches: 12.477911071777344\n",
      "After 5700 batches, average loss/100 batches: 13.26426854610443\n",
      "After 5800 batches, average loss/100 batches: 13.399410972595215\n",
      "After 5900 batches, average loss/100 batches: 13.097874503135682\n",
      "After 6000 batches, average loss/100 batches: 12.832072072029113\n",
      "After 6100 batches, average loss/100 batches: 12.367860760688782\n",
      "After 6200 batches, average loss/100 batches: 13.294570531845093\n",
      "After 6300 batches, average loss/100 batches: 12.999525647163392\n",
      "After 6400 batches, average loss/100 batches: 13.300768728256225\n",
      "After 6500 batches, average loss/100 batches: 13.023818221092224\n",
      "After 6600 batches, average loss/100 batches: 12.874583005905151\n",
      "After 6700 batches, average loss/100 batches: 13.37551854610443\n",
      "After 6800 batches, average loss/100 batches: 12.93746187210083\n",
      "After 6900 batches, average loss/100 batches: 13.175898599624634\n",
      "After 7000 batches, average loss/100 batches: 12.315133867263794\n",
      "After 7100 batches, average loss/100 batches: 12.461561603546143\n",
      "After 7200 batches, average loss/100 batches: 12.579849090576172\n",
      "After 7300 batches, average loss/100 batches: 12.528518929481507\n",
      "After 7400 batches, average loss/100 batches: 12.556056571006774\n",
      "After 7500 batches, average loss/100 batches: 12.600061674118042\n",
      "After 7600 batches, average loss/100 batches: 13.554838762283325\n",
      "After 7700 batches, average loss/100 batches: 12.98294204235077\n",
      "After 7800 batches, average loss/100 batches: 12.046344385147094\n",
      "After 7900 batches, average loss/100 batches: 13.041136655807495\n",
      "After 8000 batches, average loss/100 batches: 13.19166970729828\n",
      "After 8100 batches, average loss/100 batches: 12.667852230072022\n",
      "After 8200 batches, average loss/100 batches: 11.97170006752014\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "encoder_lstm.train() # Set model to training mode\n",
    "decoder_lstm.train() # Set model to training mode\n",
    "\n",
    "lstm_losses = trainIters(encoder_lstm, decoder_lstm, dataloader, epochs=10, learning_rate = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GRU based network.\n",
      "Epoch 1/10\n",
      "After 100 batches, average loss/100 batches: 9.799807832241058\n",
      "After 200 batches, average loss/100 batches: 9.209253978729247\n",
      "After 300 batches, average loss/100 batches: 9.205019721984863\n",
      "After 400 batches, average loss/100 batches: 9.601247034072877\n",
      "After 500 batches, average loss/100 batches: 9.532797622680665\n",
      "After 600 batches, average loss/100 batches: 9.641147303581239\n",
      "After 700 batches, average loss/100 batches: 9.700173473358154\n",
      "After 800 batches, average loss/100 batches: 9.410836434364318\n",
      "After 900 batches, average loss/100 batches: 9.355227336883544\n",
      "After 1000 batches, average loss/100 batches: 9.965714392662049\n",
      "After 1100 batches, average loss/100 batches: 10.45830316543579\n",
      "After 1200 batches, average loss/100 batches: 9.317410349845886\n",
      "After 1300 batches, average loss/100 batches: 9.5808238363266\n",
      "After 1400 batches, average loss/100 batches: 9.240122447013855\n",
      "After 1500 batches, average loss/100 batches: 9.47185999393463\n",
      "After 1600 batches, average loss/100 batches: 9.406732239723205\n",
      "After 1700 batches, average loss/100 batches: 9.47716905593872\n",
      "After 1800 batches, average loss/100 batches: 9.422497496604919\n",
      "After 1900 batches, average loss/100 batches: 9.30956708908081\n",
      "After 2000 batches, average loss/100 batches: 9.483867192268372\n",
      "After 2100 batches, average loss/100 batches: 9.277766511440277\n",
      "After 2200 batches, average loss/100 batches: 9.506448330879211\n",
      "After 2300 batches, average loss/100 batches: 9.508406348228455\n",
      "After 2400 batches, average loss/100 batches: 9.312941870689393\n",
      "After 2500 batches, average loss/100 batches: 9.53769537448883\n",
      "After 2600 batches, average loss/100 batches: 8.766818580627442\n",
      "After 2700 batches, average loss/100 batches: 9.24887954711914\n",
      "After 2800 batches, average loss/100 batches: 9.561236157417298\n",
      "After 2900 batches, average loss/100 batches: 9.58094204902649\n",
      "After 3000 batches, average loss/100 batches: 9.500071611404419\n",
      "After 3100 batches, average loss/100 batches: 9.540150413513183\n",
      "After 3200 batches, average loss/100 batches: 9.102455730438232\n",
      "After 3300 batches, average loss/100 batches: 9.661667003631592\n",
      "After 3400 batches, average loss/100 batches: 8.860978603363037\n",
      "After 3500 batches, average loss/100 batches: 9.123488929271698\n",
      "After 3600 batches, average loss/100 batches: 8.95603687286377\n",
      "After 3700 batches, average loss/100 batches: 9.409465804100037\n",
      "After 3800 batches, average loss/100 batches: 9.37273814201355\n",
      "After 3900 batches, average loss/100 batches: 9.701002025604248\n",
      "After 4000 batches, average loss/100 batches: 8.92270839214325\n",
      "After 4100 batches, average loss/100 batches: 9.728465044498444\n",
      "After 4200 batches, average loss/100 batches: 9.323296990394592\n",
      "After 4300 batches, average loss/100 batches: 9.044609880447387\n",
      "After 4400 batches, average loss/100 batches: 9.24712821483612\n",
      "After 4500 batches, average loss/100 batches: 9.289351198673248\n",
      "After 4600 batches, average loss/100 batches: 9.298553867340088\n",
      "After 4700 batches, average loss/100 batches: 9.526305072307586\n",
      "After 4800 batches, average loss/100 batches: 9.526624457836151\n",
      "After 4900 batches, average loss/100 batches: 8.189377617835998\n",
      "After 5000 batches, average loss/100 batches: 9.185918707847595\n",
      "After 5100 batches, average loss/100 batches: 9.197178225517273\n",
      "After 5200 batches, average loss/100 batches: 8.849611611366273\n",
      "After 5300 batches, average loss/100 batches: 8.897230100631713\n",
      "After 5400 batches, average loss/100 batches: 8.922706580162048\n",
      "After 5500 batches, average loss/100 batches: 9.41897982597351\n",
      "After 5600 batches, average loss/100 batches: 8.948307056427002\n",
      "After 5700 batches, average loss/100 batches: 9.457081713676452\n",
      "After 5800 batches, average loss/100 batches: 9.493847432136535\n",
      "After 5900 batches, average loss/100 batches: 9.244172830581665\n",
      "After 6000 batches, average loss/100 batches: 9.294979510307313\n",
      "After 6100 batches, average loss/100 batches: 9.31414252758026\n",
      "After 6200 batches, average loss/100 batches: 9.018180017471314\n",
      "After 6300 batches, average loss/100 batches: 8.949312872886658\n",
      "After 6400 batches, average loss/100 batches: 8.968448457717896\n",
      "After 6500 batches, average loss/100 batches: 9.31823017835617\n",
      "After 6600 batches, average loss/100 batches: 8.900930256843568\n",
      "After 6700 batches, average loss/100 batches: 9.527093195915223\n",
      "After 6800 batches, average loss/100 batches: 8.934080352783203\n",
      "After 6900 batches, average loss/100 batches: 9.565470759868623\n",
      "After 7000 batches, average loss/100 batches: 9.174448249340058\n",
      "After 7100 batches, average loss/100 batches: 9.089496822357178\n",
      "After 7200 batches, average loss/100 batches: 8.963590171337128\n",
      "After 7300 batches, average loss/100 batches: 9.262495312690735\n",
      "After 7400 batches, average loss/100 batches: 9.333488268852234\n",
      "After 7500 batches, average loss/100 batches: 9.306570229530335\n",
      "After 7600 batches, average loss/100 batches: 9.30247838973999\n",
      "After 7700 batches, average loss/100 batches: 9.367958974838256\n",
      "After 7800 batches, average loss/100 batches: 8.826133646965026\n",
      "After 7900 batches, average loss/100 batches: 8.558685812950134\n",
      "After 8000 batches, average loss/100 batches: 8.885161271095276\n",
      "After 8100 batches, average loss/100 batches: 9.090553216934204\n",
      "After 8200 batches, average loss/100 batches: 9.033239867687225\n",
      "Epoch 2/10\n",
      "After 100 batches, average loss/100 batches: 9.229021630287171\n",
      "After 200 batches, average loss/100 batches: 8.667740943431854\n",
      "After 300 batches, average loss/100 batches: 8.907016263008117\n",
      "After 400 batches, average loss/100 batches: 8.903757243156432\n",
      "After 500 batches, average loss/100 batches: 9.2284125995636\n",
      "After 600 batches, average loss/100 batches: 9.020387015342713\n",
      "After 700 batches, average loss/100 batches: 9.051744003295898\n",
      "After 800 batches, average loss/100 batches: 8.806991683244705\n",
      "After 900 batches, average loss/100 batches: 8.593022093772888\n",
      "After 1000 batches, average loss/100 batches: 8.844238276481628\n",
      "After 1100 batches, average loss/100 batches: 8.689384803771972\n",
      "After 1200 batches, average loss/100 batches: 8.486925215721131\n",
      "After 1300 batches, average loss/100 batches: 8.645524888038635\n",
      "After 1400 batches, average loss/100 batches: 8.696648058891297\n",
      "After 1500 batches, average loss/100 batches: 8.835647764205932\n",
      "After 1600 batches, average loss/100 batches: 8.849910793304444\n",
      "After 1700 batches, average loss/100 batches: 9.308674492835998\n",
      "After 1800 batches, average loss/100 batches: 9.52436355829239\n",
      "After 1900 batches, average loss/100 batches: 9.233152179718017\n",
      "After 2000 batches, average loss/100 batches: 9.534434957504272\n",
      "After 2100 batches, average loss/100 batches: 9.090647120475769\n",
      "After 2200 batches, average loss/100 batches: 8.759993534088135\n",
      "After 2300 batches, average loss/100 batches: 8.445571517944336\n",
      "After 2400 batches, average loss/100 batches: 8.814980416297912\n",
      "After 2500 batches, average loss/100 batches: 9.236730537414552\n",
      "After 2600 batches, average loss/100 batches: 8.843995516300202\n",
      "After 2700 batches, average loss/100 batches: 8.956991555690765\n",
      "After 2800 batches, average loss/100 batches: 9.31555431842804\n",
      "After 2900 batches, average loss/100 batches: 9.285551519393922\n",
      "After 3000 batches, average loss/100 batches: 8.69219055891037\n",
      "After 3100 batches, average loss/100 batches: 9.032756295204162\n",
      "After 3200 batches, average loss/100 batches: 8.557311053276061\n",
      "After 3300 batches, average loss/100 batches: 8.484355278015137\n",
      "After 3400 batches, average loss/100 batches: 8.443151049613952\n",
      "After 3500 batches, average loss/100 batches: 8.808202612400056\n",
      "After 3600 batches, average loss/100 batches: 8.766856718063355\n",
      "After 3700 batches, average loss/100 batches: 8.408668694496155\n",
      "After 3800 batches, average loss/100 batches: 8.86113396167755\n",
      "After 3900 batches, average loss/100 batches: 8.738828346729278\n",
      "After 4000 batches, average loss/100 batches: 8.723405838012695\n",
      "After 4100 batches, average loss/100 batches: 8.769034688472749\n",
      "After 4200 batches, average loss/100 batches: 9.112429611682892\n",
      "After 4300 batches, average loss/100 batches: 9.188410177230836\n",
      "After 4400 batches, average loss/100 batches: 8.75083969116211\n",
      "After 4500 batches, average loss/100 batches: 8.232839217185974\n",
      "After 4600 batches, average loss/100 batches: 8.608016033172607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 4700 batches, average loss/100 batches: 8.606460690498352\n",
      "After 4800 batches, average loss/100 batches: 9.026002869606017\n",
      "After 4900 batches, average loss/100 batches: 9.068303050994873\n",
      "After 5000 batches, average loss/100 batches: 9.324446597099303\n",
      "After 5100 batches, average loss/100 batches: 8.936569612026215\n",
      "After 5200 batches, average loss/100 batches: 8.76578628063202\n",
      "After 5300 batches, average loss/100 batches: 8.335767302513123\n",
      "After 5400 batches, average loss/100 batches: 8.905954658985138\n",
      "After 5500 batches, average loss/100 batches: 8.353756918907166\n",
      "After 5600 batches, average loss/100 batches: 9.065378942489623\n",
      "After 5700 batches, average loss/100 batches: 9.01735029220581\n",
      "After 5800 batches, average loss/100 batches: 9.074291653633118\n",
      "After 5900 batches, average loss/100 batches: 9.117249031066894\n",
      "After 6000 batches, average loss/100 batches: 9.000782957077027\n",
      "After 6100 batches, average loss/100 batches: 9.103288292884827\n",
      "After 6200 batches, average loss/100 batches: 9.331474828720093\n",
      "After 6300 batches, average loss/100 batches: 8.878799939155579\n",
      "After 6400 batches, average loss/100 batches: 9.063879101276397\n",
      "After 6500 batches, average loss/100 batches: 8.489935958385468\n",
      "After 6600 batches, average loss/100 batches: 9.200247488021851\n",
      "After 6700 batches, average loss/100 batches: 8.111820120811462\n",
      "After 6800 batches, average loss/100 batches: 9.041433475017548\n",
      "After 6900 batches, average loss/100 batches: 9.13846179485321\n",
      "After 7000 batches, average loss/100 batches: 9.533235676288605\n",
      "After 7100 batches, average loss/100 batches: 9.16446153640747\n",
      "After 7200 batches, average loss/100 batches: 8.906230182647706\n",
      "After 7300 batches, average loss/100 batches: 9.337242906093598\n",
      "After 7400 batches, average loss/100 batches: 8.868990259170532\n",
      "After 7500 batches, average loss/100 batches: 9.031369140148163\n",
      "After 7600 batches, average loss/100 batches: 8.980203304290772\n",
      "After 7700 batches, average loss/100 batches: 8.86742546081543\n",
      "After 7800 batches, average loss/100 batches: 8.920267386436462\n",
      "After 7900 batches, average loss/100 batches: 8.934061880111694\n",
      "After 8000 batches, average loss/100 batches: 8.656906538009643\n",
      "After 8100 batches, average loss/100 batches: 8.76965437889099\n",
      "After 8200 batches, average loss/100 batches: 9.50562518119812\n",
      "Epoch 3/10\n",
      "After 100 batches, average loss/100 batches: 8.243005657196045\n",
      "After 200 batches, average loss/100 batches: 8.240728068351746\n",
      "After 300 batches, average loss/100 batches: 9.280430455207824\n",
      "After 400 batches, average loss/100 batches: 8.299832787513733\n",
      "After 500 batches, average loss/100 batches: 8.716140434741973\n",
      "After 600 batches, average loss/100 batches: 9.350816173553467\n",
      "After 700 batches, average loss/100 batches: 9.070847816467285\n",
      "After 800 batches, average loss/100 batches: 9.171127915382385\n",
      "After 900 batches, average loss/100 batches: 8.60423403263092\n",
      "After 1000 batches, average loss/100 batches: 8.779515986442567\n",
      "After 1100 batches, average loss/100 batches: 8.909771780967713\n",
      "After 1200 batches, average loss/100 batches: 8.518929715156554\n",
      "After 1300 batches, average loss/100 batches: 8.346736037731171\n",
      "After 1400 batches, average loss/100 batches: 8.716151199340821\n",
      "After 1500 batches, average loss/100 batches: 8.836052660942078\n",
      "After 1600 batches, average loss/100 batches: 8.970569319725037\n",
      "After 1700 batches, average loss/100 batches: 9.213839852809906\n",
      "After 1800 batches, average loss/100 batches: 8.981859970092774\n",
      "After 1900 batches, average loss/100 batches: 8.860090699195862\n",
      "After 2000 batches, average loss/100 batches: 8.221829056739807\n",
      "After 2100 batches, average loss/100 batches: 8.895364537239075\n",
      "After 2200 batches, average loss/100 batches: 8.712372515201569\n",
      "After 2300 batches, average loss/100 batches: 8.7139923620224\n",
      "After 2400 batches, average loss/100 batches: 8.253252201080322\n",
      "After 2500 batches, average loss/100 batches: 8.61247483730316\n",
      "After 2600 batches, average loss/100 batches: 8.407484138011933\n",
      "After 2700 batches, average loss/100 batches: 8.533814659118653\n",
      "After 2800 batches, average loss/100 batches: 8.592547931671142\n",
      "After 2900 batches, average loss/100 batches: 8.538413009643556\n",
      "After 3000 batches, average loss/100 batches: 8.826346435546874\n",
      "After 3100 batches, average loss/100 batches: 8.956856563091279\n",
      "After 3200 batches, average loss/100 batches: 8.669551997184753\n",
      "After 3300 batches, average loss/100 batches: 8.964351639747619\n",
      "After 3400 batches, average loss/100 batches: 9.070809888839722\n",
      "After 3500 batches, average loss/100 batches: 8.675752115249633\n",
      "After 3600 batches, average loss/100 batches: 8.873394207954407\n",
      "After 3700 batches, average loss/100 batches: 8.388551259040833\n",
      "After 3800 batches, average loss/100 batches: 8.824455320835114\n",
      "After 3900 batches, average loss/100 batches: 8.74891806602478\n",
      "After 4000 batches, average loss/100 batches: 9.077932677268983\n",
      "After 4100 batches, average loss/100 batches: 8.989144232273102\n",
      "After 4200 batches, average loss/100 batches: 8.828880200386047\n",
      "After 4300 batches, average loss/100 batches: 8.620777020454407\n",
      "After 4400 batches, average loss/100 batches: 8.638126659393311\n",
      "After 4500 batches, average loss/100 batches: 8.825697717666626\n",
      "After 4600 batches, average loss/100 batches: 8.869514710903168\n",
      "After 4700 batches, average loss/100 batches: 8.596589756011962\n",
      "After 4800 batches, average loss/100 batches: 8.740990490913392\n",
      "After 4900 batches, average loss/100 batches: 8.426779627799988\n",
      "After 5000 batches, average loss/100 batches: 8.692827847003937\n",
      "After 5100 batches, average loss/100 batches: 8.837084381580352\n",
      "After 5200 batches, average loss/100 batches: 8.269409015178681\n",
      "After 5300 batches, average loss/100 batches: 8.767361853122711\n",
      "After 5400 batches, average loss/100 batches: 9.136567273139953\n",
      "After 5500 batches, average loss/100 batches: 8.484268069267273\n",
      "After 5600 batches, average loss/100 batches: 8.486058235168457\n",
      "After 5700 batches, average loss/100 batches: 8.768256556987762\n",
      "After 5800 batches, average loss/100 batches: 8.394382047653199\n",
      "After 5900 batches, average loss/100 batches: 8.577044508457185\n",
      "After 6000 batches, average loss/100 batches: 8.628424224853516\n",
      "After 6100 batches, average loss/100 batches: 8.946975407600403\n",
      "After 6200 batches, average loss/100 batches: 8.790912470817567\n",
      "After 6300 batches, average loss/100 batches: 8.94203948020935\n",
      "After 6400 batches, average loss/100 batches: 8.828391306400299\n",
      "After 6500 batches, average loss/100 batches: 8.580033893585204\n",
      "After 6600 batches, average loss/100 batches: 8.127721483707427\n",
      "After 6700 batches, average loss/100 batches: 8.652645871639251\n",
      "After 6800 batches, average loss/100 batches: 9.416665484905243\n",
      "After 6900 batches, average loss/100 batches: 8.467269387245178\n",
      "After 7000 batches, average loss/100 batches: 8.37279331922531\n",
      "After 7100 batches, average loss/100 batches: 9.337990627288818\n",
      "After 7200 batches, average loss/100 batches: 8.357792973518372\n",
      "After 7300 batches, average loss/100 batches: 9.0789581823349\n",
      "After 7400 batches, average loss/100 batches: 8.280367369651794\n",
      "After 7500 batches, average loss/100 batches: 8.832615752220153\n",
      "After 7600 batches, average loss/100 batches: 9.025451970100402\n",
      "After 7700 batches, average loss/100 batches: 9.184508292675018\n",
      "After 7800 batches, average loss/100 batches: 8.888072609901428\n",
      "After 7900 batches, average loss/100 batches: 8.61400378704071\n",
      "After 8000 batches, average loss/100 batches: 8.669380946159363\n",
      "After 8100 batches, average loss/100 batches: 8.644863455295562\n",
      "After 8200 batches, average loss/100 batches: 8.745717840194702\n",
      "Epoch 4/10\n",
      "After 100 batches, average loss/100 batches: 8.824070727825164\n",
      "After 200 batches, average loss/100 batches: 8.697386946678161\n",
      "After 300 batches, average loss/100 batches: 8.365796854496002\n",
      "After 400 batches, average loss/100 batches: 8.602225580215453\n",
      "After 500 batches, average loss/100 batches: 8.824592456817626\n",
      "After 600 batches, average loss/100 batches: 8.330471158027649\n",
      "After 700 batches, average loss/100 batches: 8.329598188400269\n",
      "After 800 batches, average loss/100 batches: 8.463129315376282\n",
      "After 900 batches, average loss/100 batches: 8.266126132011413\n",
      "After 1000 batches, average loss/100 batches: 8.520059845447541\n",
      "After 1100 batches, average loss/100 batches: 8.265574662685394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 1200 batches, average loss/100 batches: 8.545278685092926\n",
      "After 1300 batches, average loss/100 batches: 8.618106291294097\n",
      "After 1400 batches, average loss/100 batches: 8.44746648788452\n",
      "After 1500 batches, average loss/100 batches: 8.413414249420166\n",
      "After 1600 batches, average loss/100 batches: 8.737616696357726\n",
      "After 1700 batches, average loss/100 batches: 9.143250813484192\n",
      "After 1800 batches, average loss/100 batches: 8.628398294448852\n",
      "After 1900 batches, average loss/100 batches: 8.326998052597046\n",
      "After 2000 batches, average loss/100 batches: 8.816965088844299\n",
      "After 2100 batches, average loss/100 batches: 8.37353241443634\n",
      "After 2200 batches, average loss/100 batches: 8.766301248073578\n",
      "After 2300 batches, average loss/100 batches: 9.015792617797851\n",
      "After 2400 batches, average loss/100 batches: 8.779583368301392\n",
      "After 2500 batches, average loss/100 batches: 8.761596720218659\n",
      "After 2600 batches, average loss/100 batches: 8.747337150573731\n",
      "After 2700 batches, average loss/100 batches: 8.492421598434449\n",
      "After 2800 batches, average loss/100 batches: 8.521717228889464\n",
      "After 2900 batches, average loss/100 batches: 8.67658447265625\n",
      "After 3000 batches, average loss/100 batches: 9.233927264213563\n",
      "After 3100 batches, average loss/100 batches: 8.535079221725464\n",
      "After 3200 batches, average loss/100 batches: 8.74655433177948\n",
      "After 3300 batches, average loss/100 batches: 8.582525169849395\n",
      "After 3400 batches, average loss/100 batches: 8.599774374961854\n",
      "After 3500 batches, average loss/100 batches: 8.522141716480256\n",
      "After 3600 batches, average loss/100 batches: 8.783899960517884\n",
      "After 3700 batches, average loss/100 batches: 8.455826253890992\n",
      "After 3800 batches, average loss/100 batches: 8.142293920516968\n",
      "After 3900 batches, average loss/100 batches: 8.696284782886504\n",
      "After 4000 batches, average loss/100 batches: 8.93916930437088\n",
      "After 4100 batches, average loss/100 batches: 8.30155249118805\n",
      "After 4200 batches, average loss/100 batches: 8.248976497650146\n",
      "After 4300 batches, average loss/100 batches: 8.843129670619964\n",
      "After 4400 batches, average loss/100 batches: 8.780528285503387\n",
      "After 4500 batches, average loss/100 batches: 8.793797721862793\n",
      "After 4600 batches, average loss/100 batches: 8.550773844718933\n",
      "After 4700 batches, average loss/100 batches: 8.95595555305481\n",
      "After 4800 batches, average loss/100 batches: 8.432966163158417\n",
      "After 4900 batches, average loss/100 batches: 9.057008831501006\n",
      "After 5000 batches, average loss/100 batches: 8.949856600761414\n",
      "After 5100 batches, average loss/100 batches: 8.41573430776596\n",
      "After 5200 batches, average loss/100 batches: 8.835422630310058\n",
      "After 5300 batches, average loss/100 batches: 8.841500444412231\n",
      "After 5400 batches, average loss/100 batches: 8.720544972419738\n",
      "After 5500 batches, average loss/100 batches: 8.385149405002593\n",
      "After 5600 batches, average loss/100 batches: 9.003030109405518\n",
      "After 5700 batches, average loss/100 batches: 8.468094856739045\n",
      "After 5800 batches, average loss/100 batches: 8.455790004730225\n",
      "After 5900 batches, average loss/100 batches: 8.623986761569977\n",
      "After 6000 batches, average loss/100 batches: 8.72131693840027\n",
      "After 6100 batches, average loss/100 batches: 8.948805041313172\n",
      "After 6200 batches, average loss/100 batches: 8.896162099838257\n",
      "After 6300 batches, average loss/100 batches: 8.43268057346344\n",
      "After 6400 batches, average loss/100 batches: 9.074667248725891\n",
      "After 6500 batches, average loss/100 batches: 8.539924421310424\n",
      "After 6600 batches, average loss/100 batches: 8.528827219009399\n",
      "After 6700 batches, average loss/100 batches: 8.642971630096435\n",
      "After 6800 batches, average loss/100 batches: 8.590261192321778\n",
      "After 6900 batches, average loss/100 batches: 8.360177211761474\n",
      "After 7000 batches, average loss/100 batches: 8.522484974861145\n",
      "After 7100 batches, average loss/100 batches: 8.750034437179565\n",
      "After 7200 batches, average loss/100 batches: 8.708143548965454\n",
      "After 7300 batches, average loss/100 batches: 8.456037669181823\n",
      "After 7400 batches, average loss/100 batches: 8.260462183952331\n",
      "After 7500 batches, average loss/100 batches: 8.53784075498581\n",
      "After 7600 batches, average loss/100 batches: 8.554543800354004\n",
      "After 7700 batches, average loss/100 batches: 8.283774490356445\n",
      "After 7800 batches, average loss/100 batches: 8.783938553333282\n",
      "After 7900 batches, average loss/100 batches: 8.453285536766053\n",
      "After 8000 batches, average loss/100 batches: 8.80106119632721\n",
      "After 8100 batches, average loss/100 batches: 8.745707008838654\n",
      "After 8200 batches, average loss/100 batches: 8.93117446422577\n",
      "Epoch 5/10\n",
      "After 100 batches, average loss/100 batches: 9.097881798744202\n",
      "After 200 batches, average loss/100 batches: 8.277595570087433\n",
      "After 300 batches, average loss/100 batches: 8.662547059059143\n",
      "After 400 batches, average loss/100 batches: 8.335016374588013\n",
      "After 500 batches, average loss/100 batches: 8.736401233673096\n",
      "After 600 batches, average loss/100 batches: 8.499415163993836\n",
      "After 700 batches, average loss/100 batches: 8.200706753730774\n",
      "After 800 batches, average loss/100 batches: 8.757180514335632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-555:\n",
      "Process Process-553:\n",
      "Process Process-554:\n",
      "Process Process-556:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mac/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/mac/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/mac/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/mac/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/mac/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/mac/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/mac/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/mac/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/mac/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/mac/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/mac/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/mac/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/mac/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/mac/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/mac/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/mac/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/mac/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/mac/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/mac/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/mac/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/mac/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/mac/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/mac/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/mac/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/mac/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/mac/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/mac/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/mac/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-445-9c0ba95ec729>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdecoder_gru\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Set model to training mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mgru_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainIters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_gru\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_gru\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-390-5e28eb72dbed>\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(encoder, decoder, dataloader, epochs, print_every_n_batches, learning_rate)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             loss = train(input_tensor, target_tensor, encoder, decoder,\n\u001b[0;32m---> 30\u001b[0;31m                          encoder_optimizer, decoder_optimizer, criterion)\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mloss_avg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-389-dfefe79affb2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m# Clip the gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('Training GRU based network.')\n",
    "learning_rate = 0.001\n",
    "encoder_gru.train() # Set model to training mode\n",
    "decoder_gru.train() # Set model to training mode\n",
    "\n",
    "gru_losses = trainIters(encoder_gru, decoder_gru, dataloader, epochs=10, learning_rate = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/lstm2_losses.npy', lstm_losses)\n",
    "np.save('data/gru2_losses.npy', gru_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f9a4676e128>]"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plt.plot(lstm1_losses)\n",
    "plt.plot(gru_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model weights to continue later\n",
    "torch.save(encoder_lstm.state_dict(), 'models/encoder2_lstm.pth')\n",
    "torch.save(decoder_lstm.state_dict(), 'models/decoder2_lstm.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder_gru.state_dict(), 'models/encoder2_gru.pth')\n",
    "torch.save(decoder_gru.state_dict(), 'models/decoder2_gru.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Using the Model for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the idx to word dictionaries to convert predicted indices to words\n",
    "en_idx2word = {k:i for i, k in en_word2idx.items()}\n",
    "fr_idx2word = {k:i for i, k in fr_word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_example(dataloader):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, example, idx2word, show_attention=False):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    input_tensor = batch['french_tensor'].to(device)\n",
    "    break\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "im running out of energy\n",
      "je\n",
      "suis\n",
      "à\n",
      "court\n",
      "<unk>\n",
      "</s>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n"
     ]
    }
   ],
   "source": [
    "input_tensor = batch['french_tensor'][11].unsqueeze_(0)\n",
    "print(batch['english_sentence'][11])\n",
    "for index in input_tensor[0]:\n",
    "    word = fr_idx2word[index.item()]\n",
    "    if word != '<s>':\n",
    "        print(word)\n",
    "    else:\n",
    "        print(word)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(input_tensor, encoder, decoder):\n",
    "    with torch.no_grad():\n",
    "        encoder_hidden = encoder.initHidden(1)\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor.to(device), encoder_hidden)\n",
    "\n",
    "        decoder_input =  torch.tensor([fr_word2idx['<s>']]*input_tensor.shape[0], dtype=torch.long, device=device).unsqueeze(0)\n",
    "        try:\n",
    "            encoder.lstm\n",
    "            decoder_hidden = (encoder_hidden[0][1::2].contiguous(), encoder_hidden[1][1::2].contiguous())\n",
    "        except AttributeError:\n",
    "            decoder_hidden = encoder_hidden[1::2].contiguous()\n",
    "\n",
    "        output_list = []\n",
    "        attn_weight_list = np.zeros((seq_length, seq_length))\n",
    "        for di in range(seq_length):\n",
    "            output, decoder_hidden, attn_weights = decoder(decoder_input,\n",
    "                                                           decoder_hidden,\n",
    "                                                           encoder_output)\n",
    "\n",
    "            decoder_input = output.topk(1)[1].detach()\n",
    "            output_list.append(output.topk(1)[1])\n",
    "            word = en_idx2word[output.topk(1)[1].item()]\n",
    "\n",
    "            attn_weight_list[di] += attn_weights[0,0,:].cpu().numpy()\n",
    "        return output_list, attn_weight_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_list, attn = evaluate(input_tensor, encoder_lstm, decoder_lstm)\n",
    "gru_output_list, gru_attn = evaluate(input_tensor, encoder_gru, decoder_gru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "périgord\n",
      "périgord\n",
      "esu\n",
      "esu\n",
      "affectionately\n",
      "tiamut\n",
      "tiamut\n",
      "tiamut\n",
      "tiamut\n",
      "tiamut\n",
      "\n",
      "GRU model output:\n",
      "\n",
      "i\n",
      "am\n",
      "out\n",
      "of\n",
      "money\n",
      "</s>\n"
     ]
    }
   ],
   "source": [
    "for index in output_list:\n",
    "    word = en_idx2word[index.item()]\n",
    "    if word != '</s>':\n",
    "        print(word)\n",
    "    else:\n",
    "        print(word)\n",
    "        break\n",
    "        \n",
    "print('\\nGRU model output:\\n')\n",
    "for index in gru_output_list:\n",
    "    word = en_idx2word[index.item()]\n",
    "    if word != '</s>':\n",
    "        print(word)\n",
    "    else:\n",
    "        print(word)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9a4934dc18>"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(attn)\n",
    "plt.matshow(gru_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
