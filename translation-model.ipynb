{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Translation Model\n",
    "by Mac Brennan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project I will walk through how to build a translation model that takes in a sentence in French and outputs a sentence in English.The model that will be used is called an encoder-decoder network. What this means is we have two neural networks:\n",
    "- One called the encoder, that extracts the meaning from the French sentence, representing it as a tensor of numbers.\n",
    "- One called the decoder that converts that tensor of numbers back into a sentence in English\n",
    "\n",
    "Our job is to train the encoder and decoder learn to do this in a way that the English sentence that the decoder outputs has the same meaning as the input French sentence. Below is a rough visual of what is happening. We will go into greater detail later.\n",
    "\n",
    "/***************** **Insert picture of the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we train the networks? At a basic level the networks are a collection of parameters(random numbers) and it uses these parameters to perform various calculations on the data. We will get into the specifics of what those calculations are soon, but the purpose of training is to tweak the values of the parameters so the models translation gets more and more accurate.\n",
    "\n",
    "So we have a dataset of French sentences and matching English sentences. We give the model a French sentence and it will give us an English sentence that should match the English translation. We will tell it where it was wrong, the model will then update its parameters to account for its mistakes, and then we will repeat the process. After enough iterations of this, our model will get pretty good at translating.\n",
    "\n",
    "The first thing we must do is process our data into a format that allows our models to manipulate the data. We need to convert our sentences of words into numbers. Let's take a look at our dataset to see what we need to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we get started we will load all the packages we will need\n",
    "import os\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/small_vocab_en', \"r\") as f:\n",
    "    data1 = f.read()\n",
    "with open('data/small_vocab_fr', \"r\") as f:\n",
    "    data2 = f.read()\n",
    "    \n",
    "# The data is just in a text file with each sentence on its own line\n",
    "english_sentences = data1.split('\\n')\n",
    "french_sentences = data2.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of English sentences: 137861 \n",
      "Number of French sentences: 137861 \n",
      "\n",
      "Example/Target pair:\n",
      "\n",
      "  california is usually quiet during march , and it is usually hot in june .\n",
      "  california est généralement calme en mars , et il est généralement chaud en juin .\n"
     ]
    }
   ],
   "source": [
    "print('Number of English sentences:', len(english_sentences), \n",
    "      '\\nNumber of French sentences:', len(french_sentences),'\\n')\n",
    "print('Example/Target pair:\\n')\n",
    "print('  '+english_sentences[2])\n",
    "print('  '+french_sentences[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vocabulary\n",
    "We need to get a word count of each word in the dataset. This will give us a clearer picture of our data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.3167e-02 -4.2483e-03 -1.0572e-01  4.2783e-02 -1.4316e-01 -7.8954e-02\n",
      "  7.8187e-02 -1.9454e-01  2.2303e-02  3.1207e-01  5.7462e-02 -1.1589e-01\n",
      "  9.6633e-02 -9.3229e-02 -3.4229e-02 -1.4652e-01 -1.1094e-01 -1.1102e-01\n",
      "  6.7728e-02  1.0023e-01 -6.7413e-02  2.3761e-01 -1.3105e-01 -8.3979e-03\n",
      " -1.0593e-01  2.4526e-01  6.5903e-02 -2.3740e-01 -1.0758e-01  5.7082e-03\n",
      " -8.1413e-02  2.6264e-01 -5.2461e-02  2.0306e-01  5.0620e-02 -1.8866e-01\n",
      " -1.1494e-01 -2.5752e-01  4.6799e-02 -5.0525e-02  6.2650e-02  1.5433e-01\n",
      " -5.6289e-02 -4.8437e-02 -9.9688e-02 -3.5332e-02 -9.1647e-02 -8.1151e-02\n",
      " -1.0844e-03 -8.4140e-02 -1.3026e-01  1.4980e-02 -8.6276e-02 -5.3041e-02\n",
      " -1.0644e-01 -4.2314e-02  8.6469e-02  2.2614e-01 -1.6078e-01  1.8845e-01\n",
      "  5.3098e-02 -2.1475e-01  1.6699e-01 -1.4442e-01 -1.5930e-01  6.2456e-03\n",
      " -7.6630e-02 -9.1568e-02 -2.8984e-01  2.7078e-02  2.1275e-02  2.3939e-02\n",
      "  1.4903e-01 -3.3062e-01 -9.7811e-02 -3.3814e-02  7.0587e-02  2.3294e-02\n",
      "  6.5382e-02  1.8716e-01 -1.3444e-01  1.4431e-01 -2.6800e-02 -2.2903e-02\n",
      "  9.7554e-02 -3.2909e-02 -2.7827e-02 -6.8771e-02  1.7053e-01 -5.9460e-02\n",
      "  2.0424e-02 -7.7589e-02  1.2160e-01 -7.7437e-02  1.0665e-01  5.1087e-02\n",
      "  7.6379e-03 -6.4936e-02  9.0310e-02  5.9447e-02  4.8881e-03  7.8309e-02\n",
      " -1.2163e-02  6.2155e-02 -7.2664e-02  1.7857e-01 -2.2874e-01  6.6397e-02\n",
      " -3.9295e-02 -2.7717e-02  6.1571e-02  7.2824e-02 -9.2512e-02 -8.7984e-02\n",
      " -1.2753e-01 -1.8705e-03  1.8689e-01  5.1173e-03 -1.3532e-03  4.3246e-02\n",
      "  1.0867e-01 -1.2209e-01 -9.1676e-03  2.3938e-01 -5.9501e-02 -1.0456e-03\n",
      "  8.6584e-02  2.0238e-02  2.1686e-01  1.6495e-01  3.7256e-02  1.2343e-01\n",
      "  1.7706e-01  7.5777e-02  3.1022e-02 -1.2948e-01  3.0936e-02  9.6897e-02\n",
      " -1.0793e-01  1.2644e-01 -5.6489e-02  8.2232e-02  2.0679e-01  1.1679e-01\n",
      "  1.3965e-01  2.6362e-01  3.7603e-02 -3.1050e-03 -8.9501e-02 -7.6969e-03\n",
      " -1.1654e-01 -2.8567e-01  4.6616e-02 -8.2062e-03  1.5621e-01 -1.4641e-01\n",
      "  6.4561e-02 -1.1330e-01  2.7129e-01  1.4532e-01 -2.1773e-02  2.3305e-01\n",
      " -1.6170e-01  1.5705e-01  1.3845e-01  2.2417e-02 -1.0982e-01 -4.9431e-02\n",
      "  7.6855e-02 -4.5300e-02 -1.9029e-01  1.1183e-02 -1.0393e-02  1.6916e-03\n",
      "  8.9407e-02 -5.1022e-02 -8.6066e-02  8.3933e-02 -8.1962e-03 -7.7321e-03\n",
      "  3.3991e-02 -2.0092e-01  3.3280e-02  6.2224e-02  1.6121e-02  2.7143e-01\n",
      " -1.9754e-01 -1.5222e-01 -1.5345e-02 -6.3907e-02 -9.8597e-02 -2.0162e-01\n",
      "  1.4004e-01 -1.1533e-05 -1.8928e-01  1.2253e-01 -7.0378e-03  8.6400e-02\n",
      " -3.0255e-01 -3.9080e-02  4.5517e-02 -1.6449e-01 -2.3548e-01  5.2781e-02\n",
      "  1.3847e-01 -2.0022e-01 -1.5974e-02  2.7137e-02  1.8287e-01 -2.3890e-02\n",
      "  2.2072e-01 -4.2710e-02 -7.5939e-02 -8.7386e-02 -4.9337e-02  4.7824e-02\n",
      " -5.9078e-02 -1.5181e-01 -2.1229e-01 -5.4944e-02 -1.1453e-02 -1.1996e-01\n",
      " -1.5307e-01 -5.4828e-02 -5.3217e-02 -4.8546e-02  2.8856e-02 -9.4537e-02\n",
      "  2.7144e-01  5.4638e-02  5.9727e-02  6.1772e-02  9.2590e-03 -1.2032e-01\n",
      " -1.6646e-01 -2.9087e-02  2.8752e-03 -1.6076e-01 -1.3710e-01 -1.8988e-01\n",
      "  2.2857e-02  1.8455e-01 -1.8236e-02 -6.0562e-03  1.4302e-01  3.2535e-02\n",
      "  1.4333e-01 -3.0871e-02 -1.5218e-01  9.2813e-02  6.6358e-02  1.8316e-02\n",
      " -2.4143e-01  5.4391e-03 -6.4479e-02 -8.5960e-02  3.0446e-02  8.2157e-02\n",
      "  2.6093e-02  5.8985e-02  5.1085e-03  8.9127e-02 -1.8164e-02 -7.7821e-02\n",
      "  3.4232e-03  1.3892e-01  4.6106e-02 -5.4170e-02  8.4399e-03 -1.5362e-01\n",
      " -1.4735e-01  6.5191e-02 -2.2883e-02 -1.4498e-01 -1.6917e-01 -1.9215e-01\n",
      "  1.0611e-01  1.6780e-03 -1.6331e-01 -7.3070e-02  1.1576e-01  8.3567e-02\n",
      " -6.0317e-02 -6.4714e-02  1.5305e-01 -1.1949e-01  1.6684e-01  1.4109e-01\n",
      "  4.6036e-02 -6.0393e-02  4.6595e-02 -1.1558e-01  4.4184e-02 -2.3124e-02\n",
      "  2.5860e-02 -1.1653e-01  1.0936e-02  8.9398e-02 -1.5900e-02  1.4866e-01]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "with open('data/wiki.en.vec', \"r\") as f:\n",
    "    en_vecs = f.readline()\n",
    "    en_vecs = f.readline()\n",
    "    print(np.float32(en_vecs.split()[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess the data\n",
    "The example sentencs and label sentences need to be converted to ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['new',\n",
       " 'jersey',\n",
       " 'is',\n",
       " 'sometimes',\n",
       " 'quiet',\n",
       " 'during',\n",
       " 'autumn',\n",
       " ',',\n",
       " 'and',\n",
       " 'it',\n",
       " 'is',\n",
       " 'snowy',\n",
       " 'in',\n",
       " 'april',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_sentences[0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 5\n",
    "batch_size = 1\n",
    "input_dim = 10\n",
    "hidden_size = 3\n",
    "hidden_layers = 1\n",
    "inputs = autograd.Variable(torch.randn((seq_len, batch_size, input_dim)))  # make a sequence of length 5, 1 batch, input 10dim vector\n",
    "\n",
    "# initialize the hidden state. hidden layers have 3 nodes\n",
    "hidden = (autograd.Variable(torch.randn(hidden_layers, batch_size, hidden_size)),\n",
    "          autograd.Variable(torch.randn((hidden_layers, batch_size, hidden_size))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lstm = nn.LSTM(input_dim, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, hidden = lstm(inputs, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      " -0.1632 -0.0587 -0.0624\n",
      "\n",
      "(1 ,.,.) = \n",
      " -0.0296 -0.1508  0.0113\n",
      "\n",
      "(2 ,.,.) = \n",
      "  0.0398  0.2324 -0.0406\n",
      "\n",
      "(3 ,.,.) = \n",
      "  0.1305 -0.2526 -0.0687\n",
      "\n",
      "(4 ,.,.) = \n",
      "  0.1295  0.1184 -0.1268\n",
      "[torch.FloatTensor of size 5x1x3]\n",
      "\n",
      "(Variable containing:\n",
      "(0 ,.,.) = \n",
      "  0.1295  0.1184 -0.1268\n",
      "[torch.FloatTensor of size 1x1x3]\n",
      ", Variable containing:\n",
      "(0 ,.,.) = \n",
      "  0.3113  0.2014 -0.2873\n",
      "[torch.FloatTensor of size 1x1x3]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# outputs of each hidden node(3) for each item in sequence(5)\n",
    "print(out)\n",
    "\n",
    "# final hidden state and final cell state of sequence; notice that the hidden state equals the final output\n",
    "print(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 5\n",
    "batch_size = 1\n",
    "input_dim = 10\n",
    "hidden_size = 3\n",
    "hidden_layers = 1\n",
    "num_dir = 2 # for bidirectional lstm\n",
    "inputs = autograd.Variable(torch.randn((seq_len, batch_size, input_dim)))  # make a sequence of length 5, 1 batch, input 10dim vector\n",
    "\n",
    "# initialize the hidden state. hidden layers have 3 nodes\n",
    "hidden = (autograd.Variable(torch.randn(hidden_layers*num_dir, batch_size, hidden_size)),\n",
    "          autograd.Variable(torch.randn((hidden_layers*num_dir, batch_size, hidden_size))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(input_dim, hidden_size, bidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, hidden = lstm(inputs, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      " -0.1693  0.7509 -0.0828 -0.5793  0.5285  0.6648\n",
      "\n",
      "(1 ,.,.) = \n",
      "  0.0218  0.1844 -0.1121 -0.6127  0.2034  0.3170\n",
      "\n",
      "(2 ,.,.) = \n",
      "  0.0731 -0.0683  0.0742 -0.0712  0.1552  0.0725\n",
      "\n",
      "(3 ,.,.) = \n",
      "  0.0098 -0.0052 -0.1215 -0.0741  0.2897  0.0834\n",
      "\n",
      "(4 ,.,.) = \n",
      " -0.0263 -0.3019 -0.1845 -0.1061  0.4653  0.0832\n",
      "[torch.FloatTensor of size 5x1x6]\n",
      "\n",
      "(Variable containing:\n",
      "(0 ,.,.) = \n",
      " -0.0263 -0.3019 -0.1845\n",
      "\n",
      "(1 ,.,.) = \n",
      " -0.5793  0.5285  0.6648\n",
      "[torch.FloatTensor of size 2x1x3]\n",
      ", Variable containing:\n",
      "(0 ,.,.) = \n",
      " -0.3161 -0.5384 -0.4495\n",
      "\n",
      "(1 ,.,.) = \n",
      " -0.7322  0.9997  0.9236\n",
      "[torch.FloatTensor of size 2x1x3]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# outputs of each hidden node in both directions(3*2) for each item in sequence(5)\n",
    "print(out)\n",
    "\n",
    "# final hidden and cell state of model in both directions\n",
    "# notice that the first 3 output of the final item equals the final first hidden state\n",
    "# the second 3 outputs from the first item equals the final second hidden state\n",
    "print(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-Directional LSTM Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBiLSTM(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(EncoderBiLSTM, self).__init__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Decoder with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderLSTM(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(AttnDecoderLSTM, self).__init__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Visualizing Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
